{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdbsS6fRT7c4",
        "outputId": "f6600a20-5a52-4611-8fba-389ed4bb1ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.2+cu102\n",
            "10.2\n",
            "7605\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNcP4gXhW-Am"
      },
      "source": [
        "## GPU properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ySYR-2n2XITp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 15 16:02:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   58C    P0    N/A /  N/A |    357MiB /  4096MiB |      9%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1261      G   /usr/lib/xorg/Xorg                 37MiB |\n",
            "|    0   N/A  N/A      1897      G   /usr/lib/xorg/Xorg                133MiB |\n",
            "|    0   N/A  N/A      2145      G   /usr/bin/gnome-shell               45MiB |\n",
            "|    0   N/A  N/A      2758      G   ...842281716764834793,131072       44MiB |\n",
            "|    0   N/A  N/A      5275      G   ...AAAAAAAAA= --shared-files       78MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Activate GPU usage, Runtime -> Change Runtime Type -> Choose GPU type\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oOMsdTVQXJfA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eYxcTNpOXbMp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xj-oZl--Xe1x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9j2PnlM8XheR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 15 16:02:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   59C    P0    N/A /  N/A |   1053MiB /  4096MiB |     25%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1261      G   /usr/lib/xorg/Xorg                 37MiB |\n",
            "|    0   N/A  N/A      1897      G   /usr/lib/xorg/Xorg                133MiB |\n",
            "|    0   N/A  N/A      2145      G   /usr/bin/gnome-shell               45MiB |\n",
            "|    0   N/A  N/A      2758      G   ...842281716764834793,131072       44MiB |\n",
            "|    0   N/A  N/A      5275      G   ...AAAAAAAAA= --shared-files       92MiB |\n",
            "|    0   N/A  N/A     14529      C   /usr/bin/python3.9                682MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(10000000,device='cuda')\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nOi3ZX0vXjdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 15 16:02:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   59C    P0    N/A /  N/A |   1034MiB /  4096MiB |     14%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1261      G   /usr/lib/xorg/Xorg                 37MiB |\n",
            "|    0   N/A  N/A      1897      G   /usr/lib/xorg/Xorg                133MiB |\n",
            "|    0   N/A  N/A      2145      G   /usr/bin/gnome-shell               45MiB |\n",
            "|    0   N/A  N/A      2758      G   ...842281716764834793,131072       44MiB |\n",
            "|    0   N/A  N/A      5275      G   ...AAAAAAAAA= --shared-files      112MiB |\n",
            "|    0   N/A  N/A     14529      C   /usr/bin/python3.9                642MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "del a \n",
        "torch.cuda.empty_cache()\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npBMwhEAUNKS"
      },
      "source": [
        "## Initialization by torch.distributed.init_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tuiqhGDZUN1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from process 0 (out of 4)!\n",
            "Hello from process 2 (out of 4)!\n",
            "Hello from process 3 (out of 4)!\n",
            "\n",
            "\n",
            "\n",
            "Hello from process 1 (out of 4)!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "\n",
        "\n",
        "def print_rank():\n",
        "    print(f\"Hello from process {dist.get_rank()} (out of {dist.get_world_size()})!\\n\")\n",
        "\n",
        "\n",
        "def init_process(rank, size, fn, backend=\"gloo\"):\n",
        "    \"\"\"Initialize the distributed environment.\"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"20951\"\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn()\n",
        "\n",
        "\n",
        "def main(fn, size=4):\n",
        "    processes = []\n",
        "    for rank in range(size):\n",
        "        p = Process(target=init_process, args=(rank, size, fn))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "\n",
        "\n",
        "main(print_rank, size=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky9itE6vUxfi"
      },
      "source": [
        "Q1: Which method is used to launch multiple processes?  \n",
        "Q2: After initilization, the rank of the process and the worldsize can be obtained by which functions in torch.distributed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xzJ4l8BVGYS"
      },
      "source": [
        "## Communication: broadcast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ar2QfftcU3aK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am 0 of 4 with a tensor 0I am 1 of 4 with a tensor 1I am 2 of 4 with a tensor 2I am 3 of 4 with a tensor 3\n",
            "\n",
            "\n",
            "\n",
            "************ Starting Communication ************\n",
            "Rank Rank Rank Rank      2301    has data    has data  has data  tensor(0)\n",
            "tensor(0)\n",
            "tensor(0) has data \n",
            " tensor(0)\n"
          ]
        }
      ],
      "source": [
        "def broadcast():\n",
        "\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank)\n",
        "    group = dist.new_group([0, 1, 2, 3])\n",
        "    print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "\n",
        "    if rank == 0:\n",
        "        print(\"************ Starting Communication ************\")\n",
        "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
        "    print(\"Rank \", rank, \" has data \", tensor)\n",
        "\n",
        "\n",
        "main(broadcast, size=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMyzvRnVYEa"
      },
      "source": [
        "Q3: In the above code, which rank is the one who broadcasts?\n",
        "<br>\n",
        "Task 1: If Rank 0 just wants to broadcast to a random subset of all the processes, please write down the new code to acheive that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dct9a9x9VYjj"
      },
      "outputs": [],
      "source": [
        "#Answer for Task 1\n",
        "import random\n",
        "\n",
        "def broadcast_random(seed=1234):\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank)\n",
        "    #print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    \n",
        "    random.seed(seed)\n",
        "    random_group = random.sample([i for i in range(1,size)], 2)\n",
        "    random_group = random_group + [0]\n",
        "    print(f\"Rank 0 broadcasts to the group {random_group}\")\n",
        "    group = dist.new_group(random_group)\n",
        "\n",
        "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
        "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
        "    print('Rank ', rank, ' has data ', tensor)\n",
        "\n",
        "main(broadcast_random, size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3XSReuNVize"
      },
      "source": [
        "## Communication: reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hgGZWhSQVpbC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am 0 of 4 with a tensor 1I am 3 of 4 with a tensor 4I am 2 of 4 with a tensor 3I am 1 of 4 with a tensor 2\n",
            "\n",
            "\n",
            "\n",
            "************ Starting Communication ************\n",
            "Rank 3 has data 4\n",
            "Rank 2 has data 7\n",
            "\n",
            "Rank 1 has data 9Rank 0 has data 9\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def reduce():\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank + 1)\n",
        "    \n",
        "    if rank == 0:\n",
        "        tensor_old = tensor.clone()\n",
        "    \n",
        "    group = dist.new_group([0, 1, 2, 3])\n",
        "    print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    \n",
        "    if rank == 0:\n",
        "        print(\"************ Starting Communication ************\\n\")\n",
        "    \n",
        "    dist.reduce(tensor=tensor, dst=0, op=dist.ReduceOp.SUM, group=group)\n",
        "    \n",
        "    if rank == 0:\n",
        "        tensor -= tensor_old\n",
        "    \n",
        "    print(f\"Rank {rank} has data {tensor.item()}\")\n",
        "\n",
        "\n",
        "main(reduce, size=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xK_TR68V0Pp"
      },
      "source": [
        "Q4: What does the above code acheive?\n",
        "<br>\n",
        "Q5: Check the values of every rank after \"reduce\", try to explain the reason.\n",
        "<br>\n",
        "\n",
        "Task 2 [Server-Client communication]: Write a function which runs for 10 iterations: Among each iteration, \n",
        "- rank 0 broadcasts to a random subset of all the processes, \n",
        "- the processes in the subset update their states by adding one unit, \n",
        "- rank 0 gets the average of the states from the processes in the subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLEtkl2YV4GY"
      },
      "outputs": [],
      "source": [
        "#Answer for Task 2\n",
        "\n",
        "def server_client_communication(group_size=2):\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(float(rank))\n",
        "    iterations = 10\n",
        "    random.seed(0)\n",
        "    seeds = [random.randint(0,10000) for i in range(iterations)]\n",
        "    for i, sd in zip(range(iterations), seeds):\n",
        "        # Step 1\n",
        "        random.seed(sd)\n",
        "        random_group = random.sample([i for i in range(1,size)], group_size)\n",
        "        random_group = random_group + [0]\n",
        "        if rank == 0: print(f\"Iter {i}: Rank 0 broadcasts to the group {random_group}\")\n",
        "        random_group_dist = dist.new_group(random_group)\n",
        "        dist.broadcast(tensor=tensor, src=0, group=random_group_dist)\n",
        "        \n",
        "        # Step 2\n",
        "        if rank in random_group and rank != 0: \n",
        "            tensor += 1\n",
        "\n",
        "        # Step 3\n",
        "        if rank == 0: tensor_old = tensor.clone()\n",
        "        dist.reduce(tensor=tensor, dst=0, op=dist.ReduceOp.SUM, group=random_group_dist)\n",
        "        if rank == 0:\n",
        "            tensor -= tensor_old\n",
        "            tensor = tensor/group_size\n",
        "\n",
        "    if rank == 0: print(f\"The final value of Rank {0} is {tensor}\")\n",
        "\n",
        "main(server_client_communication, size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3soH0YwJWLg-"
      },
      "source": [
        "## Communication: send and receive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTx0fHoKWPGy"
      },
      "outputs": [],
      "source": [
        "def send_receive():\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    tensor = torch.tensor(rank+1)\n",
        "    print(f\"I am {rank} of {size} with a tensor {tensor}\")\n",
        "    if rank == 0:\n",
        "        print(\"**********\\nStarting Communication\\n************\")\n",
        "        dist.recv(tensor, src=1)\n",
        "    if rank == 1:\n",
        "        dist.send(tensor, dst=0)\n",
        "    if rank == 2:\n",
        "        dist.recv(tensor)\n",
        "    if rank == 3:\n",
        "        dist.send(tensor, dst=2)\n",
        "    print('Rank ', rank, ' has data ', tensor.item())\n",
        "\n",
        "main(send_receive, size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oi1pyrxWfzb"
      },
      "source": [
        "## torch.distributed.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anlCvG5VhXZU",
        "outputId": "8be30228-6d77-4e64-d38f-0b8b552002eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Launch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Launch.py\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import argparse\n",
        "\n",
        "\n",
        "def parse():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--func', type=str, help='choose the function to execute')\n",
        "    parser.add_argument('--backend', type=str, help='choose the backend')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def print_rank():\n",
        "    print('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\n",
        "\n",
        "def broadcast():\n",
        "    rank = dist.get_rank()\n",
        "    size = dist.get_world_size()\n",
        "    if 'OMP_NUM_THREADS' not in os.environ:\n",
        "        current_env[\"OMP_NUM_THREADS\"] = 1\n",
        "    if torch.cuda.is_available() == True:\n",
        "        device = torch.device('cuda:'+str(rank))\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    tensor = torch.tensor(rank, device=device)\n",
        "    group = dist.new_group([0,1])\n",
        "    #print(f\"I am {rank} of {size} with a tensor {tensor.item()}\")\n",
        "    if rank == 0 : print(\"**********\\nStarting Communication\\n************\")\n",
        "    dist.broadcast(tensor=tensor, src=0, group=group)\n",
        "    print('Rank ', rank, ' has data ', tensor)\n",
        "\n",
        "\n",
        "if __name__== '__main__':\n",
        "    args = parse()\n",
        "    backend = args.backend\n",
        "    if torch.cuda.is_available() == True:\n",
        "        size = int(os.environ['WORLD_SIZE'])\n",
        "        # if torch.cuda.device_count()<size:\n",
        "            # raise ValueError('size should not larger than the number of GPUs')\n",
        "    rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    function_mapping = {'print_rank': print_rank, 'broadcast': broadcast}\n",
        "    dist.init_process_group(backend)\n",
        "    function_mapping[args.func]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYaDYfZchcn5",
        "outputId": "bbb25ae7-13a4-4944-acdf-0c67e62afd56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from process 0 (out of 2)!\n",
            "Hello from process 1 (out of 2)!\n"
          ]
        }
      ],
      "source": [
        "# Useful detail: \n",
        "# https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py\n",
        "!OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 Launch.py --func \"print_rank\" --backend gloo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0cliqJhjXgr"
      },
      "source": [
        "Q6: Which package is used for launching multiple processes in torch.distributed.launch? [check the source code in the detail link]\n",
        "<br>\n",
        "Task 3: Reserve two GPUs from NEF and try to run the script Launch.py."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PyTorch distributed.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
