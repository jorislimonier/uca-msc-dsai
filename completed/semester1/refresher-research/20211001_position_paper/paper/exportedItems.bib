
@inproceedings{bodla_soft-nms_2017,
	address = {Venice},
	title = {Soft-{NMS} — {Improving} {Object} {Detection} with {One} {Line} of {Code}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237855/},
	doi = {10.1109/ICCV.2017.593},
	abstract = {Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a signiﬁcant overlap (using a pre-deﬁned threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predeﬁned overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC 2007 (1.7\% for both RFCN and Faster-RCNN) and MS-COCO (1.3\% for R-FCN and 1.1\% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8\% to 40.9\% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efﬁciently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for SoftNMS is publicly available on GitHub http://bit.ly/ 2nJLNMu.},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
	month = oct,
	year = {2017},
	pages = {5562--5570},
	file = {Bodla et al. - 2017 - Soft-NMS — Improving Object Detection with One Lin.pdf:/home/joris/Zotero/storage/G7J2ESCG/Bodla et al. - 2017 - Soft-NMS — Improving Object Detection with One Lin.pdf:application/pdf},
}

@inproceedings{cai_cascade_2018,
	address = {Salt Lake City, UT},
	title = {Cascade {R}-{CNN}: {Delving} {Into} {High} {Quality} {Object} {Detection}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Cascade {R}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8578742/},
	doi = {10.1109/CVPR.2018.00644},
	abstract = {In object detection, an intersection over union (IoU) threshold is required to deﬁne positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overﬁtting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overﬁtting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	month = jun,
	year = {2018},
	pages = {6154--6162},
	file = {Cai and Vasconcelos - 2018 - Cascade R-CNN Delving Into High Quality Object De.pdf:/home/joris/Zotero/storage/UTZNUDYH/Cai and Vasconcelos - 2018 - Cascade R-CNN Delving Into High Quality Object De.pdf:application/pdf},
}

@inproceedings{chao_hardnet_2019,
	address = {Seoul, Korea (South)},
	title = {{HarDNet}: {A} {Low} {Memory} {Traffic} {Network}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{HarDNet}},
	url = {https://ieeexplore.ieee.org/document/9010717/},
	doi = {10.1109/ICCV.2019.00365},
	abstract = {State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory trafﬁc for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efﬁciency in terms of both low MACs and memory trafﬁc. The new network achieves 35\%, 36\%, 30\%, 32\%, and 45\% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia proﬁler and ARM Scale-Sim to measure the memory trafﬁc and verify that the inference latency is indeed proportional to the memory trafﬁc consumption and the proposed network consumes low memory trafﬁc. We conclude that one should take memory trafﬁc into consideration when designing neural network architectures for high-resolution applications at the edge.},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Chao, Ping and Kao, Chao-Yang and Ruan, Yushan and Huang, Chien-Hsiang and Lin, Youn-Long},
	month = oct,
	year = {2019},
	pages = {3551--3560},
	file = {Chao et al. - 2019 - HarDNet A Low Memory Traffic Network.pdf:/home/joris/Zotero/storage/5UPFEBNT/Chao et al. - 2019 - HarDNet A Low Memory Traffic Network.pdf:application/pdf},
}

@inproceedings{cao_hierarchical_2019,
	address = {Seoul, Korea (South)},
	title = {Hierarchical {Shot} {Detector}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009086/},
	doi = {10.1109/ICCV.2019.00980},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Cao, Jiale and Pang, Yanwei and Han, Jungong and Li, Xuelong},
	month = oct,
	year = {2019},
	pages = {9704--9713},
	file = {Cao et al. - 2019 - Hierarchical Shot Detector.pdf:/home/joris/Zotero/storage/Q9DCPBT4/Cao et al. - 2019 - Hierarchical Shot Detector.pdf:application/pdf},
}

@article{chen_gridmask_2020,
	title = {{GridMask} {Data} {Augmentation}},
	url = {http://arxiv.org/abs/2001.04086},
	abstract = {We propose a novel data augmentation method ‘GridMask’ in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to ﬁnd the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.},
	language = {en},
	urldate = {2021-09-17},
	journal = {arXiv:2001.04086 [cs]},
	author = {Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.04086},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2020 - GridMask Data Augmentation.pdf:/home/joris/Zotero/storage/GGELHNCC/Chen et al. - 2020 - GridMask Data Augmentation.pdf:application/pdf},
}

@article{chen_deeplab_2017,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled ﬁlters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the ﬁeld of view of ﬁlters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with ﬁlters at multiple sampling rates and effective ﬁelds-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	language = {en},
	urldate = {2021-09-17},
	journal = {arXiv:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv: 1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:/home/joris/Zotero/storage/L5KPIBPH/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf},
}

@inproceedings{chen_detnas_2019,
	title = {{DetNAS}: {Backbone} {Search} for {Object} {Detection}},
	volume = {32},
	shorttitle = {{DetNAS}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/228b25587479f2fc7570428e8bcbabdc-Abstract.html},
	urldate = {2021-09-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Yukang and Yang, Tong and Zhang, Xiangyu and MENG, GAOFENG and Xiao, Xinyu and Sun, Jian},
	year = {2019},
	file = {Full Text PDF:/home/joris/Zotero/storage/ZPMRJGCZ/Chen et al. - 2019 - DetNAS Backbone Search for Object Detection.pdf:application/pdf},
}

@inproceedings{choi_gaussian_2019,
	address = {Seoul, Korea (South)},
	title = {Gaussian {YOLOv3}: {An} {Accurate} and {Fast} {Object} {Detector} {Using} {Localization} {Uncertainty} for {Autonomous} {Driving}},
	isbn = {978-1-72814-803-8},
	shorttitle = {Gaussian {YOLOv3}},
	url = {https://ieeexplore.ieee.org/document/9008565/},
	doi = {10.1109/ICCV.2019.00059},
	abstract = {The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efﬁcient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can signiﬁcantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Choi, Jiwoong and Chun, Dayoung and Kim, Hyun and Lee, Hyuk-Jae},
	month = oct,
	year = {2019},
	pages = {502--511},
	file = {Choi et al. - 2019 - Gaussian YOLOv3 An Accurate and Fast Object Detec.pdf:/home/joris/Zotero/storage/VICWSAD2/Choi et al. - 2019 - Gaussian YOLOv3 An Accurate and Fast Object Detec.pdf:application/pdf},
}

@inproceedings{ustinova_learning_2016,
	title = {Learning {Deep} {Embeddings} with {Histogram} {Loss}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html},
	urldate = {2021-09-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ustinova, Evgeniya and Lempitsky, Victor},
	year = {2016},
	file = {Full Text PDF:/home/joris/Zotero/storage/VS8JDIWT/Ustinova and Lempitsky - 2016 - Learning Deep Embeddings with Histogram Loss.pdf:application/pdf},
}

@article{devries_improved_2017,
	title = {Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}},
	url = {http://arxiv.org/abs/1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
	urldate = {2021-09-22},
	journal = {arXiv:1708.04552 [cs]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.04552},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/joris/Zotero/storage/QRZX74H7/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/home/joris/Zotero/storage/RLTDK3IZ/1708.html:text/html},
}

@inproceedings{viola_rapid_2001,
	address = {Kauai, HI, USA},
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	isbn = {978-0-7695-1272-3},
	url = {http://ieeexplore.ieee.org/document/990517/},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The ﬁrst is the introduction of a new image representation called the Integral Image which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efﬁcient classiﬁers[6]. The third contribution is a method for combining increasingly more complex classiﬁers in a cascade which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object speciﬁc focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	language = {en},
	urldate = {2021-09-28},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	publisher = {IEEE Comput. Soc},
	author = {Viola, P. and Jones, M.},
	year = {2001},
	pages = {I--511--I--518},
	file = {Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:/home/joris/Zotero/storage/C7HMCNFY/Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:application/pdf;remotesensing-13-00089-v2.pdf:/home/joris/Zotero/storage/9CTVXXXN/remotesensing-13-00089-v2.pdf:application/pdf},
}

@article{zou_object_2019,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {http://arxiv.org/abs/1905.05055},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	urldate = {2021-09-28},
	journal = {arXiv:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/joris/Zotero/storage/3JHMLQW5/Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/joris/Zotero/storage/JKF4F48E/1905.html:text/html},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2021-09-28},
	journal = {arXiv:2004.10934 [cs, eess]},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/joris/Zotero/storage/VHA4N5DQ/Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:application/pdf;arXiv.org Snapshot:/home/joris/Zotero/storage/F9VV4BGU/2004.html:text/html},
}

@inproceedings{li_dynamic_2019,
	address = {Seoul, Korea (South)},
	title = {Dynamic {Anchor} {Feature} {Selection} for {Single}-{Shot} {Object} {Detection}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009487/},
	doi = {10.1109/ICCV.2019.00671},
	abstract = {The design of anchors is critical to the performance of one-stage detectors. Recently, the anchor reﬁnement module (ARM) has been proposed to adjust the initialization of default anchors, providing the detector a better anchor reference. However, this module brings another problem: all pixels at a feature map have the same receptive ﬁeld while the anchors associated with each pixel have different positions and sizes. This discordance may lead to a less effective detector. In this paper, we present a dynamic feature selection operation to select new pixels in a feature map for each reﬁned anchor received from the ARM. The pixels are selected based on the new anchor position and size so that the receptive ﬁled of these pixels can ﬁt the anchor areas well, which makes the detector, especially the regression part, much easier to optimize. Furthermore, to enhance the representation ability of selected feature pixels, we design a bidirectional feature fusion module by combining features from early and deep layers. Extensive experiments on both PASCAL VOC and COCO demonstrate the effectiveness of our dynamic anchor feature selection (DAFS) operation. For the case of high IoU threshold, our DAFS can improve the mAP by a large margin.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Li, Shuai and Yang, Lingxiao and Huang, Jianqiang and Hua, Xian-Sheng and Zhang, Lei},
	month = oct,
	year = {2019},
	pages = {6608--6617},
	file = {Li et al. - 2019 - Dynamic Anchor Feature Selection for Single-Shot O.pdf:/home/joris/Zotero/storage/WKBJDNQK/Li et al. - 2019 - Dynamic Anchor Feature Selection for Single-Shot O.pdf:application/pdf},
}

@article{liu_learning_2019,
	title = {Learning {Spatial} {Fusion} for {Single}-{Shot} {Object} {Detection}},
	url = {http://arxiv.org/abs/1911.09516},
	abstract = {Pyramidal feature representation is the common practice to address the challenge of scale variation in object detection. However, the inconsistency across different feature scales is a primary limitation for the single-shot detectors based on feature pyramid. In this work, we propose a novel and data driven strategy for pyramidal feature fusion, referred to as adaptively spatial feature fusion (ASFF). It learns the way to spatially ﬁlter conﬂictive information to suppress the inconsistency, thus improving the scale-invariance of features, and introduces nearly free inference overhead. With the ASFF strategy and a solid baseline of YOLOv3, we achieve the best speed-accuracy trade-off on the MS COCO dataset, reporting 38.1\% AP at 60 FPS, 42.4\% AP at 45 FPS and 43.9\% AP at 29 FPS. The code is available at https://github.com/ruinmessi/ASFF.},
	language = {en},
	urldate = {2021-09-29},
	journal = {arXiv:1911.09516 [cs]},
	author = {Liu, Songtao and Huang, Di and Wang, Yunhong},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.09516},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Liu et al. - 2019 - Learning Spatial Fusion for Single-Shot Object Det.pdf:/home/joris/Zotero/storage/WN7XX4DN/Liu et al. - 2019 - Learning Spatial Fusion for Single-Shot Object Det.pdf:application/pdf},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
	language = {en},
	urldate = {2021-09-29},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:/home/joris/Zotero/storage/HL5N7L3C/Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf},
}

@inproceedings{tan_efficientdet_2020,
	address = {Seattle, WA, USA},
	title = {{EfficientDet}: {Scalable} and {Efficient} {Object} {Detection}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{EfficientDet}},
	url = {https://ieeexplore.ieee.org/document/9156454/},
	doi = {10.1109/CVPR42600.2020.01079},
	abstract = {Model efﬁciency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efﬁciency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfﬁcientNet backbones, we have developed a new family of object detectors, called EfﬁcientDet, which consistently achieve much better efﬁciency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfﬁcientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs1, being 4x – 9x smaller and using 13x – 42x fewer FLOPs than previous detector. Code is available at https://github.com/google/ automl/tree/master/efficientdet.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
	month = jun,
	year = {2020},
	pages = {10778--10787},
	file = {Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf:/home/joris/Zotero/storage/HMJT5IZQ/Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf:application/pdf},
}

@inproceedings{zhang_bridging_2020,
	address = {Seattle, WA, USA},
	title = {Bridging the {Gap} {Between} {Anchor}-{Based} and {Anchor}-{Free} {Detection} via {Adaptive} {Training} {Sample} {Selection}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156746/},
	doi = {10.1109/CVPR42600.2020.00978},
	abstract = {Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we ﬁrst point out that the essential difference between anchor-based and anchor-free detection is actually how to deﬁne positive and negative training samples, which leads to the performance gap between them. If they adopt the same deﬁnition of positive and negative samples during training, there is no obvious difference in the ﬁnal performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It signiﬁcantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve stateof-the-art detectors by a large margin to 50.7\% AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Shifeng and Chi, Cheng and Yao, Yongqiang and Lei, Zhen and Li, Stan Z.},
	month = jun,
	year = {2020},
	pages = {9756--9765},
	file = {Zhang et al. - 2020 - Bridging the Gap Between Anchor-Based and Anchor-F.pdf:/home/joris/Zotero/storage/ZSKS5IFH/Zhang et al. - 2020 - Bridging the Gap Between Anchor-Based and Anchor-F.pdf:application/pdf},
}

@inproceedings{lee_centermask_2020,
	address = {Seattle, WA, USA},
	title = {{CenterMask}: {Real}-{Time} {Anchor}-{Free} {Instance} {Segmentation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{CenterMask}},
	url = {https://ieeexplore.ieee.org/document/9156712/},
	doi = {10.1109/CVPR42600.2020.01392},
	abstract = {We propose a simple yet efﬁcient anchor-free instance segmentation, called CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch to anchorfree one stage object detector (FCOS [33]) in the same vein with Mask R-CNN [9]. Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each detected box with the spatial attention map that helps to focus on informative pixels and suppress noise. We also present an improved backbone networks, VoVNetV2, with two effective strategies: (1) residual connection for alleviating the optimization problem of larger VoVNet [19] and (2) effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of original SE. With SAG-Mask and VoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted each to large and small models, respectively. Using the same ResNet-101-FPN backbone, CenterMask achieves 38.3\%, surpassing all previous state-ofthe-art methods while at a much faster speed. CenterMaskLite also outperforms the state-of-the-art by large margins at over 35fps on Titan Xp. We hope that CenterMask and VoVNetV2 can serve as a solid baseline of real-time instance segmentation and backbone network for various vision tasks, respectively. The Code is available at https: //github.com/youngwanLEE/CenterMask.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lee, Youngwan and Park, Jongyoul},
	month = jun,
	year = {2020},
	pages = {13903--13912},
	file = {Lee and Park - 2020 - CenterMask Real-Time Anchor-Free Instance Segment.pdf:/home/joris/Zotero/storage/VVNW4C55/Lee and Park - 2020 - CenterMask Real-Time Anchor-Free Instance Segment.pdf:application/pdf},
}

@article{carranza-garcia_performance_2020,
	title = {On the {Performance} of {One}-{Stage} and {Two}-{Stage} {Object} {Detectors} in {Autonomous} {Vehicles} {Using} {Camera} {Data}},
	volume = {13},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/1/89},
	doi = {10.3390/rs13010089},
	abstract = {Object detection using remote sensing data is a key task of the perception systems of self-driving vehicles. While many generic deep learning architectures have been proposed for this problem, there is little guidance on their suitability when using them in a particular scenario such as autonomous driving. In this work, we aim to assess the performance of existing 2D detection systems on a multi-class problem (vehicles, pedestrians, and cyclists) with images obtained from the on-board camera sensors of a car. We evaluate several one-stage (RetinaNet, FCOS, and YOLOv3) and two-stage (Faster R-CNN) deep learning meta-architectures under different image resolutions and feature extractors (ResNet, ResNeXt, Res2Net, DarkNet, and MobileNet). These models are trained using transfer learning and compared in terms of both precision and efﬁciency, with special attention to the real-time requirements of this context. For the experimental study, we use the Waymo Open Dataset, which is the largest existing benchmark. Despite the rising popularity of one-stage detectors, our ﬁndings show that two-stage detectors still provide the most robust performance. Faster R-CNN models outperform one-stage detectors in accuracy, being also more reliable in the detection of minority classes. Faster R-CNN Res2Net-101 achieves the best speed/accuracy tradeoff but needs lower resolution images to reach real-time speed. Furthermore, the anchor-free FCOS detector is a slightly faster alternative to RetinaNet, with similar precision and lower memory usage.},
	language = {en},
	number = {1},
	urldate = {2021-09-29},
	journal = {Remote Sensing},
	author = {Carranza-García, Manuel and Torres-Mateo, Jesús and Lara-Benítez, Pedro and García-Gutiérrez, Jorge},
	month = dec,
	year = {2020},
	pages = {89},
	file = {Carranza-García et al. - 2020 - On the Performance of One-Stage and Two-Stage Obje.pdf:/home/joris/Zotero/storage/FVJ4NW9R/Carranza-García et al. - 2020 - On the Performance of One-Stage and Two-Stage Obje.pdf:application/pdf},
}
