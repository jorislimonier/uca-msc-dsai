
@inproceedings{bodla_soft-nms_2017,
	address = {Venice},
	title = {Soft-{NMS} — {Improving} {Object} {Detection} with {One} {Line} of {Code}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237855/},
	doi = {10.1109/ICCV.2017.593},
	abstract = {Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a signiﬁcant overlap (using a pre-deﬁned threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predeﬁned overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC 2007 (1.7\% for both RFCN and Faster-RCNN) and MS-COCO (1.3\% for R-FCN and 1.1\% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8\% to 40.9\% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efﬁciently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for SoftNMS is publicly available on GitHub http://bit.ly/ 2nJLNMu.},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
	month = oct,
	year = {2017},
	pages = {5562--5570},
	file = {Bodla et al. - 2017 - Soft-NMS — Improving Object Detection with One Lin.pdf:/home/joris/Zotero/storage/G7J2ESCG/Bodla et al. - 2017 - Soft-NMS — Improving Object Detection with One Lin.pdf:application/pdf},
}

@inproceedings{cai_cascade_2018,
	address = {Salt Lake City, UT},
	title = {Cascade {R}-{CNN}: {Delving} {Into} {High} {Quality} {Object} {Detection}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Cascade {R}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8578742/},
	doi = {10.1109/CVPR.2018.00644},
	abstract = {In object detection, an intersection over union (IoU) threshold is required to deﬁne positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overﬁtting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overﬁtting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	month = jun,
	year = {2018},
	pages = {6154--6162},
	file = {Cai and Vasconcelos - 2018 - Cascade R-CNN Delving Into High Quality Object De.pdf:/home/joris/Zotero/storage/UTZNUDYH/Cai and Vasconcelos - 2018 - Cascade R-CNN Delving Into High Quality Object De.pdf:application/pdf},
}

@inproceedings{chao_hardnet_2019,
	address = {Seoul, Korea (South)},
	title = {{HarDNet}: {A} {Low} {Memory} {Traffic} {Network}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{HarDNet}},
	url = {https://ieeexplore.ieee.org/document/9010717/},
	doi = {10.1109/ICCV.2019.00365},
	abstract = {State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory trafﬁc for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efﬁciency in terms of both low MACs and memory trafﬁc. The new network achieves 35\%, 36\%, 30\%, 32\%, and 45\% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia proﬁler and ARM Scale-Sim to measure the memory trafﬁc and verify that the inference latency is indeed proportional to the memory trafﬁc consumption and the proposed network consumes low memory trafﬁc. We conclude that one should take memory trafﬁc into consideration when designing neural network architectures for high-resolution applications at the edge.},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Chao, Ping and Kao, Chao-Yang and Ruan, Yushan and Huang, Chien-Hsiang and Lin, Youn-Long},
	month = oct,
	year = {2019},
	pages = {3551--3560},
	file = {Chao et al. - 2019 - HarDNet A Low Memory Traffic Network.pdf:/home/joris/Zotero/storage/5UPFEBNT/Chao et al. - 2019 - HarDNet A Low Memory Traffic Network.pdf:application/pdf},
}

@inproceedings{cao_hierarchical_2019,
	address = {Seoul, Korea (South)},
	title = {Hierarchical {Shot} {Detector}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009086/},
	doi = {10.1109/ICCV.2019.00980},
	language = {en},
	urldate = {2021-09-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Cao, Jiale and Pang, Yanwei and Han, Jungong and Li, Xuelong},
	month = oct,
	year = {2019},
	pages = {9704--9713},
	file = {Cao et al. - 2019 - Hierarchical Shot Detector.pdf:/home/joris/Zotero/storage/Q9DCPBT4/Cao et al. - 2019 - Hierarchical Shot Detector.pdf:application/pdf},
}

@article{chen_gridmask_2020,
	title = {{GridMask} {Data} {Augmentation}},
	url = {http://arxiv.org/abs/2001.04086},
	abstract = {We propose a novel data augmentation method ‘GridMask’ in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to ﬁnd the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.},
	language = {en},
	urldate = {2021-09-17},
	journal = {arXiv:2001.04086 [cs]},
	author = {Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.04086},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2020 - GridMask Data Augmentation.pdf:/home/joris/Zotero/storage/GGELHNCC/Chen et al. - 2020 - GridMask Data Augmentation.pdf:application/pdf},
}

@article{chen_deeplab_2017,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled ﬁlters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the ﬁeld of view of ﬁlters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with ﬁlters at multiple sampling rates and effective ﬁelds-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	language = {en},
	urldate = {2021-09-17},
	journal = {arXiv:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv: 1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:/home/joris/Zotero/storage/L5KPIBPH/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf},
}

@article{chen_deeplab_2017-1,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled ﬁlters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the ﬁeld of view of ﬁlters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with ﬁlters at multiple sampling rates and effective ﬁelds-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	language = {en},
	urldate = {2021-09-17},
	journal = {arXiv:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv: 1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:/home/joris/Zotero/storage/86ZN6VHG/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf},
}

@article{chen_deeplab_2017-2,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled ﬁlters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the ﬁeld of view of ﬁlters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with ﬁlters at multiple sampling rates and effective ﬁelds-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	language = {en},
	urldate = {2021-09-17},
	journal = {arXiv:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv: 1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:/home/joris/Zotero/storage/YADIH8TN/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf},
}
