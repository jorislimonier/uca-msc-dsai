{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I5d6WWs65aR"
      },
      "source": [
        "# Explore `torch.nn.Module`\n",
        "\n",
        "[`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is the base class for neural network modules in PyTorch. In this notebook, we explore some attributes / methods of `nn.Module`.\n",
        "\n",
        "Last week, we have seen how to define a feed forward neural network: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tL96cHGE5h3w"
      },
      "outputs": [],
      "source": [
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.drop(out)\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37IfsRAN8kSu"
      },
      "source": [
        "This week we will use pre-built neural network. [torchvision](https://pytorch.org/vision/stable/index.html) and [torchaudio](https://pytorch.org/audio/stable/index.html) provide an implementation of different neural architectures, with pretrained weights. This is similar to  [tensorflow.keras.applications](https://keras.io/api/applications/). \n",
        "\n",
        "**Exercice:** Initialize a ResNet34 classification model. You may want to have a look at [the documentation](https://pytorch.org/vision/stable/models.html#models-and-pre-trained-weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XiVJdcS89mYW"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "\n",
        "\n",
        "net = models.resnet34(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5llYM1b_L1n",
        "outputId": "1b61474e-f66a-4fe8-aa28-3bf32a4c3745"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5fKUJ50-ldX"
      },
      "source": [
        "Different methods can be used to iterate over the layers / parameters of the network. This includes `.parameters()`, `.named_parameters()` and `.state_dict()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVCoAktW-Uhb",
        "outputId": "583a4a63-1a9b-4b3e-ef30-4538ae3fb0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 7, 7])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([128, 64, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 64, 1, 1])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([256, 128, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 128, 1, 1])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([512, 256, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 256, 1, 1])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([1000, 512])\n",
            "torch.Size([1000])\n",
            "******************************\n",
            "number of parameters is 110\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for param in net.parameters():\n",
        "    print(param.shape)\n",
        "    count += 1\n",
        "\n",
        "print(\"*\"*30)\n",
        "print(f\"number of parameters is {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCplLyD1-23A",
        "outputId": "00629f0a-62df-46d8-ff4e-e142833a8fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight has shape: torch.Size([64, 3, 7, 7])\n",
            "bn1.weight has shape: torch.Size([64])\n",
            "bn1.bias has shape: torch.Size([64])\n",
            "layer1.0.conv1.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.0.bn1.weight has shape: torch.Size([64])\n",
            "layer1.0.bn1.bias has shape: torch.Size([64])\n",
            "layer1.0.conv2.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.0.bn2.weight has shape: torch.Size([64])\n",
            "layer1.0.bn2.bias has shape: torch.Size([64])\n",
            "layer1.1.conv1.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.1.bn1.weight has shape: torch.Size([64])\n",
            "layer1.1.bn1.bias has shape: torch.Size([64])\n",
            "layer1.1.conv2.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.1.bn2.weight has shape: torch.Size([64])\n",
            "layer1.1.bn2.bias has shape: torch.Size([64])\n",
            "layer1.2.conv1.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.2.bn1.weight has shape: torch.Size([64])\n",
            "layer1.2.bn1.bias has shape: torch.Size([64])\n",
            "layer1.2.conv2.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.2.bn2.weight has shape: torch.Size([64])\n",
            "layer1.2.bn2.bias has shape: torch.Size([64])\n",
            "layer2.0.conv1.weight has shape: torch.Size([128, 64, 3, 3])\n",
            "layer2.0.bn1.weight has shape: torch.Size([128])\n",
            "layer2.0.bn1.bias has shape: torch.Size([128])\n",
            "layer2.0.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.0.bn2.weight has shape: torch.Size([128])\n",
            "layer2.0.bn2.bias has shape: torch.Size([128])\n",
            "layer2.0.downsample.0.weight has shape: torch.Size([128, 64, 1, 1])\n",
            "layer2.0.downsample.1.weight has shape: torch.Size([128])\n",
            "layer2.0.downsample.1.bias has shape: torch.Size([128])\n",
            "layer2.1.conv1.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.1.bn1.weight has shape: torch.Size([128])\n",
            "layer2.1.bn1.bias has shape: torch.Size([128])\n",
            "layer2.1.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.1.bn2.weight has shape: torch.Size([128])\n",
            "layer2.1.bn2.bias has shape: torch.Size([128])\n",
            "layer2.2.conv1.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.2.bn1.weight has shape: torch.Size([128])\n",
            "layer2.2.bn1.bias has shape: torch.Size([128])\n",
            "layer2.2.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.2.bn2.weight has shape: torch.Size([128])\n",
            "layer2.2.bn2.bias has shape: torch.Size([128])\n",
            "layer2.3.conv1.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.3.bn1.weight has shape: torch.Size([128])\n",
            "layer2.3.bn1.bias has shape: torch.Size([128])\n",
            "layer2.3.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.3.bn2.weight has shape: torch.Size([128])\n",
            "layer2.3.bn2.bias has shape: torch.Size([128])\n",
            "layer3.0.conv1.weight has shape: torch.Size([256, 128, 3, 3])\n",
            "layer3.0.bn1.weight has shape: torch.Size([256])\n",
            "layer3.0.bn1.bias has shape: torch.Size([256])\n",
            "layer3.0.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.0.bn2.weight has shape: torch.Size([256])\n",
            "layer3.0.bn2.bias has shape: torch.Size([256])\n",
            "layer3.0.downsample.0.weight has shape: torch.Size([256, 128, 1, 1])\n",
            "layer3.0.downsample.1.weight has shape: torch.Size([256])\n",
            "layer3.0.downsample.1.bias has shape: torch.Size([256])\n",
            "layer3.1.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.1.bn1.weight has shape: torch.Size([256])\n",
            "layer3.1.bn1.bias has shape: torch.Size([256])\n",
            "layer3.1.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.1.bn2.weight has shape: torch.Size([256])\n",
            "layer3.1.bn2.bias has shape: torch.Size([256])\n",
            "layer3.2.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.2.bn1.weight has shape: torch.Size([256])\n",
            "layer3.2.bn1.bias has shape: torch.Size([256])\n",
            "layer3.2.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.2.bn2.weight has shape: torch.Size([256])\n",
            "layer3.2.bn2.bias has shape: torch.Size([256])\n",
            "layer3.3.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.3.bn1.weight has shape: torch.Size([256])\n",
            "layer3.3.bn1.bias has shape: torch.Size([256])\n",
            "layer3.3.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.3.bn2.weight has shape: torch.Size([256])\n",
            "layer3.3.bn2.bias has shape: torch.Size([256])\n",
            "layer3.4.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.4.bn1.weight has shape: torch.Size([256])\n",
            "layer3.4.bn1.bias has shape: torch.Size([256])\n",
            "layer3.4.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.4.bn2.weight has shape: torch.Size([256])\n",
            "layer3.4.bn2.bias has shape: torch.Size([256])\n",
            "layer3.5.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.5.bn1.weight has shape: torch.Size([256])\n",
            "layer3.5.bn1.bias has shape: torch.Size([256])\n",
            "layer3.5.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.5.bn2.weight has shape: torch.Size([256])\n",
            "layer3.5.bn2.bias has shape: torch.Size([256])\n",
            "layer4.0.conv1.weight has shape: torch.Size([512, 256, 3, 3])\n",
            "layer4.0.bn1.weight has shape: torch.Size([512])\n",
            "layer4.0.bn1.bias has shape: torch.Size([512])\n",
            "layer4.0.conv2.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.0.bn2.weight has shape: torch.Size([512])\n",
            "layer4.0.bn2.bias has shape: torch.Size([512])\n",
            "layer4.0.downsample.0.weight has shape: torch.Size([512, 256, 1, 1])\n",
            "layer4.0.downsample.1.weight has shape: torch.Size([512])\n",
            "layer4.0.downsample.1.bias has shape: torch.Size([512])\n",
            "layer4.1.conv1.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.1.bn1.weight has shape: torch.Size([512])\n",
            "layer4.1.bn1.bias has shape: torch.Size([512])\n",
            "layer4.1.conv2.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.1.bn2.weight has shape: torch.Size([512])\n",
            "layer4.1.bn2.bias has shape: torch.Size([512])\n",
            "layer4.2.conv1.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.2.bn1.weight has shape: torch.Size([512])\n",
            "layer4.2.bn1.bias has shape: torch.Size([512])\n",
            "layer4.2.conv2.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.2.bn2.weight has shape: torch.Size([512])\n",
            "layer4.2.bn2.bias has shape: torch.Size([512])\n",
            "fc.weight has shape: torch.Size([1000, 512])\n",
            "fc.bias has shape: torch.Size([1000])\n",
            "******************************\n",
            "number of named parameters is 110\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for key, value in net.named_parameters():\n",
        "    count += 1 \n",
        "    print(f\"{key} has shape: {value.shape}\")\n",
        "\n",
        "print(\"*\"*30)\n",
        "print(f\"number of named parameters is {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-29gmYIAWDU",
        "outputId": "dfe1225b-d26d-42a9-f5b7-77b3832ff37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight has shape: torch.Size([64, 3, 7, 7])\n",
            "bn1.weight has shape: torch.Size([64])\n",
            "bn1.bias has shape: torch.Size([64])\n",
            "bn1.running_mean has shape: torch.Size([64])\n",
            "bn1.running_var has shape: torch.Size([64])\n",
            "bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer1.0.conv1.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.0.bn1.weight has shape: torch.Size([64])\n",
            "layer1.0.bn1.bias has shape: torch.Size([64])\n",
            "layer1.0.bn1.running_mean has shape: torch.Size([64])\n",
            "layer1.0.bn1.running_var has shape: torch.Size([64])\n",
            "layer1.0.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer1.0.conv2.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.0.bn2.weight has shape: torch.Size([64])\n",
            "layer1.0.bn2.bias has shape: torch.Size([64])\n",
            "layer1.0.bn2.running_mean has shape: torch.Size([64])\n",
            "layer1.0.bn2.running_var has shape: torch.Size([64])\n",
            "layer1.0.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer1.1.conv1.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.1.bn1.weight has shape: torch.Size([64])\n",
            "layer1.1.bn1.bias has shape: torch.Size([64])\n",
            "layer1.1.bn1.running_mean has shape: torch.Size([64])\n",
            "layer1.1.bn1.running_var has shape: torch.Size([64])\n",
            "layer1.1.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer1.1.conv2.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.1.bn2.weight has shape: torch.Size([64])\n",
            "layer1.1.bn2.bias has shape: torch.Size([64])\n",
            "layer1.1.bn2.running_mean has shape: torch.Size([64])\n",
            "layer1.1.bn2.running_var has shape: torch.Size([64])\n",
            "layer1.1.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer1.2.conv1.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.2.bn1.weight has shape: torch.Size([64])\n",
            "layer1.2.bn1.bias has shape: torch.Size([64])\n",
            "layer1.2.bn1.running_mean has shape: torch.Size([64])\n",
            "layer1.2.bn1.running_var has shape: torch.Size([64])\n",
            "layer1.2.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer1.2.conv2.weight has shape: torch.Size([64, 64, 3, 3])\n",
            "layer1.2.bn2.weight has shape: torch.Size([64])\n",
            "layer1.2.bn2.bias has shape: torch.Size([64])\n",
            "layer1.2.bn2.running_mean has shape: torch.Size([64])\n",
            "layer1.2.bn2.running_var has shape: torch.Size([64])\n",
            "layer1.2.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.0.conv1.weight has shape: torch.Size([128, 64, 3, 3])\n",
            "layer2.0.bn1.weight has shape: torch.Size([128])\n",
            "layer2.0.bn1.bias has shape: torch.Size([128])\n",
            "layer2.0.bn1.running_mean has shape: torch.Size([128])\n",
            "layer2.0.bn1.running_var has shape: torch.Size([128])\n",
            "layer2.0.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.0.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.0.bn2.weight has shape: torch.Size([128])\n",
            "layer2.0.bn2.bias has shape: torch.Size([128])\n",
            "layer2.0.bn2.running_mean has shape: torch.Size([128])\n",
            "layer2.0.bn2.running_var has shape: torch.Size([128])\n",
            "layer2.0.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.0.downsample.0.weight has shape: torch.Size([128, 64, 1, 1])\n",
            "layer2.0.downsample.1.weight has shape: torch.Size([128])\n",
            "layer2.0.downsample.1.bias has shape: torch.Size([128])\n",
            "layer2.0.downsample.1.running_mean has shape: torch.Size([128])\n",
            "layer2.0.downsample.1.running_var has shape: torch.Size([128])\n",
            "layer2.0.downsample.1.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.1.conv1.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.1.bn1.weight has shape: torch.Size([128])\n",
            "layer2.1.bn1.bias has shape: torch.Size([128])\n",
            "layer2.1.bn1.running_mean has shape: torch.Size([128])\n",
            "layer2.1.bn1.running_var has shape: torch.Size([128])\n",
            "layer2.1.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.1.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.1.bn2.weight has shape: torch.Size([128])\n",
            "layer2.1.bn2.bias has shape: torch.Size([128])\n",
            "layer2.1.bn2.running_mean has shape: torch.Size([128])\n",
            "layer2.1.bn2.running_var has shape: torch.Size([128])\n",
            "layer2.1.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.2.conv1.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.2.bn1.weight has shape: torch.Size([128])\n",
            "layer2.2.bn1.bias has shape: torch.Size([128])\n",
            "layer2.2.bn1.running_mean has shape: torch.Size([128])\n",
            "layer2.2.bn1.running_var has shape: torch.Size([128])\n",
            "layer2.2.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.2.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.2.bn2.weight has shape: torch.Size([128])\n",
            "layer2.2.bn2.bias has shape: torch.Size([128])\n",
            "layer2.2.bn2.running_mean has shape: torch.Size([128])\n",
            "layer2.2.bn2.running_var has shape: torch.Size([128])\n",
            "layer2.2.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.3.conv1.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.3.bn1.weight has shape: torch.Size([128])\n",
            "layer2.3.bn1.bias has shape: torch.Size([128])\n",
            "layer2.3.bn1.running_mean has shape: torch.Size([128])\n",
            "layer2.3.bn1.running_var has shape: torch.Size([128])\n",
            "layer2.3.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer2.3.conv2.weight has shape: torch.Size([128, 128, 3, 3])\n",
            "layer2.3.bn2.weight has shape: torch.Size([128])\n",
            "layer2.3.bn2.bias has shape: torch.Size([128])\n",
            "layer2.3.bn2.running_mean has shape: torch.Size([128])\n",
            "layer2.3.bn2.running_var has shape: torch.Size([128])\n",
            "layer2.3.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.0.conv1.weight has shape: torch.Size([256, 128, 3, 3])\n",
            "layer3.0.bn1.weight has shape: torch.Size([256])\n",
            "layer3.0.bn1.bias has shape: torch.Size([256])\n",
            "layer3.0.bn1.running_mean has shape: torch.Size([256])\n",
            "layer3.0.bn1.running_var has shape: torch.Size([256])\n",
            "layer3.0.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.0.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.0.bn2.weight has shape: torch.Size([256])\n",
            "layer3.0.bn2.bias has shape: torch.Size([256])\n",
            "layer3.0.bn2.running_mean has shape: torch.Size([256])\n",
            "layer3.0.bn2.running_var has shape: torch.Size([256])\n",
            "layer3.0.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.0.downsample.0.weight has shape: torch.Size([256, 128, 1, 1])\n",
            "layer3.0.downsample.1.weight has shape: torch.Size([256])\n",
            "layer3.0.downsample.1.bias has shape: torch.Size([256])\n",
            "layer3.0.downsample.1.running_mean has shape: torch.Size([256])\n",
            "layer3.0.downsample.1.running_var has shape: torch.Size([256])\n",
            "layer3.0.downsample.1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.1.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.1.bn1.weight has shape: torch.Size([256])\n",
            "layer3.1.bn1.bias has shape: torch.Size([256])\n",
            "layer3.1.bn1.running_mean has shape: torch.Size([256])\n",
            "layer3.1.bn1.running_var has shape: torch.Size([256])\n",
            "layer3.1.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.1.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.1.bn2.weight has shape: torch.Size([256])\n",
            "layer3.1.bn2.bias has shape: torch.Size([256])\n",
            "layer3.1.bn2.running_mean has shape: torch.Size([256])\n",
            "layer3.1.bn2.running_var has shape: torch.Size([256])\n",
            "layer3.1.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.2.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.2.bn1.weight has shape: torch.Size([256])\n",
            "layer3.2.bn1.bias has shape: torch.Size([256])\n",
            "layer3.2.bn1.running_mean has shape: torch.Size([256])\n",
            "layer3.2.bn1.running_var has shape: torch.Size([256])\n",
            "layer3.2.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.2.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.2.bn2.weight has shape: torch.Size([256])\n",
            "layer3.2.bn2.bias has shape: torch.Size([256])\n",
            "layer3.2.bn2.running_mean has shape: torch.Size([256])\n",
            "layer3.2.bn2.running_var has shape: torch.Size([256])\n",
            "layer3.2.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.3.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.3.bn1.weight has shape: torch.Size([256])\n",
            "layer3.3.bn1.bias has shape: torch.Size([256])\n",
            "layer3.3.bn1.running_mean has shape: torch.Size([256])\n",
            "layer3.3.bn1.running_var has shape: torch.Size([256])\n",
            "layer3.3.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.3.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.3.bn2.weight has shape: torch.Size([256])\n",
            "layer3.3.bn2.bias has shape: torch.Size([256])\n",
            "layer3.3.bn2.running_mean has shape: torch.Size([256])\n",
            "layer3.3.bn2.running_var has shape: torch.Size([256])\n",
            "layer3.3.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.4.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.4.bn1.weight has shape: torch.Size([256])\n",
            "layer3.4.bn1.bias has shape: torch.Size([256])\n",
            "layer3.4.bn1.running_mean has shape: torch.Size([256])\n",
            "layer3.4.bn1.running_var has shape: torch.Size([256])\n",
            "layer3.4.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.4.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.4.bn2.weight has shape: torch.Size([256])\n",
            "layer3.4.bn2.bias has shape: torch.Size([256])\n",
            "layer3.4.bn2.running_mean has shape: torch.Size([256])\n",
            "layer3.4.bn2.running_var has shape: torch.Size([256])\n",
            "layer3.4.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.5.conv1.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.5.bn1.weight has shape: torch.Size([256])\n",
            "layer3.5.bn1.bias has shape: torch.Size([256])\n",
            "layer3.5.bn1.running_mean has shape: torch.Size([256])\n",
            "layer3.5.bn1.running_var has shape: torch.Size([256])\n",
            "layer3.5.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer3.5.conv2.weight has shape: torch.Size([256, 256, 3, 3])\n",
            "layer3.5.bn2.weight has shape: torch.Size([256])\n",
            "layer3.5.bn2.bias has shape: torch.Size([256])\n",
            "layer3.5.bn2.running_mean has shape: torch.Size([256])\n",
            "layer3.5.bn2.running_var has shape: torch.Size([256])\n",
            "layer3.5.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.0.conv1.weight has shape: torch.Size([512, 256, 3, 3])\n",
            "layer4.0.bn1.weight has shape: torch.Size([512])\n",
            "layer4.0.bn1.bias has shape: torch.Size([512])\n",
            "layer4.0.bn1.running_mean has shape: torch.Size([512])\n",
            "layer4.0.bn1.running_var has shape: torch.Size([512])\n",
            "layer4.0.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.0.conv2.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.0.bn2.weight has shape: torch.Size([512])\n",
            "layer4.0.bn2.bias has shape: torch.Size([512])\n",
            "layer4.0.bn2.running_mean has shape: torch.Size([512])\n",
            "layer4.0.bn2.running_var has shape: torch.Size([512])\n",
            "layer4.0.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.0.downsample.0.weight has shape: torch.Size([512, 256, 1, 1])\n",
            "layer4.0.downsample.1.weight has shape: torch.Size([512])\n",
            "layer4.0.downsample.1.bias has shape: torch.Size([512])\n",
            "layer4.0.downsample.1.running_mean has shape: torch.Size([512])\n",
            "layer4.0.downsample.1.running_var has shape: torch.Size([512])\n",
            "layer4.0.downsample.1.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.1.conv1.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.1.bn1.weight has shape: torch.Size([512])\n",
            "layer4.1.bn1.bias has shape: torch.Size([512])\n",
            "layer4.1.bn1.running_mean has shape: torch.Size([512])\n",
            "layer4.1.bn1.running_var has shape: torch.Size([512])\n",
            "layer4.1.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.1.conv2.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.1.bn2.weight has shape: torch.Size([512])\n",
            "layer4.1.bn2.bias has shape: torch.Size([512])\n",
            "layer4.1.bn2.running_mean has shape: torch.Size([512])\n",
            "layer4.1.bn2.running_var has shape: torch.Size([512])\n",
            "layer4.1.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.2.conv1.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.2.bn1.weight has shape: torch.Size([512])\n",
            "layer4.2.bn1.bias has shape: torch.Size([512])\n",
            "layer4.2.bn1.running_mean has shape: torch.Size([512])\n",
            "layer4.2.bn1.running_var has shape: torch.Size([512])\n",
            "layer4.2.bn1.num_batches_tracked has shape: torch.Size([])\n",
            "layer4.2.conv2.weight has shape: torch.Size([512, 512, 3, 3])\n",
            "layer4.2.bn2.weight has shape: torch.Size([512])\n",
            "layer4.2.bn2.bias has shape: torch.Size([512])\n",
            "layer4.2.bn2.running_mean has shape: torch.Size([512])\n",
            "layer4.2.bn2.running_var has shape: torch.Size([512])\n",
            "layer4.2.bn2.num_batches_tracked has shape: torch.Size([])\n",
            "fc.weight has shape: torch.Size([1000, 512])\n",
            "fc.bias has shape: torch.Size([1000])\n",
            "******************************\n",
            "number of named parameters is 218\n"
          ]
        }
      ],
      "source": [
        "for key, value in net.state_dict().items():\n",
        "    print(f\"{key} has shape: {value.shape}\")\n",
        "\n",
        "print(\"*\"*30)\n",
        "print(f\"number of named parameters is {len(net.state_dict())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC_m0_BbBzyo"
      },
      "source": [
        "Let's have a look at one parameter returned by the method `.parameters()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7niCBRn1AXFp",
        "outputId": "5bde19b7-dad1-4ce2-f5be-8a7e3292ce1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.nn.parameter.Parameter"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfTMq8aZCWrZ",
        "outputId": "0fddbae6-2d26-4502-fdae-96ea1e95cbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on Parameter in module torch.nn.parameter object:\n",
            "\n",
            "class Parameter(torch.Tensor)\n",
            " |  Parameter(data=None, requires_grad=True)\n",
            " |  \n",
            " |  A kind of Tensor that is to be considered a module parameter.\n",
            " |  \n",
            " |  Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
            " |  very special property when used with :class:`Module` s - when they're\n",
            " |  assigned as Module attributes they are automatically added to the list of\n",
            " |  its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
            " |  Assigning a Tensor doesn't have such effect. This is because one might\n",
            " |  want to cache some temporary state, like last hidden state of the RNN, in\n",
            " |  the model. If there was no such class as :class:`Parameter`, these\n",
            " |  temporaries would get registered too.\n",
            " |  \n",
            " |  Args:\n",
            " |      data (Tensor): parameter tensor.\n",
            " |      requires_grad (bool, optional): if the parameter requires gradient. See\n",
            " |          :ref:`locally-disable-grad-doc` for more details. Default: `True`\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Parameter\n",
            " |      torch.Tensor\n",
            " |      torch._C._TensorBase\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __deepcopy__(self, memo)\n",
            " |  \n",
            " |  __reduce_ex__(self, proto)\n",
            " |      Helper for pickle.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, data=None, requires_grad=True)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  __torch_function__ = _disabled_torch_function_impl(...)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.Tensor:\n",
            " |  \n",
            " |  __abs__ = abs(...)\n",
            " |  \n",
            " |  __array__(self, dtype=None)\n",
            " |  \n",
            " |  __array_wrap__(self, array)\n",
            " |      # Wrap Numpy array again in a suitable tensor when done, to support e.g.\n",
            " |      # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\n",
            " |  \n",
            " |  __contains__(self, element)\n",
            " |      Check if `element` is present in tensor\n",
            " |      \n",
            " |      Args:\n",
            " |          element (Tensor or scalar): element to be checked\n",
            " |              for presence in current tensor\"\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __dlpack__(self, stream=None)\n",
            " |      Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_\n",
            " |      of the current tensor to be exported to other libraries.\n",
            " |      \n",
            " |      This function will be called from the `from_dlpack` method\n",
            " |      of the library that will consume the capsule. `from_dlpack` passes the current\n",
            " |      stream to this method as part of the specification.\n",
            " |      \n",
            " |      Args:\n",
            " |          stream (integer or None): An optional Python integer representing a\n",
            " |          pointer to a CUDA stream. The current stream is synchronized with\n",
            " |          this stream before the capsule is created, and since the capsule\n",
            " |          shares its storage with the tensor this make it safe to access from\n",
            " |          both streams.  If None or -1 is passed then no synchronization is performed.\n",
            " |  \n",
            " |  __dlpack_device__(self) -> Tuple[enum.IntEnum, int]\n",
            " |  \n",
            " |  __floordiv__(self, other)\n",
            " |  \n",
            " |  __format__(self, format_spec)\n",
            " |      Default object formatter.\n",
            " |  \n",
            " |  __hash__(self)\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ipow__(self, other)\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |  \n",
            " |  __itruediv__ = __idiv__(...)\n",
            " |  \n",
            " |  __len__(self)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __neg__ = neg(...)\n",
            " |  \n",
            " |  __pos__ = positive(...)\n",
            " |  \n",
            " |  __pow__ = pow(...)\n",
            " |  \n",
            " |  __rdiv__(self, other)\n",
            " |  \n",
            " |  __reversed__(self)\n",
            " |      Reverses the tensor along dimension 0.\n",
            " |  \n",
            " |  __rfloordiv__(self, other)\n",
            " |  \n",
            " |  __rlshift__(self, other)\n",
            " |  \n",
            " |  __rmatmul__(self, other)\n",
            " |  \n",
            " |  __rmod__(self, other)\n",
            " |  \n",
            " |  __rpow__(self, other)\n",
            " |  \n",
            " |  __rrshift__(self, other)\n",
            " |  \n",
            " |  __rsub__(self, other)\n",
            " |  \n",
            " |  __rtruediv__ = __rdiv__(self, other)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  align_to(self, *names)\n",
            " |      Permutes the dimensions of the :attr:`self` tensor to match the order\n",
            " |      specified in :attr:`names`, adding size-one dims for any new names.\n",
            " |      \n",
            " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
            " |      The resulting tensor is a view on the original tensor.\n",
            " |      \n",
            " |      All dimension names of :attr:`self` must be present in :attr:`names`.\n",
            " |      :attr:`names` may contain additional names that are not in ``self.names``;\n",
            " |      the output tensor has a size-one dimension for each of those new names.\n",
            " |      \n",
            " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
            " |      The Ellipsis is expanded to be equal to all dimension names of :attr:`self`\n",
            " |      that are not mentioned in :attr:`names`, in the order that they appear\n",
            " |      in :attr:`self`.\n",
            " |      \n",
            " |      Python 2 does not support Ellipsis but one may use a string literal\n",
            " |      instead (``'...'``).\n",
            " |      \n",
            " |      Args:\n",
            " |          names (iterable of str): The desired dimension ordering of the\n",
            " |              output tensor. May contain up to one Ellipsis that is expanded\n",
            " |              to all unmentioned dim names of :attr:`self`.\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n",
            " |          >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n",
            " |      \n",
            " |          # Move the F and E dims to the front while keeping the rest in order\n",
            " |          >>> named_tensor.align_to('F', 'E', ...)\n",
            " |      \n",
            " |      .. warning::\n",
            " |          The named tensor API is experimental and subject to change.\n",
            " |  \n",
            " |  backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None)\n",
            " |      Computes the gradient of current tensor w.r.t. graph leaves.\n",
            " |      \n",
            " |      The graph is differentiated using the chain rule. If the tensor is\n",
            " |      non-scalar (i.e. its data has more than one element) and requires\n",
            " |      gradient, the function additionally requires specifying ``gradient``.\n",
            " |      It should be a tensor of matching type and location, that contains\n",
            " |      the gradient of the differentiated function w.r.t. ``self``.\n",
            " |      \n",
            " |      This function accumulates gradients in the leaves - you might need to zero\n",
            " |      ``.grad`` attributes or set them to ``None`` before calling it.\n",
            " |      See :ref:`Default gradient layouts<default-grad-layouts>`\n",
            " |      for details on the memory layout of accumulated gradients.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
            " |          in a user-specified CUDA stream context, see\n",
            " |          :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          When ``inputs`` are provided and a given input is not a leaf,\n",
            " |          the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
            " |          It is an implementation detail on which the user should not rely.\n",
            " |          See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
            " |      \n",
            " |      Args:\n",
            " |          gradient (Tensor or None): Gradient w.r.t. the\n",
            " |              tensor. If it is a tensor, it will be automatically converted\n",
            " |              to a Tensor that does not require grad unless ``create_graph`` is True.\n",
            " |              None values can be specified for scalar Tensors or ones that\n",
            " |              don't require grad. If a None value would be acceptable then\n",
            " |              this argument is optional.\n",
            " |          retain_graph (bool, optional): If ``False``, the graph used to compute\n",
            " |              the grads will be freed. Note that in nearly all cases setting\n",
            " |              this option to True is not needed and often can be worked around\n",
            " |              in a much more efficient way. Defaults to the value of\n",
            " |              ``create_graph``.\n",
            " |          create_graph (bool, optional): If ``True``, graph of the derivative will\n",
            " |              be constructed, allowing to compute higher order derivative\n",
            " |              products. Defaults to ``False``.\n",
            " |          inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n",
            " |              accumulated into ``.grad``. All other Tensors will be ignored. If not\n",
            " |              provided, the gradient is accumulated into all the leaf Tensors that were\n",
            " |              used to compute the attr::tensors.\n",
            " |  \n",
            " |  detach(...)\n",
            " |      Returns a new Tensor, detached from the current graph.\n",
            " |      \n",
            " |      The result will never require gradient.\n",
            " |      \n",
            " |      This method also affects forward mode AD gradients and the result will never\n",
            " |      have forward mode AD gradients.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |        Returned Tensor shares the same storage with the original one.\n",
            " |        In-place modifications on either of them will be seen, and may trigger\n",
            " |        errors in correctness checks.\n",
            " |        IMPORTANT NOTE: Previously, in-place size / stride / storage changes\n",
            " |        (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor\n",
            " |        also update the original tensor. Now, these in-place changes will not update the\n",
            " |        original tensor anymore, and will instead trigger an error.\n",
            " |        For sparse tensors:\n",
            " |        In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the\n",
            " |        returned tensor will not update the original tensor anymore, and will instead\n",
            " |        trigger an error.\n",
            " |  \n",
            " |  detach_(...)\n",
            " |      Detaches the Tensor from the graph that created it, making it a leaf.\n",
            " |      Views cannot be detached in-place.\n",
            " |      \n",
            " |      This method also affects forward mode AD gradients and the result will never\n",
            " |      have forward mode AD gradients.\n",
            " |  \n",
            " |  is_shared(self)\n",
            " |      Checks if tensor is in shared memory.\n",
            " |      \n",
            " |      This is always ``True`` for CUDA tensors.\n",
            " |  \n",
            " |  istft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False)\n",
            " |      See :func:`torch.istft`\n",
            " |  \n",
            " |  lu(self, pivot=True, get_infos=False)\n",
            " |      See :func:`torch.lu`\n",
            " |  \n",
            " |  norm(self, p='fro', dim=None, keepdim=False, dtype=None)\n",
            " |      See :func:`torch.norm`\n",
            " |  \n",
            " |  refine_names(self, *names)\n",
            " |      Refines the dimension names of :attr:`self` according to :attr:`names`.\n",
            " |      \n",
            " |      Refining is a special case of renaming that \"lifts\" unnamed dimensions.\n",
            " |      A ``None`` dim can be refined to have any name; a named dim can only be\n",
            " |      refined to have the same name.\n",
            " |      \n",
            " |      Because named tensors can coexist with unnamed tensors, refining names\n",
            " |      gives a nice way to write named-tensor-aware code that works with both\n",
            " |      named and unnamed tensors.\n",
            " |      \n",
            " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
            " |      The Ellipsis is expanded greedily; it is expanded in-place to fill\n",
            " |      :attr:`names` to the same length as ``self.dim()`` using names from the\n",
            " |      corresponding indices of ``self.names``.\n",
            " |      \n",
            " |      Python 2 does not support Ellipsis but one may use a string literal\n",
            " |      instead (``'...'``).\n",
            " |      \n",
            " |      Args:\n",
            " |          names (iterable of str): The desired names of the output tensor. May\n",
            " |              contain up to one Ellipsis.\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> imgs = torch.randn(32, 3, 128, 128)\n",
            " |          >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n",
            " |          >>> named_imgs.names\n",
            " |          ('N', 'C', 'H', 'W')\n",
            " |      \n",
            " |          >>> tensor = torch.randn(2, 3, 5, 7, 11)\n",
            " |          >>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n",
            " |          >>> tensor.names\n",
            " |          ('A', None, None, 'B', 'C')\n",
            " |      \n",
            " |      .. warning::\n",
            " |          The named tensor API is experimental and subject to change.\n",
            " |  \n",
            " |  register_hook(self, hook)\n",
            " |      Registers a backward hook.\n",
            " |      \n",
            " |      The hook will be called every time a gradient with respect to the\n",
            " |      Tensor is computed. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(grad) -> Tensor or None\n",
            " |      \n",
            " |      \n",
            " |      The hook should not modify its argument, but it can optionally return\n",
            " |      a new gradient which will be used in place of :attr:`grad`.\n",
            " |      \n",
            " |      This function returns a handle with a method ``handle.remove()``\n",
            " |      that removes the hook from the module.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
            " |          >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
            " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
            " |          >>> v.grad\n",
            " |      \n",
            " |           2\n",
            " |           4\n",
            " |           6\n",
            " |          [torch.FloatTensor of size (3,)]\n",
            " |      \n",
            " |          >>> h.remove()  # removes the hook\n",
            " |  \n",
            " |  reinforce(self, reward)\n",
            " |  \n",
            " |  rename(self, *names, **rename_map)\n",
            " |      Renames dimension names of :attr:`self`.\n",
            " |      \n",
            " |      There are two main usages:\n",
            " |      \n",
            " |      ``self.rename(**rename_map)`` returns a view on tensor that has dims\n",
            " |      renamed as specified in the mapping :attr:`rename_map`.\n",
            " |      \n",
            " |      ``self.rename(*names)`` returns a view on tensor, renaming all\n",
            " |      dimensions positionally using :attr:`names`.\n",
            " |      Use ``self.rename(None)`` to drop names on a tensor.\n",
            " |      \n",
            " |      One cannot specify both positional args :attr:`names` and keyword args\n",
            " |      :attr:`rename_map`.\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n",
            " |          >>> renamed_imgs = imgs.rename(N='batch', C='channels')\n",
            " |          >>> renamed_imgs.names\n",
            " |          ('batch', 'channels', 'H', 'W')\n",
            " |      \n",
            " |          >>> renamed_imgs = imgs.rename(None)\n",
            " |          >>> renamed_imgs.names\n",
            " |          (None,)\n",
            " |      \n",
            " |          >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n",
            " |          >>> renamed_imgs.names\n",
            " |          ('batch', 'channel', 'height', 'width')\n",
            " |      \n",
            " |      .. warning::\n",
            " |          The named tensor API is experimental and subject to change.\n",
            " |  \n",
            " |  rename_(self, *names, **rename_map)\n",
            " |      In-place version of :meth:`~Tensor.rename`.\n",
            " |  \n",
            " |  resize(self, *sizes)\n",
            " |  \n",
            " |  resize_as(self, tensor)\n",
            " |  \n",
            " |  share_memory_(self)\n",
            " |      Moves the underlying storage to shared memory.\n",
            " |      \n",
            " |      This is a no-op if the underlying storage is already in shared memory\n",
            " |      and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
            " |  \n",
            " |  split(self, split_size, dim=0)\n",
            " |      See :func:`torch.split`\n",
            " |  \n",
            " |  stft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)\n",
            " |      See :func:`torch.stft`\n",
            " |      \n",
            " |      .. warning::\n",
            " |        This function changed signature at version 0.4.1. Calling with\n",
            " |        the previous signature may cause error or return incorrect result.\n",
            " |  \n",
            " |  to_sparse_csr(self)\n",
            " |      Convert a tensor to compressed row storage format. Only works with 2D tensors.\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> dense = torch.randn(5, 5)\n",
            " |          >>> sparse = dense.to_sparse_csr()\n",
            " |          >>> sparse._nnz()\n",
            " |          25\n",
            " |  \n",
            " |  unflatten(self, dim, sizes)\n",
            " |      Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions\n",
            " |      of sizes given by :attr:`sizes`.\n",
            " |      \n",
            " |      * :attr:`sizes` is the new shape of the unflattened dimension and it can be a `Tuple[int]` as well\n",
            " |        as `torch.Size` if :attr:`self` is a `Tensor`, or `namedshape` (Tuple[(name: str, size: int)])\n",
            " |        if :attr:`self` is a `NamedTensor`. The total number of elements in sizes must match the number\n",
            " |        of elements in the original dim being unflattened.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (Union[int, str]): Dimension to unflatten\n",
            " |          sizes (Union[Tuple[int] or torch.Size, Tuple[Tuple[str, int]]]): New shape of the unflattened dimension\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> torch.randn(3, 4, 1).unflatten(1, (2, 2)).shape\n",
            " |          torch.Size([3, 2, 2, 1])\n",
            " |          >>> torch.randn(3, 4, 1).unflatten(1, (-1, 2)).shape # the size -1 is inferred from the size of dimension 1\n",
            " |          torch.Size([3, 2, 2, 1])\n",
            " |          >>> torch.randn(2, 4, names=('A', 'B')).unflatten('B', (('B1', 2), ('B2', 2)))\n",
            " |          tensor([[[-1.1772,  0.0180],\n",
            " |                  [ 0.2412,  0.1431]],\n",
            " |                  [[-1.1819, -0.8899],\n",
            " |                  [ 1.5813,  0.2274]]], names=('A', 'B1', 'B2'))\n",
            " |          >>> torch.randn(2, names=('A',)).unflatten('A', (('B1', -1), ('B2', 1)))\n",
            " |          tensor([[-0.8591],\n",
            " |                  [ 0.3100]], names=('B1', 'B2'))\n",
            " |      \n",
            " |      .. warning::\n",
            " |          The named tensor API is experimental and subject to change.\n",
            " |  \n",
            " |  unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None)\n",
            " |      Returns the unique elements of the input tensor.\n",
            " |      \n",
            " |      See :func:`torch.unique`\n",
            " |  \n",
            " |  unique_consecutive(self, return_inverse=False, return_counts=False, dim=None)\n",
            " |      Eliminates all but the first element from every consecutive group of equivalent elements.\n",
            " |      \n",
            " |      See :func:`torch.unique_consecutive`\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from torch.Tensor:\n",
            " |  \n",
            " |  __cuda_array_interface__\n",
            " |      Array view description for cuda tensors.\n",
            " |      \n",
            " |      See:\n",
            " |      https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.Tensor:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  grad\n",
            " |      This attribute is ``None`` by default and becomes a Tensor the first time a call to\n",
            " |      :func:`backward` computes gradients for ``self``.\n",
            " |      The attribute will then contain the gradients computed and future calls to\n",
            " |      :func:`backward` will accumulate (add) gradients into it.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.Tensor:\n",
            " |  \n",
            " |  __array_priority__ = 1000\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch._C._TensorBase:\n",
            " |  \n",
            " |  __add__(...)\n",
            " |  \n",
            " |  __and__(...)\n",
            " |  \n",
            " |  __bool__(...)\n",
            " |  \n",
            " |  __complex__(...)\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __div__(...)\n",
            " |  \n",
            " |  __eq__(...)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __float__(...)\n",
            " |  \n",
            " |  __ge__(...)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getitem__(self, key, /)\n",
            " |      Return self[key].\n",
            " |  \n",
            " |  __gt__(...)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __iadd__(...)\n",
            " |  \n",
            " |  __iand__(...)\n",
            " |  \n",
            " |  __idiv__(...)\n",
            " |  \n",
            " |  __ifloordiv__(...)\n",
            " |  \n",
            " |  __ilshift__(...)\n",
            " |  \n",
            " |  __imod__(...)\n",
            " |  \n",
            " |  __imul__(...)\n",
            " |  \n",
            " |  __index__(...)\n",
            " |  \n",
            " |  __int__(...)\n",
            " |  \n",
            " |  __invert__(...)\n",
            " |  \n",
            " |  __ior__(...)\n",
            " |  \n",
            " |  __irshift__(...)\n",
            " |  \n",
            " |  __isub__(...)\n",
            " |  \n",
            " |  __ixor__(...)\n",
            " |  \n",
            " |  __le__(...)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __long__(...)\n",
            " |  \n",
            " |  __lshift__(...)\n",
            " |  \n",
            " |  __lt__(...)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __matmul__(...)\n",
            " |  \n",
            " |  __mod__(...)\n",
            " |  \n",
            " |  __mul__(...)\n",
            " |  \n",
            " |  __ne__(...)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __nonzero__(...)\n",
            " |  \n",
            " |  __or__(...)\n",
            " |  \n",
            " |  __radd__(...)\n",
            " |  \n",
            " |  __rand__(...)\n",
            " |  \n",
            " |  __rmul__(...)\n",
            " |  \n",
            " |  __ror__(...)\n",
            " |  \n",
            " |  __rshift__(...)\n",
            " |  \n",
            " |  __rxor__(...)\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sub__(...)\n",
            " |  \n",
            " |  __truediv__(...)\n",
            " |  \n",
            " |  __xor__(...)\n",
            " |  \n",
            " |  abs(...)\n",
            " |      abs() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.abs`\n",
            " |  \n",
            " |  abs_(...)\n",
            " |      abs_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.abs`\n",
            " |  \n",
            " |  absolute(...)\n",
            " |      absolute() -> Tensor\n",
            " |      \n",
            " |      Alias for :func:`abs`\n",
            " |  \n",
            " |  absolute_(...)\n",
            " |      absolute_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.absolute`\n",
            " |      Alias for :func:`abs_`\n",
            " |  \n",
            " |  acos(...)\n",
            " |      acos() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.acos`\n",
            " |  \n",
            " |  acos_(...)\n",
            " |      acos_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.acos`\n",
            " |  \n",
            " |  acosh(...)\n",
            " |      acosh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.acosh`\n",
            " |  \n",
            " |  acosh_(...)\n",
            " |      acosh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.acosh`\n",
            " |  \n",
            " |  add(...)\n",
            " |      add(other, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`\n",
            " |      and :attr:`other` are specified, each element of :attr:`other` is scaled by\n",
            " |      :attr:`alpha` before being used.\n",
            " |      \n",
            " |      When :attr:`other` is a tensor, the shape of :attr:`other` must be\n",
            " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
            " |      tensor\n",
            " |      \n",
            " |      See :func:`torch.add`\n",
            " |  \n",
            " |  add_(...)\n",
            " |      add_(other, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.add`\n",
            " |  \n",
            " |  addbmm(...)\n",
            " |      addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.addbmm`\n",
            " |  \n",
            " |  addbmm_(...)\n",
            " |      addbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.addbmm`\n",
            " |  \n",
            " |  addcdiv(...)\n",
            " |      addcdiv(tensor1, tensor2, *, value=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.addcdiv`\n",
            " |  \n",
            " |  addcdiv_(...)\n",
            " |      addcdiv_(tensor1, tensor2, *, value=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.addcdiv`\n",
            " |  \n",
            " |  addcmul(...)\n",
            " |      addcmul(tensor1, tensor2, *, value=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.addcmul`\n",
            " |  \n",
            " |  addcmul_(...)\n",
            " |      addcmul_(tensor1, tensor2, *, value=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.addcmul`\n",
            " |  \n",
            " |  addmm(...)\n",
            " |      addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.addmm`\n",
            " |  \n",
            " |  addmm_(...)\n",
            " |      addmm_(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.addmm`\n",
            " |  \n",
            " |  addmv(...)\n",
            " |      addmv(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.addmv`\n",
            " |  \n",
            " |  addmv_(...)\n",
            " |      addmv_(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.addmv`\n",
            " |  \n",
            " |  addr(...)\n",
            " |      addr(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.addr`\n",
            " |  \n",
            " |  addr_(...)\n",
            " |      addr_(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.addr`\n",
            " |  \n",
            " |  align_as(...)\n",
            " |      align_as(other) -> Tensor\n",
            " |      \n",
            " |      Permutes the dimensions of the :attr:`self` tensor to match the dimension order\n",
            " |      in the :attr:`other` tensor, adding size-one dims for any new names.\n",
            " |      \n",
            " |      This operation is useful for explicit broadcasting by names (see examples).\n",
            " |      \n",
            " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
            " |      The resulting tensor is a view on the original tensor.\n",
            " |      \n",
            " |      All dimension names of :attr:`self` must be present in ``other.names``.\n",
            " |      :attr:`other` may contain named dimensions that are not in ``self.names``;\n",
            " |      the output tensor has a size-one dimension for each of those new names.\n",
            " |      \n",
            " |      To align a tensor to a specific order, use :meth:`~Tensor.align_to`.\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          # Example 1: Applying a mask\n",
            " |          >>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n",
            " |          >>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n",
            " |          >>> imgs.masked_fill_(mask.align_as(imgs), 0)\n",
            " |      \n",
            " |      \n",
            " |          # Example 2: Applying a per-channel-scale\n",
            " |          >>> def scale_channels(input, scale):\n",
            " |          >>>    scale = scale.refine_names('C')\n",
            " |          >>>    return input * scale.align_as(input)\n",
            " |      \n",
            " |          >>> num_channels = 3\n",
            " |          >>> scale = torch.randn(num_channels, names=('C',))\n",
            " |          >>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n",
            " |          >>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n",
            " |          >>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n",
            " |      \n",
            " |          # scale_channels is agnostic to the dimension order of the input\n",
            " |          >>> scale_channels(imgs, scale)\n",
            " |          >>> scale_channels(more_imgs, scale)\n",
            " |          >>> scale_channels(videos, scale)\n",
            " |      \n",
            " |      .. warning::\n",
            " |          The named tensor API is experimental and subject to change.\n",
            " |  \n",
            " |  all(...)\n",
            " |      all(dim=None, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.all`\n",
            " |  \n",
            " |  allclose(...)\n",
            " |      allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.allclose`\n",
            " |  \n",
            " |  amax(...)\n",
            " |      amax(dim=None, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.amax`\n",
            " |  \n",
            " |  amin(...)\n",
            " |      amin(dim=None, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.amin`\n",
            " |  \n",
            " |  aminmax(...)\n",
            " |      aminmax(*, dim=None, keepdim=False) -> (Tensor min, Tensor max)\n",
            " |      \n",
            " |      See :func:`torch.aminmax`\n",
            " |  \n",
            " |  angle(...)\n",
            " |      angle() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.angle`\n",
            " |  \n",
            " |  any(...)\n",
            " |      any(dim=None, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.any`\n",
            " |  \n",
            " |  apply_(...)\n",
            " |      apply_(callable) -> Tensor\n",
            " |      \n",
            " |      Applies the function :attr:`callable` to each element in the tensor, replacing\n",
            " |      each element with the value returned by :attr:`callable`.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          This function only works with CPU tensors and should not be used in code\n",
            " |          sections that require high performance.\n",
            " |  \n",
            " |  arccos(...)\n",
            " |      arccos() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.arccos`\n",
            " |  \n",
            " |  arccos_(...)\n",
            " |      arccos_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.arccos`\n",
            " |  \n",
            " |  arccosh(...)\n",
            " |      acosh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.arccosh`\n",
            " |  \n",
            " |  arccosh_(...)\n",
            " |      acosh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.arccosh`\n",
            " |  \n",
            " |  arcsin(...)\n",
            " |      arcsin() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.arcsin`\n",
            " |  \n",
            " |  arcsin_(...)\n",
            " |      arcsin_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.arcsin`\n",
            " |  \n",
            " |  arcsinh(...)\n",
            " |      arcsinh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.arcsinh`\n",
            " |  \n",
            " |  arcsinh_(...)\n",
            " |      arcsinh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.arcsinh`\n",
            " |  \n",
            " |  arctan(...)\n",
            " |      arctan() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.arctan`\n",
            " |  \n",
            " |  arctan_(...)\n",
            " |      arctan_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.arctan`\n",
            " |  \n",
            " |  arctanh(...)\n",
            " |      arctanh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.arctanh`\n",
            " |  \n",
            " |  arctanh_(...)\n",
            " |      arctanh_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.arctanh`\n",
            " |  \n",
            " |  argmax(...)\n",
            " |      argmax(dim=None, keepdim=False) -> LongTensor\n",
            " |      \n",
            " |      See :func:`torch.argmax`\n",
            " |  \n",
            " |  argmin(...)\n",
            " |      argmin(dim=None, keepdim=False) -> LongTensor\n",
            " |      \n",
            " |      See :func:`torch.argmin`\n",
            " |  \n",
            " |  argsort(...)\n",
            " |      argsort(dim=-1, descending=False) -> LongTensor\n",
            " |      \n",
            " |      See :func:`torch.argsort`\n",
            " |  \n",
            " |  as_strided(...)\n",
            " |      as_strided(size, stride, storage_offset=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.as_strided`\n",
            " |  \n",
            " |  as_strided_(...)\n",
            " |  \n",
            " |  as_subclass(...)\n",
            " |      as_subclass(cls) -> Tensor\n",
            " |      \n",
            " |      Makes a ``cls`` instance with the same data pointer as ``self``. Changes\n",
            " |      in the output mirror changes in ``self``, and the output stays attached\n",
            " |      to the autograd graph. ``cls`` must be a subclass of ``Tensor``.\n",
            " |  \n",
            " |  asin(...)\n",
            " |      asin() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.asin`\n",
            " |  \n",
            " |  asin_(...)\n",
            " |      asin_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.asin`\n",
            " |  \n",
            " |  asinh(...)\n",
            " |      asinh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.asinh`\n",
            " |  \n",
            " |  asinh_(...)\n",
            " |      asinh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.asinh`\n",
            " |  \n",
            " |  atan(...)\n",
            " |      atan() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.atan`\n",
            " |  \n",
            " |  atan2(...)\n",
            " |      atan2(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.atan2`\n",
            " |  \n",
            " |  atan2_(...)\n",
            " |      atan2_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.atan2`\n",
            " |  \n",
            " |  atan_(...)\n",
            " |      atan_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.atan`\n",
            " |  \n",
            " |  atanh(...)\n",
            " |      atanh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.atanh`\n",
            " |  \n",
            " |  atanh_(...)\n",
            " |      atanh_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.atanh`\n",
            " |  \n",
            " |  baddbmm(...)\n",
            " |      baddbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.baddbmm`\n",
            " |  \n",
            " |  baddbmm_(...)\n",
            " |      baddbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.baddbmm`\n",
            " |  \n",
            " |  bernoulli(...)\n",
            " |      bernoulli(*, generator=None) -> Tensor\n",
            " |      \n",
            " |      Returns a result tensor where each :math:`\\texttt{result[i]}` is independently\n",
            " |      sampled from :math:`\\text{Bernoulli}(\\texttt{self[i]})`. :attr:`self` must have\n",
            " |      floating point ``dtype``, and the result will have the same ``dtype``.\n",
            " |      \n",
            " |      See :func:`torch.bernoulli`\n",
            " |  \n",
            " |  bernoulli_(...)\n",
            " |      bernoulli_(p=0.5, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      Fills each location of :attr:`self` with an independent sample from\n",
            " |      :math:`\\text{Bernoulli}(\\texttt{p})`. :attr:`self` can have integral\n",
            " |      ``dtype``.\n",
            " |      \n",
            " |      :attr:`p` should either be a scalar or tensor containing probabilities to be\n",
            " |      used for drawing the binary random number.\n",
            " |      \n",
            " |      If it is a tensor, the :math:`\\text{i}^{th}` element of :attr:`self` tensor\n",
            " |      will be set to a value sampled from\n",
            " |      :math:`\\text{Bernoulli}(\\texttt{p\\_tensor[i]})`. In this case `p` must have\n",
            " |      floating point ``dtype``.\n",
            " |      \n",
            " |      See also :meth:`~Tensor.bernoulli` and :func:`torch.bernoulli`\n",
            " |  \n",
            " |  bfloat16(...)\n",
            " |      bfloat16(memory_format=torch.preserve_format) -> Tensor\n",
            " |      ``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  bincount(...)\n",
            " |      bincount(weights=None, minlength=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bincount`\n",
            " |  \n",
            " |  bitwise_and(...)\n",
            " |      bitwise_and() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bitwise_and`\n",
            " |  \n",
            " |  bitwise_and_(...)\n",
            " |      bitwise_and_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.bitwise_and`\n",
            " |  \n",
            " |  bitwise_left_shift(...)\n",
            " |      bitwise_left_shift(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bitwise_left_shift`\n",
            " |  \n",
            " |  bitwise_left_shift_(...)\n",
            " |      bitwise_left_shift_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.bitwise_left_shift`\n",
            " |  \n",
            " |  bitwise_not(...)\n",
            " |      bitwise_not() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bitwise_not`\n",
            " |  \n",
            " |  bitwise_not_(...)\n",
            " |      bitwise_not_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.bitwise_not`\n",
            " |  \n",
            " |  bitwise_or(...)\n",
            " |      bitwise_or() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bitwise_or`\n",
            " |  \n",
            " |  bitwise_or_(...)\n",
            " |      bitwise_or_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.bitwise_or`\n",
            " |  \n",
            " |  bitwise_right_shift(...)\n",
            " |      bitwise_right_shift(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bitwise_right_shift`\n",
            " |  \n",
            " |  bitwise_right_shift_(...)\n",
            " |      bitwise_right_shift_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.bitwise_right_shift`\n",
            " |  \n",
            " |  bitwise_xor(...)\n",
            " |      bitwise_xor() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bitwise_xor`\n",
            " |  \n",
            " |  bitwise_xor_(...)\n",
            " |      bitwise_xor_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.bitwise_xor`\n",
            " |  \n",
            " |  bmm(...)\n",
            " |      bmm(batch2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.bmm`\n",
            " |  \n",
            " |  bool(...)\n",
            " |      bool(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  broadcast_to(...)\n",
            " |      broadcast_to(shape) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.broadcast_to`.\n",
            " |  \n",
            " |  byte(...)\n",
            " |      byte(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  cauchy_(...)\n",
            " |      cauchy_(median=0, sigma=1, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      Fills the tensor with numbers drawn from the Cauchy distribution:\n",
            " |      \n",
            " |      .. math::\n",
            " |      \n",
            " |          f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}\n",
            " |  \n",
            " |  cdouble(...)\n",
            " |      cdouble(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  ceil(...)\n",
            " |      ceil() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.ceil`\n",
            " |  \n",
            " |  ceil_(...)\n",
            " |      ceil_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.ceil`\n",
            " |  \n",
            " |  cfloat(...)\n",
            " |      cfloat(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  char(...)\n",
            " |      char(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  cholesky(...)\n",
            " |      cholesky(upper=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cholesky`\n",
            " |  \n",
            " |  cholesky_inverse(...)\n",
            " |      cholesky_inverse(upper=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cholesky_inverse`\n",
            " |  \n",
            " |  cholesky_solve(...)\n",
            " |      cholesky_solve(input2, upper=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cholesky_solve`\n",
            " |  \n",
            " |  chunk(...)\n",
            " |      chunk(chunks, dim=0) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.chunk`\n",
            " |  \n",
            " |  clamp(...)\n",
            " |      clamp(min=None, max=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.clamp`\n",
            " |  \n",
            " |  clamp_(...)\n",
            " |      clamp_(min=None, max=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.clamp`\n",
            " |  \n",
            " |  clamp_max(...)\n",
            " |  \n",
            " |  clamp_max_(...)\n",
            " |  \n",
            " |  clamp_min(...)\n",
            " |  \n",
            " |  clamp_min_(...)\n",
            " |  \n",
            " |  clip(...)\n",
            " |      clip(min=None, max=None) -> Tensor\n",
            " |      \n",
            " |      Alias for :meth:`~Tensor.clamp`.\n",
            " |  \n",
            " |  clip_(...)\n",
            " |      clip_(min=None, max=None) -> Tensor\n",
            " |      \n",
            " |      Alias for :meth:`~Tensor.clamp_`.\n",
            " |  \n",
            " |  clone(...)\n",
            " |      clone(*, memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.clone`\n",
            " |  \n",
            " |  coalesce(...)\n",
            " |      coalesce() -> Tensor\n",
            " |      \n",
            " |      Returns a coalesced copy of :attr:`self` if :attr:`self` is an\n",
            " |      :ref:`uncoalesced tensor <sparse-uncoalesced-coo-docs>`.\n",
            " |      \n",
            " |      Returns :attr:`self` if :attr:`self` is a coalesced tensor.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
            " |  \n",
            " |  col_indices(...)\n",
            " |      col_indices() -> IntTensor\n",
            " |      \n",
            " |      Returns the tensor containing the column indices of the :attr:`self`\n",
            " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
            " |      The ``col_indices`` tensor is strictly of shape (:attr:`self`.nnz())\n",
            " |      and of type ``int32`` or ``int64``.  When using MKL routines such as sparse\n",
            " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
            " |      to avoid downcasting and potentially losing information.\n",
            " |      \n",
            " |      Example::\n",
            " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
            " |          >>> csr.col_indices()\n",
            " |          tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n",
            " |  \n",
            " |  conj(...)\n",
            " |      conj() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.conj`\n",
            " |  \n",
            " |  conj_physical(...)\n",
            " |      conj_physical() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.conj_physical`\n",
            " |  \n",
            " |  conj_physical_(...)\n",
            " |      conj_physical_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.conj_physical`\n",
            " |  \n",
            " |  contiguous(...)\n",
            " |      contiguous(memory_format=torch.contiguous_format) -> Tensor\n",
            " |      \n",
            " |      Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If\n",
            " |      :attr:`self` tensor is already in the specified memory format, this function returns the\n",
            " |      :attr:`self` tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.contiguous_format``.\n",
            " |  \n",
            " |  copy_(...)\n",
            " |      copy_(src, non_blocking=False) -> Tensor\n",
            " |      \n",
            " |      Copies the elements from :attr:`src` into :attr:`self` tensor and returns\n",
            " |      :attr:`self`.\n",
            " |      \n",
            " |      The :attr:`src` tensor must be :ref:`broadcastable <broadcasting-semantics>`\n",
            " |      with the :attr:`self` tensor. It may be of a different data type or reside on a\n",
            " |      different device.\n",
            " |      \n",
            " |      Args:\n",
            " |          src (Tensor): the source tensor to copy from\n",
            " |          non_blocking (bool): if ``True`` and this copy is between CPU and GPU,\n",
            " |              the copy may occur asynchronously with respect to the host. For other\n",
            " |              cases, this argument has no effect.\n",
            " |  \n",
            " |  copysign(...)\n",
            " |      copysign(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.copysign`\n",
            " |  \n",
            " |  copysign_(...)\n",
            " |      copysign_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.copysign`\n",
            " |  \n",
            " |  corrcoef(...)\n",
            " |      corrcoef() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.corrcoef`\n",
            " |  \n",
            " |  cos(...)\n",
            " |      cos() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cos`\n",
            " |  \n",
            " |  cos_(...)\n",
            " |      cos_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.cos`\n",
            " |  \n",
            " |  cosh(...)\n",
            " |      cosh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cosh`\n",
            " |  \n",
            " |  cosh_(...)\n",
            " |      cosh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.cosh`\n",
            " |  \n",
            " |  count_nonzero(...)\n",
            " |      count_nonzero(dim=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.count_nonzero`\n",
            " |  \n",
            " |  cov(...)\n",
            " |      cov(*, correction=1, fweights=None, aweights=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cov`\n",
            " |  \n",
            " |  cpu(...)\n",
            " |      cpu(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      Returns a copy of this object in CPU memory.\n",
            " |      \n",
            " |      If this object is already in CPU memory and on the correct device,\n",
            " |      then no copy is performed and the original object is returned.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  cross(...)\n",
            " |      cross(other, dim=-1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cross`\n",
            " |  \n",
            " |  crow_indices(...)\n",
            " |      crow_indices() -> IntTensor\n",
            " |      \n",
            " |      Returns the tensor containing the compressed row indices of the :attr:`self`\n",
            " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
            " |      The ``crow_indices`` tensor is strictly of shape (:attr:`self`.size(0) + 1)\n",
            " |      and of type ``int32`` or ``int64``. When using MKL routines such as sparse\n",
            " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
            " |      to avoid downcasting and potentially losing information.\n",
            " |      \n",
            " |      Example::\n",
            " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
            " |          >>> csr.crow_indices()\n",
            " |          tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n",
            " |  \n",
            " |  cuda(...)\n",
            " |      cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      Returns a copy of this object in CUDA memory.\n",
            " |      \n",
            " |      If this object is already in CUDA memory and on the correct device,\n",
            " |      then no copy is performed and the original object is returned.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The destination GPU device.\n",
            " |              Defaults to the current CUDA device.\n",
            " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
            " |              the copy will be asynchronous with respect to the host.\n",
            " |              Otherwise, the argument has no effect. Default: ``False``.\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  cummax(...)\n",
            " |      cummax(dim) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.cummax`\n",
            " |  \n",
            " |  cummin(...)\n",
            " |      cummin(dim) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.cummin`\n",
            " |  \n",
            " |  cumprod(...)\n",
            " |      cumprod(dim, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cumprod`\n",
            " |  \n",
            " |  cumprod_(...)\n",
            " |      cumprod_(dim, dtype=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.cumprod`\n",
            " |  \n",
            " |  cumsum(...)\n",
            " |      cumsum(dim, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.cumsum`\n",
            " |  \n",
            " |  cumsum_(...)\n",
            " |      cumsum_(dim, dtype=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.cumsum`\n",
            " |  \n",
            " |  data_ptr(...)\n",
            " |      data_ptr() -> int\n",
            " |      \n",
            " |      Returns the address of the first element of :attr:`self` tensor.\n",
            " |  \n",
            " |  deg2rad(...)\n",
            " |      deg2rad() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.deg2rad`\n",
            " |  \n",
            " |  deg2rad_(...)\n",
            " |      deg2rad_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.deg2rad`\n",
            " |  \n",
            " |  dense_dim(...)\n",
            " |      dense_dim() -> int\n",
            " |      \n",
            " |      Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
            " |      \n",
            " |      See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
            " |  \n",
            " |  dequantize(...)\n",
            " |      dequantize() -> Tensor\n",
            " |      \n",
            " |      Given a quantized Tensor, dequantize it and return the dequantized float Tensor.\n",
            " |  \n",
            " |  det(...)\n",
            " |      det() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.det`\n",
            " |  \n",
            " |  diag(...)\n",
            " |      diag(diagonal=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.diag`\n",
            " |  \n",
            " |  diag_embed(...)\n",
            " |      diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.diag_embed`\n",
            " |  \n",
            " |  diagflat(...)\n",
            " |      diagflat(offset=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.diagflat`\n",
            " |  \n",
            " |  diagonal(...)\n",
            " |      diagonal(offset=0, dim1=0, dim2=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.diagonal`\n",
            " |  \n",
            " |  diff(...)\n",
            " |      diff(n=1, dim=-1, prepend=None, append=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.diff`\n",
            " |  \n",
            " |  digamma(...)\n",
            " |      digamma() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.digamma`\n",
            " |  \n",
            " |  digamma_(...)\n",
            " |      digamma_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.digamma`\n",
            " |  \n",
            " |  dim(...)\n",
            " |      dim() -> int\n",
            " |      \n",
            " |      Returns the number of dimensions of :attr:`self` tensor.\n",
            " |  \n",
            " |  dist(...)\n",
            " |      dist(other, p=2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.dist`\n",
            " |  \n",
            " |  div(...)\n",
            " |      div(value, *, rounding_mode=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.div`\n",
            " |  \n",
            " |  div_(...)\n",
            " |      div_(value, *, rounding_mode=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.div`\n",
            " |  \n",
            " |  divide(...)\n",
            " |      divide(value, *, rounding_mode=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.divide`\n",
            " |  \n",
            " |  divide_(...)\n",
            " |      divide_(value, *, rounding_mode=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.divide`\n",
            " |  \n",
            " |  dot(...)\n",
            " |      dot(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.dot`\n",
            " |  \n",
            " |  double(...)\n",
            " |      double(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  dsplit(...)\n",
            " |      dsplit(split_size_or_sections) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.dsplit`\n",
            " |  \n",
            " |  eig(...)\n",
            " |      eig(eigenvectors=False) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.eig`\n",
            " |  \n",
            " |  element_size(...)\n",
            " |      element_size() -> int\n",
            " |      \n",
            " |      Returns the size in bytes of an individual element.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> torch.tensor([]).element_size()\n",
            " |          4\n",
            " |          >>> torch.tensor([], dtype=torch.uint8).element_size()\n",
            " |          1\n",
            " |  \n",
            " |  eq(...)\n",
            " |      eq(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.eq`\n",
            " |  \n",
            " |  eq_(...)\n",
            " |      eq_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.eq`\n",
            " |  \n",
            " |  equal(...)\n",
            " |      equal(other) -> bool\n",
            " |      \n",
            " |      See :func:`torch.equal`\n",
            " |  \n",
            " |  erf(...)\n",
            " |      erf() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.erf`\n",
            " |  \n",
            " |  erf_(...)\n",
            " |      erf_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.erf`\n",
            " |  \n",
            " |  erfc(...)\n",
            " |      erfc() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.erfc`\n",
            " |  \n",
            " |  erfc_(...)\n",
            " |      erfc_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.erfc`\n",
            " |  \n",
            " |  erfinv(...)\n",
            " |      erfinv() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.erfinv`\n",
            " |  \n",
            " |  erfinv_(...)\n",
            " |      erfinv_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.erfinv`\n",
            " |  \n",
            " |  exp(...)\n",
            " |      exp() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.exp`\n",
            " |  \n",
            " |  exp2(...)\n",
            " |      exp2() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.exp2`\n",
            " |  \n",
            " |  exp2_(...)\n",
            " |      exp2_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.exp2`\n",
            " |  \n",
            " |  exp_(...)\n",
            " |      exp_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.exp`\n",
            " |  \n",
            " |  expand(...)\n",
            " |      expand(*sizes) -> Tensor\n",
            " |      \n",
            " |      Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n",
            " |      to a larger size.\n",
            " |      \n",
            " |      Passing -1 as the size for a dimension means not changing the size of\n",
            " |      that dimension.\n",
            " |      \n",
            " |      Tensor can be also expanded to a larger number of dimensions, and the\n",
            " |      new ones will be appended at the front. For the new dimensions, the\n",
            " |      size cannot be set to -1.\n",
            " |      \n",
            " |      Expanding a tensor does not allocate new memory, but only creates a\n",
            " |      new view on the existing tensor where a dimension of size one is\n",
            " |      expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
            " |      of size 1 can be expanded to an arbitrary value without allocating new\n",
            " |      memory.\n",
            " |      \n",
            " |      Args:\n",
            " |          *sizes (torch.Size or int...): the desired expanded size\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          More than one element of an expanded tensor may refer to a single\n",
            " |          memory location. As a result, in-place operations (especially ones that\n",
            " |          are vectorized) may result in incorrect behavior. If you need to write\n",
            " |          to the tensors, please clone them first.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([[1], [2], [3]])\n",
            " |          >>> x.size()\n",
            " |          torch.Size([3, 1])\n",
            " |          >>> x.expand(3, 4)\n",
            " |          tensor([[ 1,  1,  1,  1],\n",
            " |                  [ 2,  2,  2,  2],\n",
            " |                  [ 3,  3,  3,  3]])\n",
            " |          >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
            " |          tensor([[ 1,  1,  1,  1],\n",
            " |                  [ 2,  2,  2,  2],\n",
            " |                  [ 3,  3,  3,  3]])\n",
            " |  \n",
            " |  expand_as(...)\n",
            " |      expand_as(other) -> Tensor\n",
            " |      \n",
            " |      Expand this tensor to the same size as :attr:`other`.\n",
            " |      ``self.expand_as(other)`` is equivalent to ``self.expand(other.size())``.\n",
            " |      \n",
            " |      Please see :meth:`~Tensor.expand` for more information about ``expand``.\n",
            " |      \n",
            " |      Args:\n",
            " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
            " |              as :attr:`other`.\n",
            " |  \n",
            " |  expm1(...)\n",
            " |      expm1() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.expm1`\n",
            " |  \n",
            " |  expm1_(...)\n",
            " |      expm1_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.expm1`\n",
            " |  \n",
            " |  exponential_(...)\n",
            " |      exponential_(lambd=1, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with elements drawn from the exponential distribution:\n",
            " |      \n",
            " |      .. math::\n",
            " |      \n",
            " |          f(x) = \\lambda e^{-\\lambda x}\n",
            " |  \n",
            " |  fill_(...)\n",
            " |      fill_(value) -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with the specified value.\n",
            " |  \n",
            " |  fill_diagonal_(...)\n",
            " |      fill_diagonal_(fill_value, wrap=False) -> Tensor\n",
            " |      \n",
            " |      Fill the main diagonal of a tensor that has at least 2-dimensions.\n",
            " |      When dims>2, all dimensions of input must be of equal length.\n",
            " |      This function modifies the input tensor in-place, and returns the input tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          fill_value (Scalar): the fill value\n",
            " |          wrap (bool): the diagonal 'wrapped' after N columns for tall matrices.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> a = torch.zeros(3, 3)\n",
            " |          >>> a.fill_diagonal_(5)\n",
            " |          tensor([[5., 0., 0.],\n",
            " |                  [0., 5., 0.],\n",
            " |                  [0., 0., 5.]])\n",
            " |          >>> b = torch.zeros(7, 3)\n",
            " |          >>> b.fill_diagonal_(5)\n",
            " |          tensor([[5., 0., 0.],\n",
            " |                  [0., 5., 0.],\n",
            " |                  [0., 0., 5.],\n",
            " |                  [0., 0., 0.],\n",
            " |                  [0., 0., 0.],\n",
            " |                  [0., 0., 0.],\n",
            " |                  [0., 0., 0.]])\n",
            " |          >>> c = torch.zeros(7, 3)\n",
            " |          >>> c.fill_diagonal_(5, wrap=True)\n",
            " |          tensor([[5., 0., 0.],\n",
            " |                  [0., 5., 0.],\n",
            " |                  [0., 0., 5.],\n",
            " |                  [0., 0., 0.],\n",
            " |                  [5., 0., 0.],\n",
            " |                  [0., 5., 0.],\n",
            " |                  [0., 0., 5.]])\n",
            " |  \n",
            " |  fix(...)\n",
            " |      fix() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.fix`.\n",
            " |  \n",
            " |  fix_(...)\n",
            " |      fix_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.fix`\n",
            " |  \n",
            " |  flatten(...)\n",
            " |      flatten(start_dim=0, end_dim=-1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.flatten`\n",
            " |  \n",
            " |  flip(...)\n",
            " |      flip(dims) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.flip`\n",
            " |  \n",
            " |  fliplr(...)\n",
            " |      fliplr() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.fliplr`\n",
            " |  \n",
            " |  flipud(...)\n",
            " |      flipud() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.flipud`\n",
            " |  \n",
            " |  float(...)\n",
            " |      float(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  float_power(...)\n",
            " |      float_power(exponent) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.float_power`\n",
            " |  \n",
            " |  float_power_(...)\n",
            " |      float_power_(exponent) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.float_power`\n",
            " |  \n",
            " |  floor(...)\n",
            " |      floor() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.floor`\n",
            " |  \n",
            " |  floor_(...)\n",
            " |      floor_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.floor`\n",
            " |  \n",
            " |  floor_divide(...)\n",
            " |      floor_divide(value) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.floor_divide`\n",
            " |  \n",
            " |  floor_divide_(...)\n",
            " |      floor_divide_(value) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.floor_divide`\n",
            " |  \n",
            " |  fmax(...)\n",
            " |      fmax(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.fmax`\n",
            " |  \n",
            " |  fmin(...)\n",
            " |      fmin(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.fmin`\n",
            " |  \n",
            " |  fmod(...)\n",
            " |      fmod(divisor) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.fmod`\n",
            " |  \n",
            " |  fmod_(...)\n",
            " |      fmod_(divisor) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.fmod`\n",
            " |  \n",
            " |  frac(...)\n",
            " |      frac() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.frac`\n",
            " |  \n",
            " |  frac_(...)\n",
            " |      frac_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.frac`\n",
            " |  \n",
            " |  frexp(...)\n",
            " |      frexp(input) -> (Tensor mantissa, Tensor exponent)\n",
            " |      \n",
            " |      See :func:`torch.frexp`\n",
            " |  \n",
            " |  gather(...)\n",
            " |      gather(dim, index) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.gather`\n",
            " |  \n",
            " |  gcd(...)\n",
            " |      gcd(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.gcd`\n",
            " |  \n",
            " |  gcd_(...)\n",
            " |      gcd_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.gcd`\n",
            " |  \n",
            " |  ge(...)\n",
            " |      ge(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.ge`.\n",
            " |  \n",
            " |  ge_(...)\n",
            " |      ge_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.ge`.\n",
            " |  \n",
            " |  geometric_(...)\n",
            " |      geometric_(p, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with elements drawn from the geometric distribution:\n",
            " |      \n",
            " |      .. math::\n",
            " |      \n",
            " |          f(X=k) = p^{k - 1} (1 - p)\n",
            " |  \n",
            " |  geqrf(...)\n",
            " |      geqrf() -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.geqrf`\n",
            " |  \n",
            " |  ger(...)\n",
            " |      ger(vec2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.ger`\n",
            " |  \n",
            " |  get_device(...)\n",
            " |      get_device() -> Device ordinal (Integer)\n",
            " |      \n",
            " |      For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\n",
            " |      For CPU tensors, an error is thrown.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.randn(3, 4, 5, device='cuda:0')\n",
            " |          >>> x.get_device()\n",
            " |          0\n",
            " |          >>> x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor\n",
            " |  \n",
            " |  greater(...)\n",
            " |      greater(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.greater`.\n",
            " |  \n",
            " |  greater_(...)\n",
            " |      greater_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.greater`.\n",
            " |  \n",
            " |  greater_equal(...)\n",
            " |      greater_equal(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.greater_equal`.\n",
            " |  \n",
            " |  greater_equal_(...)\n",
            " |      greater_equal_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.greater_equal`.\n",
            " |  \n",
            " |  gt(...)\n",
            " |      gt(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.gt`.\n",
            " |  \n",
            " |  gt_(...)\n",
            " |      gt_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.gt`.\n",
            " |  \n",
            " |  half(...)\n",
            " |      half(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  hardshrink(...)\n",
            " |      hardshrink(lambd=0.5) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.nn.functional.hardshrink`\n",
            " |  \n",
            " |  has_names(...)\n",
            " |      Is ``True`` if any of this tensor's dimensions are named. Otherwise, is ``False``.\n",
            " |  \n",
            " |  heaviside(...)\n",
            " |      heaviside(values) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.heaviside`\n",
            " |  \n",
            " |  heaviside_(...)\n",
            " |      heaviside_(values) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.heaviside`\n",
            " |  \n",
            " |  histc(...)\n",
            " |      histc(bins=100, min=0, max=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.histc`\n",
            " |  \n",
            " |  histogram(...)\n",
            " |      histogram(input, bins, *, range=None, weight=None, density=False) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.histogram`\n",
            " |  \n",
            " |  hsplit(...)\n",
            " |      hsplit(split_size_or_sections) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.hsplit`\n",
            " |  \n",
            " |  hypot(...)\n",
            " |      hypot(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.hypot`\n",
            " |  \n",
            " |  hypot_(...)\n",
            " |      hypot_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.hypot`\n",
            " |  \n",
            " |  i0(...)\n",
            " |      i0() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.i0`\n",
            " |  \n",
            " |  i0_(...)\n",
            " |      i0_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.i0`\n",
            " |  \n",
            " |  igamma(...)\n",
            " |      igamma(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.igamma`\n",
            " |  \n",
            " |  igamma_(...)\n",
            " |      igamma_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.igamma`\n",
            " |  \n",
            " |  igammac(...)\n",
            " |      igammac(other) -> Tensor\n",
            " |      See :func:`torch.igammac`\n",
            " |  \n",
            " |  igammac_(...)\n",
            " |      igammac_(other) -> Tensor\n",
            " |      In-place version of :meth:`~Tensor.igammac`\n",
            " |  \n",
            " |  index_add(...)\n",
            " |      index_add(dim, index, tensor2) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.index_add_`.\n",
            " |  \n",
            " |  index_add_(...)\n",
            " |      index_add_(dim, index, tensor, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      Accumulate the elements of :attr:`alpha` times :attr:`tensor` into the :attr:`self`\n",
            " |      tensor by adding to the indices in the order given in :attr:`index`. For example,\n",
            " |      if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\\ th row of\n",
            " |      :attr:`tensor` is subtracted from the ``j``\\ th row of :attr:`self`.\n",
            " |      \n",
            " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
            " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
            " |      match :attr:`self`, or an error will be raised.\n",
            " |      \n",
            " |      Note:\n",
            " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int): dimension along which to index\n",
            " |          index (IntTensor or LongTensor): indices of :attr:`tensor` to select from\n",
            " |          tensor (Tensor): the tensor containing values to add\n",
            " |      \n",
            " |      Keyword args:\n",
            " |          alpha (Number): the scalar multiplier for :attr:`tensor`\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.ones(5, 3)\n",
            " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
            " |          >>> index = torch.tensor([0, 4, 2])\n",
            " |          >>> x.index_add_(0, index, t)\n",
            " |          tensor([[  2.,   3.,   4.],\n",
            " |                  [  1.,   1.,   1.],\n",
            " |                  [  8.,   9.,  10.],\n",
            " |                  [  1.,   1.,   1.],\n",
            " |                  [  5.,   6.,   7.]])\n",
            " |          >>> x.index_add_(0, index, t, alpha=-1)\n",
            " |          tensor([[  1.,   1.,   1.],\n",
            " |                  [  1.,   1.,   1.],\n",
            " |                  [  1.,   1.,   1.],\n",
            " |                  [  1.,   1.,   1.],\n",
            " |                  [  1.,   1.,   1.]])\n",
            " |  \n",
            " |  index_copy(...)\n",
            " |      index_copy(dim, index, tensor2) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.index_copy_`.\n",
            " |  \n",
            " |  index_copy_(...)\n",
            " |      index_copy_(dim, index, tensor) -> Tensor\n",
            " |      \n",
            " |      Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting\n",
            " |      the indices in the order given in :attr:`index`. For example, if ``dim == 0``\n",
            " |      and ``index[i] == j``, then the ``i``\\ th row of :attr:`tensor` is copied to the\n",
            " |      ``j``\\ th row of :attr:`self`.\n",
            " |      \n",
            " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
            " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
            " |      match :attr:`self`, or an error will be raised.\n",
            " |      \n",
            " |      .. note::\n",
            " |          If :attr:`index` contains duplicate entries, multiple elements from\n",
            " |          :attr:`tensor` will be copied to the same index of :attr:`self`. The result\n",
            " |          is nondeterministic since it depends on which copy occurs last.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int): dimension along which to index\n",
            " |          index (LongTensor): indices of :attr:`tensor` to select from\n",
            " |          tensor (Tensor): the tensor containing values to copy\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.zeros(5, 3)\n",
            " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
            " |          >>> index = torch.tensor([0, 4, 2])\n",
            " |          >>> x.index_copy_(0, index, t)\n",
            " |          tensor([[ 1.,  2.,  3.],\n",
            " |                  [ 0.,  0.,  0.],\n",
            " |                  [ 7.,  8.,  9.],\n",
            " |                  [ 0.,  0.,  0.],\n",
            " |                  [ 4.,  5.,  6.]])\n",
            " |  \n",
            " |  index_fill(...)\n",
            " |      index_fill(dim, index, value) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.index_fill_`.\n",
            " |  \n",
            " |  index_fill_(...)\n",
            " |      index_fill_(dim, index, value) -> Tensor\n",
            " |      \n",
            " |      Fills the elements of the :attr:`self` tensor with value :attr:`value` by\n",
            " |      selecting the indices in the order given in :attr:`index`.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int): dimension along which to index\n",
            " |          index (LongTensor): indices of :attr:`self` tensor to fill in\n",
            " |          value (float): the value to fill with\n",
            " |      \n",
            " |      Example::\n",
            " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
            " |          >>> index = torch.tensor([0, 2])\n",
            " |          >>> x.index_fill_(1, index, -1)\n",
            " |          tensor([[-1.,  2., -1.],\n",
            " |                  [-1.,  5., -1.],\n",
            " |                  [-1.,  8., -1.]])\n",
            " |  \n",
            " |  index_put(...)\n",
            " |      index_put(indices, values, accumulate=False) -> Tensor\n",
            " |      \n",
            " |      Out-place version of :meth:`~Tensor.index_put_`.\n",
            " |  \n",
            " |  index_put_(...)\n",
            " |      index_put_(indices, values, accumulate=False) -> Tensor\n",
            " |      \n",
            " |      Puts values from the tensor :attr:`values` into the tensor :attr:`self` using\n",
            " |      the indices specified in :attr:`indices` (which is a tuple of Tensors). The\n",
            " |      expression ``tensor.index_put_(indices, values)`` is equivalent to\n",
            " |      ``tensor[indices] = values``. Returns :attr:`self`.\n",
            " |      \n",
            " |      If :attr:`accumulate` is ``True``, the elements in :attr:`values` are added to\n",
            " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if indices\n",
            " |      contain duplicate elements.\n",
            " |      \n",
            " |      Args:\n",
            " |          indices (tuple of LongTensor): tensors used to index into `self`.\n",
            " |          values (Tensor): tensor of same dtype as `self`.\n",
            " |          accumulate (bool): whether to accumulate into self\n",
            " |  \n",
            " |  index_select(...)\n",
            " |      index_select(dim, index) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.index_select`\n",
            " |  \n",
            " |  indices(...)\n",
            " |      indices() -> Tensor\n",
            " |      \n",
            " |      Return the indices tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
            " |      \n",
            " |      See also :meth:`Tensor.values`.\n",
            " |      \n",
            " |      .. note::\n",
            " |        This method can only be called on a coalesced sparse tensor. See\n",
            " |        :meth:`Tensor.coalesce` for details.\n",
            " |  \n",
            " |  inner(...)\n",
            " |      inner(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.inner`.\n",
            " |  \n",
            " |  int(...)\n",
            " |      int(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  int_repr(...)\n",
            " |      int_repr() -> Tensor\n",
            " |      \n",
            " |      Given a quantized Tensor,\n",
            " |      ``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the\n",
            " |      underlying uint8_t values of the given Tensor.\n",
            " |  \n",
            " |  inverse(...)\n",
            " |      inverse() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.inverse`\n",
            " |  \n",
            " |  is_coalesced(...)\n",
            " |      is_coalesced() -> bool\n",
            " |      \n",
            " |      Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor\n",
            " |      <sparse-coo-docs>` that is coalesced, ``False`` otherwise.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
            " |      \n",
            " |      See :meth:`coalesce` and :ref:`uncoalesced tensors <sparse-uncoalesced-coo-docs>`.\n",
            " |  \n",
            " |  is_complex(...)\n",
            " |      is_complex() -> bool\n",
            " |      \n",
            " |      Returns True if the data type of :attr:`self` is a complex data type.\n",
            " |  \n",
            " |  is_conj(...)\n",
            " |      is_conj() -> bool\n",
            " |      \n",
            " |      Returns True if the conjugate bit of :attr:`self` is set to true.\n",
            " |  \n",
            " |  is_contiguous(...)\n",
            " |      is_contiguous(memory_format=torch.contiguous_format) -> bool\n",
            " |      \n",
            " |      Returns True if :attr:`self` tensor is contiguous in memory in the order specified\n",
            " |      by memory format.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): Specifies memory allocation\n",
            " |              order. Default: ``torch.contiguous_format``.\n",
            " |  \n",
            " |  is_distributed(...)\n",
            " |  \n",
            " |  is_floating_point(...)\n",
            " |      is_floating_point() -> bool\n",
            " |      \n",
            " |      Returns True if the data type of :attr:`self` is a floating point data type.\n",
            " |  \n",
            " |  is_inference(...)\n",
            " |      is_inference() -> bool\n",
            " |      \n",
            " |      See :func:`torch.is_inference`\n",
            " |  \n",
            " |  is_neg(...)\n",
            " |      is_neg() -> bool\n",
            " |      \n",
            " |      Returns True if the negative bit of :attr:`self` is set to true.\n",
            " |  \n",
            " |  is_nonzero(...)\n",
            " |  \n",
            " |  is_pinned(...)\n",
            " |      Returns true if this tensor resides in pinned memory.\n",
            " |  \n",
            " |  is_same_size(...)\n",
            " |  \n",
            " |  is_set_to(...)\n",
            " |      is_set_to(tensor) -> bool\n",
            " |      \n",
            " |      Returns True if both tensors are pointing to the exact same memory (same\n",
            " |      storage, offset, size and stride).\n",
            " |  \n",
            " |  is_signed(...)\n",
            " |      is_signed() -> bool\n",
            " |      \n",
            " |      Returns True if the data type of :attr:`self` is a signed data type.\n",
            " |  \n",
            " |  isclose(...)\n",
            " |      isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isclose`\n",
            " |  \n",
            " |  isfinite(...)\n",
            " |      isfinite() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isfinite`\n",
            " |  \n",
            " |  isinf(...)\n",
            " |      isinf() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isinf`\n",
            " |  \n",
            " |  isnan(...)\n",
            " |      isnan() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isnan`\n",
            " |  \n",
            " |  isneginf(...)\n",
            " |      isneginf() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isneginf`\n",
            " |  \n",
            " |  isposinf(...)\n",
            " |      isposinf() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isposinf`\n",
            " |  \n",
            " |  isreal(...)\n",
            " |      isreal() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.isreal`\n",
            " |  \n",
            " |  item(...)\n",
            " |      item() -> number\n",
            " |      \n",
            " |      Returns the value of this tensor as a standard Python number. This only works\n",
            " |      for tensors with one element. For other cases, see :meth:`~Tensor.tolist`.\n",
            " |      \n",
            " |      This operation is not differentiable.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([1.0])\n",
            " |          >>> x.item()\n",
            " |          1.0\n",
            " |  \n",
            " |  kron(...)\n",
            " |      kron(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.kron`\n",
            " |  \n",
            " |  kthvalue(...)\n",
            " |      kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
            " |      \n",
            " |      See :func:`torch.kthvalue`\n",
            " |  \n",
            " |  lcm(...)\n",
            " |      lcm(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.lcm`\n",
            " |  \n",
            " |  lcm_(...)\n",
            " |      lcm_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.lcm`\n",
            " |  \n",
            " |  ldexp(...)\n",
            " |      ldexp(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.ldexp`\n",
            " |  \n",
            " |  ldexp_(...)\n",
            " |      ldexp_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.ldexp`\n",
            " |  \n",
            " |  le(...)\n",
            " |      le(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.le`.\n",
            " |  \n",
            " |  le_(...)\n",
            " |      le_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.le`.\n",
            " |  \n",
            " |  lerp(...)\n",
            " |      lerp(end, weight) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.lerp`\n",
            " |  \n",
            " |  lerp_(...)\n",
            " |      lerp_(end, weight) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.lerp`\n",
            " |  \n",
            " |  less(...)\n",
            " |      lt(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.less`.\n",
            " |  \n",
            " |  less_(...)\n",
            " |      less_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.less`.\n",
            " |  \n",
            " |  less_equal(...)\n",
            " |      less_equal(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.less_equal`.\n",
            " |  \n",
            " |  less_equal_(...)\n",
            " |      less_equal_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.less_equal`.\n",
            " |  \n",
            " |  lgamma(...)\n",
            " |      lgamma() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.lgamma`\n",
            " |  \n",
            " |  lgamma_(...)\n",
            " |      lgamma_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.lgamma`\n",
            " |  \n",
            " |  log(...)\n",
            " |      log() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.log`\n",
            " |  \n",
            " |  log10(...)\n",
            " |      log10() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.log10`\n",
            " |  \n",
            " |  log10_(...)\n",
            " |      log10_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.log10`\n",
            " |  \n",
            " |  log1p(...)\n",
            " |      log1p() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.log1p`\n",
            " |  \n",
            " |  log1p_(...)\n",
            " |      log1p_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.log1p`\n",
            " |  \n",
            " |  log2(...)\n",
            " |      log2() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.log2`\n",
            " |  \n",
            " |  log2_(...)\n",
            " |      log2_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.log2`\n",
            " |  \n",
            " |  log_(...)\n",
            " |      log_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.log`\n",
            " |  \n",
            " |  log_normal_(...)\n",
            " |      log_normal_(mean=1, std=2, *, generator=None)\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with numbers samples from the log-normal distribution\n",
            " |      parameterized by the given mean :math:`\\mu` and standard deviation\n",
            " |      :math:`\\sigma`. Note that :attr:`mean` and :attr:`std` are the mean and\n",
            " |      standard deviation of the underlying normal distribution, and not of the\n",
            " |      returned distribution:\n",
            " |      \n",
            " |      .. math::\n",
            " |      \n",
            " |          f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}\n",
            " |  \n",
            " |  log_softmax(...)\n",
            " |  \n",
            " |  logaddexp(...)\n",
            " |      logaddexp(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logaddexp`\n",
            " |  \n",
            " |  logaddexp2(...)\n",
            " |      logaddexp2(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logaddexp2`\n",
            " |  \n",
            " |  logcumsumexp(...)\n",
            " |      logcumsumexp(dim) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logcumsumexp`\n",
            " |  \n",
            " |  logdet(...)\n",
            " |      logdet() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logdet`\n",
            " |  \n",
            " |  logical_and(...)\n",
            " |      logical_and() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logical_and`\n",
            " |  \n",
            " |  logical_and_(...)\n",
            " |      logical_and_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.logical_and`\n",
            " |  \n",
            " |  logical_not(...)\n",
            " |      logical_not() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logical_not`\n",
            " |  \n",
            " |  logical_not_(...)\n",
            " |      logical_not_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.logical_not`\n",
            " |  \n",
            " |  logical_or(...)\n",
            " |      logical_or() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logical_or`\n",
            " |  \n",
            " |  logical_or_(...)\n",
            " |      logical_or_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.logical_or`\n",
            " |  \n",
            " |  logical_xor(...)\n",
            " |      logical_xor() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logical_xor`\n",
            " |  \n",
            " |  logical_xor_(...)\n",
            " |      logical_xor_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.logical_xor`\n",
            " |  \n",
            " |  logit(...)\n",
            " |      logit() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logit`\n",
            " |  \n",
            " |  logit_(...)\n",
            " |      logit_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.logit`\n",
            " |  \n",
            " |  logsumexp(...)\n",
            " |      logsumexp(dim, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.logsumexp`\n",
            " |  \n",
            " |  long(...)\n",
            " |      long(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  lstsq(...)\n",
            " |      lstsq(A) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.lstsq`\n",
            " |  \n",
            " |  lt(...)\n",
            " |      lt(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.lt`.\n",
            " |  \n",
            " |  lt_(...)\n",
            " |      lt_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.lt`.\n",
            " |  \n",
            " |  lu_solve(...)\n",
            " |      lu_solve(LU_data, LU_pivots) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.lu_solve`\n",
            " |  \n",
            " |  map2_(...)\n",
            " |  \n",
            " |  map_(...)\n",
            " |      map_(tensor, callable)\n",
            " |      \n",
            " |      Applies :attr:`callable` for each element in :attr:`self` tensor and the given\n",
            " |      :attr:`tensor` and stores the results in :attr:`self` tensor. :attr:`self` tensor and\n",
            " |      the given :attr:`tensor` must be :ref:`broadcastable <broadcasting-semantics>`.\n",
            " |      \n",
            " |      The :attr:`callable` should have the signature::\n",
            " |      \n",
            " |          def callable(a, b) -> number\n",
            " |  \n",
            " |  masked_fill(...)\n",
            " |      masked_fill(mask, value) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.masked_fill_`\n",
            " |  \n",
            " |  masked_fill_(...)\n",
            " |      masked_fill_(mask, value)\n",
            " |      \n",
            " |      Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is\n",
            " |      True. The shape of :attr:`mask` must be\n",
            " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
            " |      tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          mask (BoolTensor): the boolean mask\n",
            " |          value (float): the value to fill in with\n",
            " |  \n",
            " |  masked_scatter(...)\n",
            " |      masked_scatter(mask, tensor) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.masked_scatter_`\n",
            " |  \n",
            " |  masked_scatter_(...)\n",
            " |      masked_scatter_(mask, source)\n",
            " |      \n",
            " |      Copies elements from :attr:`source` into :attr:`self` tensor at positions where\n",
            " |      the :attr:`mask` is True.\n",
            " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
            " |      with the shape of the underlying tensor. The :attr:`source` should have at least\n",
            " |      as many elements as the number of ones in :attr:`mask`\n",
            " |      \n",
            " |      Args:\n",
            " |          mask (BoolTensor): the boolean mask\n",
            " |          source (Tensor): the tensor to copy from\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          The :attr:`mask` operates on the :attr:`self` tensor, not on the given\n",
            " |          :attr:`source` tensor.\n",
            " |  \n",
            " |  masked_select(...)\n",
            " |      masked_select(mask) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.masked_select`\n",
            " |  \n",
            " |  matmul(...)\n",
            " |      matmul(tensor2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.matmul`\n",
            " |  \n",
            " |  matrix_exp(...)\n",
            " |      matrix_exp() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.matrix_exp`\n",
            " |  \n",
            " |  matrix_power(...)\n",
            " |      matrix_power(n) -> Tensor\n",
            " |      \n",
            " |      .. note:: :meth:`~Tensor.matrix_power` is deprecated, use :func:`torch.linalg.matrix_power` instead.\n",
            " |      \n",
            " |      Alias for :func:`torch.linalg.matrix_power`\n",
            " |  \n",
            " |  max(...)\n",
            " |      max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.max`\n",
            " |  \n",
            " |  maximum(...)\n",
            " |      maximum(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.maximum`\n",
            " |  \n",
            " |  mean(...)\n",
            " |      mean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.mean`\n",
            " |  \n",
            " |  median(...)\n",
            " |      median(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
            " |      \n",
            " |      See :func:`torch.median`\n",
            " |  \n",
            " |  min(...)\n",
            " |      min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.min`\n",
            " |  \n",
            " |  minimum(...)\n",
            " |      minimum(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.minimum`\n",
            " |  \n",
            " |  mm(...)\n",
            " |      mm(mat2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.mm`\n",
            " |  \n",
            " |  mode(...)\n",
            " |      mode(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
            " |      \n",
            " |      See :func:`torch.mode`\n",
            " |  \n",
            " |  moveaxis(...)\n",
            " |      moveaxis(source, destination) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.moveaxis`\n",
            " |  \n",
            " |  movedim(...)\n",
            " |      movedim(source, destination) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.movedim`\n",
            " |  \n",
            " |  msort(...)\n",
            " |      msort() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.msort`\n",
            " |  \n",
            " |  mul(...)\n",
            " |      mul(value) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.mul`.\n",
            " |  \n",
            " |  mul_(...)\n",
            " |      mul_(value) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.mul`.\n",
            " |  \n",
            " |  multinomial(...)\n",
            " |      multinomial(num_samples, replacement=False, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.multinomial`\n",
            " |  \n",
            " |  multiply(...)\n",
            " |      multiply(value) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.multiply`.\n",
            " |  \n",
            " |  multiply_(...)\n",
            " |      multiply_(value) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.multiply`.\n",
            " |  \n",
            " |  mv(...)\n",
            " |      mv(vec) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.mv`\n",
            " |  \n",
            " |  mvlgamma(...)\n",
            " |      mvlgamma(p) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.mvlgamma`\n",
            " |  \n",
            " |  mvlgamma_(...)\n",
            " |      mvlgamma_(p) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.mvlgamma`\n",
            " |  \n",
            " |  nan_to_num(...)\n",
            " |      nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.nan_to_num`.\n",
            " |  \n",
            " |  nan_to_num_(...)\n",
            " |      nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.nan_to_num`.\n",
            " |  \n",
            " |  nanmean(...)\n",
            " |      nanmean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.nanmean`\n",
            " |  \n",
            " |  nanmedian(...)\n",
            " |      nanmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
            " |      \n",
            " |      See :func:`torch.nanmedian`\n",
            " |  \n",
            " |  nanquantile(...)\n",
            " |      nanquantile(q, dim=None, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.nanquantile`\n",
            " |  \n",
            " |  nansum(...)\n",
            " |      nansum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.nansum`\n",
            " |  \n",
            " |  narrow(...)\n",
            " |      narrow(dimension, start, length) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.narrow`\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
            " |          >>> x.narrow(0, 0, 2)\n",
            " |          tensor([[ 1,  2,  3],\n",
            " |                  [ 4,  5,  6]])\n",
            " |          >>> x.narrow(1, 1, 2)\n",
            " |          tensor([[ 2,  3],\n",
            " |                  [ 5,  6],\n",
            " |                  [ 8,  9]])\n",
            " |  \n",
            " |  narrow_copy(...)\n",
            " |      narrow_copy(dimension, start, length) -> Tensor\n",
            " |      \n",
            " |      Same as :meth:`Tensor.narrow` except returning a copy rather\n",
            " |      than shared storage.  This is primarily for sparse tensors, which\n",
            " |      do not have a shared-storage narrow method.  Calling ``narrow_copy``\n",
            " |      with ``dimemsion > self.sparse_dim()`` will return a copy with the\n",
            " |      relevant dense dimension narrowed, and ``self.shape`` updated accordingly.\n",
            " |  \n",
            " |  ndimension(...)\n",
            " |      ndimension() -> int\n",
            " |      \n",
            " |      Alias for :meth:`~Tensor.dim()`\n",
            " |  \n",
            " |  ne(...)\n",
            " |      ne(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.ne`.\n",
            " |  \n",
            " |  ne_(...)\n",
            " |      ne_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.ne`.\n",
            " |  \n",
            " |  neg(...)\n",
            " |      neg() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.neg`\n",
            " |  \n",
            " |  neg_(...)\n",
            " |      neg_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.neg`\n",
            " |  \n",
            " |  negative(...)\n",
            " |      negative() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.negative`\n",
            " |  \n",
            " |  negative_(...)\n",
            " |      negative_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.negative`\n",
            " |  \n",
            " |  nelement(...)\n",
            " |      nelement() -> int\n",
            " |      \n",
            " |      Alias for :meth:`~Tensor.numel`\n",
            " |  \n",
            " |  new(...)\n",
            " |  \n",
            " |  new_empty(...)\n",
            " |      new_empty(size, dtype=None, device=None, requires_grad=False) -> Tensor\n",
            " |      \n",
            " |      Returns a Tensor of size :attr:`size` filled with uninitialized data.\n",
            " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
            " |      :class:`torch.device` as this tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
            " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
            " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            " |              Default: if None, same :class:`torch.device` as this tensor.\n",
            " |          requires_grad (bool, optional): If autograd should record operations on the\n",
            " |              returned tensor. Default: ``False``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.ones(())\n",
            " |          >>> tensor.new_empty((2, 3))\n",
            " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
            " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
            " |  \n",
            " |  new_empty_strided(...)\n",
            " |      new_empty_strided(size, stride, dtype=None, device=None, requires_grad=False) -> Tensor\n",
            " |      \n",
            " |      Returns a Tensor of size :attr:`size` and strides :attr:`stride` filled with\n",
            " |      uninitialized data. By default, the returned Tensor has the same\n",
            " |      :class:`torch.dtype` and :class:`torch.device` as this tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
            " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
            " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            " |              Default: if None, same :class:`torch.device` as this tensor.\n",
            " |          requires_grad (bool, optional): If autograd should record operations on the\n",
            " |              returned tensor. Default: ``False``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.ones(())\n",
            " |          >>> tensor.new_empty_strided((2, 3), (3, 1))\n",
            " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
            " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
            " |  \n",
            " |  new_full(...)\n",
            " |      new_full(size, fill_value, dtype=None, device=None, requires_grad=False) -> Tensor\n",
            " |      \n",
            " |      Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.\n",
            " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
            " |      :class:`torch.device` as this tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          fill_value (scalar): the number to fill the output tensor with.\n",
            " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
            " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
            " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            " |              Default: if None, same :class:`torch.device` as this tensor.\n",
            " |          requires_grad (bool, optional): If autograd should record operations on the\n",
            " |              returned tensor. Default: ``False``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.ones((2,), dtype=torch.float64)\n",
            " |          >>> tensor.new_full((3, 4), 3.141592)\n",
            " |          tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n",
            " |                  [ 3.1416,  3.1416,  3.1416,  3.1416],\n",
            " |                  [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n",
            " |  \n",
            " |  new_ones(...)\n",
            " |      new_ones(size, dtype=None, device=None, requires_grad=False) -> Tensor\n",
            " |      \n",
            " |      Returns a Tensor of size :attr:`size` filled with ``1``.\n",
            " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
            " |      :class:`torch.device` as this tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
            " |              shape of the output tensor.\n",
            " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
            " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
            " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            " |              Default: if None, same :class:`torch.device` as this tensor.\n",
            " |          requires_grad (bool, optional): If autograd should record operations on the\n",
            " |              returned tensor. Default: ``False``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.tensor((), dtype=torch.int32)\n",
            " |          >>> tensor.new_ones((2, 3))\n",
            " |          tensor([[ 1,  1,  1],\n",
            " |                  [ 1,  1,  1]], dtype=torch.int32)\n",
            " |  \n",
            " |  new_tensor(...)\n",
            " |      new_tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor\n",
            " |      \n",
            " |      Returns a new Tensor with :attr:`data` as the tensor data.\n",
            " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
            " |      :class:`torch.device` as this tensor.\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          :func:`new_tensor` always copies :attr:`data`. If you have a Tensor\n",
            " |          ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
            " |          or :func:`torch.Tensor.detach`.\n",
            " |          If you have a numpy array and want to avoid a copy, use\n",
            " |          :func:`torch.from_numpy`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          When data is a tensor `x`, :func:`new_tensor()` reads out 'the data' from whatever it is passed,\n",
            " |          and constructs a leaf variable. Therefore ``tensor.new_tensor(x)`` is equivalent to ``x.clone().detach()``\n",
            " |          and ``tensor.new_tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.\n",
            " |          The equivalents using ``clone()`` and ``detach()`` are recommended.\n",
            " |      \n",
            " |      Args:\n",
            " |          data (array_like): The returned Tensor copies :attr:`data`.\n",
            " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
            " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
            " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            " |              Default: if None, same :class:`torch.device` as this tensor.\n",
            " |          requires_grad (bool, optional): If autograd should record operations on the\n",
            " |              returned tensor. Default: ``False``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.ones((2,), dtype=torch.int8)\n",
            " |          >>> data = [[0, 1], [2, 3]]\n",
            " |          >>> tensor.new_tensor(data)\n",
            " |          tensor([[ 0,  1],\n",
            " |                  [ 2,  3]], dtype=torch.int8)\n",
            " |  \n",
            " |  new_zeros(...)\n",
            " |      new_zeros(size, dtype=None, device=None, requires_grad=False) -> Tensor\n",
            " |      \n",
            " |      Returns a Tensor of size :attr:`size` filled with ``0``.\n",
            " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
            " |      :class:`torch.device` as this tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
            " |              shape of the output tensor.\n",
            " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
            " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
            " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            " |              Default: if None, same :class:`torch.device` as this tensor.\n",
            " |          requires_grad (bool, optional): If autograd should record operations on the\n",
            " |              returned tensor. Default: ``False``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.tensor((), dtype=torch.float64)\n",
            " |          >>> tensor.new_zeros((2, 3))\n",
            " |          tensor([[ 0.,  0.,  0.],\n",
            " |                  [ 0.,  0.,  0.]], dtype=torch.float64)\n",
            " |  \n",
            " |  nextafter(...)\n",
            " |      nextafter(other) -> Tensor\n",
            " |      See :func:`torch.nextafter`\n",
            " |  \n",
            " |  nextafter_(...)\n",
            " |      nextafter_(other) -> Tensor\n",
            " |      In-place version of :meth:`~Tensor.nextafter`\n",
            " |  \n",
            " |  nonzero(...)\n",
            " |      nonzero() -> LongTensor\n",
            " |      \n",
            " |      See :func:`torch.nonzero`\n",
            " |  \n",
            " |  normal_(...)\n",
            " |      normal_(mean=0, std=1, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with elements samples from the normal distribution\n",
            " |      parameterized by :attr:`mean` and :attr:`std`.\n",
            " |  \n",
            " |  not_equal(...)\n",
            " |      not_equal(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.not_equal`.\n",
            " |  \n",
            " |  not_equal_(...)\n",
            " |      not_equal_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.not_equal`.\n",
            " |  \n",
            " |  numel(...)\n",
            " |      numel() -> int\n",
            " |      \n",
            " |      See :func:`torch.numel`\n",
            " |  \n",
            " |  numpy(...)\n",
            " |      numpy() -> numpy.ndarray\n",
            " |      \n",
            " |      Returns :attr:`self` tensor as a NumPy :class:`ndarray`. This tensor and the\n",
            " |      returned :class:`ndarray` share the same underlying storage. Changes to\n",
            " |      :attr:`self` tensor will be reflected in the :class:`ndarray` and vice versa.\n",
            " |  \n",
            " |  orgqr(...)\n",
            " |      orgqr(input2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.orgqr`\n",
            " |  \n",
            " |  ormqr(...)\n",
            " |      ormqr(input2, input3, left=True, transpose=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.ormqr`\n",
            " |  \n",
            " |  outer(...)\n",
            " |      outer(vec2) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.outer`.\n",
            " |  \n",
            " |  permute(...)\n",
            " |      permute(*dims) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.permute`\n",
            " |  \n",
            " |  pin_memory(...)\n",
            " |      pin_memory() -> Tensor\n",
            " |      \n",
            " |      Copies the tensor to pinned memory, if it's not already pinned.\n",
            " |  \n",
            " |  pinverse(...)\n",
            " |      pinverse() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.pinverse`\n",
            " |  \n",
            " |  polygamma(...)\n",
            " |      polygamma(n) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.polygamma`\n",
            " |  \n",
            " |  polygamma_(...)\n",
            " |      polygamma_(n) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.polygamma`\n",
            " |  \n",
            " |  positive(...)\n",
            " |      positive() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.positive`\n",
            " |  \n",
            " |  pow(...)\n",
            " |      pow(exponent) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.pow`\n",
            " |  \n",
            " |  pow_(...)\n",
            " |      pow_(exponent) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.pow`\n",
            " |  \n",
            " |  prelu(...)\n",
            " |  \n",
            " |  prod(...)\n",
            " |      prod(dim=None, keepdim=False, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.prod`\n",
            " |  \n",
            " |  put(...)\n",
            " |      put(input, index, source, accumulate=False) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.put_`.\n",
            " |      `input` corresponds to `self` in :meth:`torch.Tensor.put_`.\n",
            " |  \n",
            " |  put_(...)\n",
            " |      put_(index, source, accumulate=False) -> Tensor\n",
            " |      \n",
            " |      Copies the elements from :attr:`source` into the positions specified by\n",
            " |      :attr:`index`. For the purpose of indexing, the :attr:`self` tensor is treated as if\n",
            " |      it were a 1-D tensor.\n",
            " |      \n",
            " |      :attr:`index` and :attr:`source` need to have the same number of elements, but not necessarily\n",
            " |      the same shape.\n",
            " |      \n",
            " |      If :attr:`accumulate` is ``True``, the elements in :attr:`source` are added to\n",
            " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if :attr:`index`\n",
            " |      contain duplicate elements.\n",
            " |      \n",
            " |      Args:\n",
            " |          index (LongTensor): the indices into self\n",
            " |          source (Tensor): the tensor containing values to copy from\n",
            " |          accumulate (bool): whether to accumulate into self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> src = torch.tensor([[4, 3, 5],\n",
            " |          ...                     [6, 7, 8]])\n",
            " |          >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\n",
            " |          tensor([[  4,   9,   5],\n",
            " |                  [ 10,   7,   8]])\n",
            " |  \n",
            " |  q_per_channel_axis(...)\n",
            " |      q_per_channel_axis() -> int\n",
            " |      \n",
            " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
            " |      returns the index of dimension on which per-channel quantization is applied.\n",
            " |  \n",
            " |  q_per_channel_scales(...)\n",
            " |      q_per_channel_scales() -> Tensor\n",
            " |      \n",
            " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
            " |      returns a Tensor of scales of the underlying quantizer. It has the number of\n",
            " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
            " |      the tensor.\n",
            " |  \n",
            " |  q_per_channel_zero_points(...)\n",
            " |      q_per_channel_zero_points() -> Tensor\n",
            " |      \n",
            " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
            " |      returns a tensor of zero_points of the underlying quantizer. It has the number of\n",
            " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
            " |      the tensor.\n",
            " |  \n",
            " |  q_scale(...)\n",
            " |      q_scale() -> float\n",
            " |      \n",
            " |      Given a Tensor quantized by linear(affine) quantization,\n",
            " |      returns the scale of the underlying quantizer().\n",
            " |  \n",
            " |  q_zero_point(...)\n",
            " |      q_zero_point() -> int\n",
            " |      \n",
            " |      Given a Tensor quantized by linear(affine) quantization,\n",
            " |      returns the zero_point of the underlying quantizer().\n",
            " |  \n",
            " |  qr(...)\n",
            " |      qr(some=True) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.qr`\n",
            " |  \n",
            " |  qscheme(...)\n",
            " |      qscheme() -> torch.qscheme\n",
            " |      \n",
            " |      Returns the quantization scheme of a given QTensor.\n",
            " |  \n",
            " |  quantile(...)\n",
            " |      quantile(q, dim=None, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.quantile`\n",
            " |  \n",
            " |  rad2deg(...)\n",
            " |      rad2deg() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.rad2deg`\n",
            " |  \n",
            " |  rad2deg_(...)\n",
            " |      rad2deg_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.rad2deg`\n",
            " |  \n",
            " |  random_(...)\n",
            " |      random_(from=0, to=None, *, generator=None) -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with numbers sampled from the discrete uniform\n",
            " |      distribution over ``[from, to - 1]``. If not specified, the values are usually\n",
            " |      only bounded by :attr:`self` tensor's data type. However, for floating point\n",
            " |      types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every\n",
            " |      value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`\n",
            " |      will be uniform in ``[0, 2^53]``.\n",
            " |  \n",
            " |  ravel(...)\n",
            " |      ravel(input) -> Tensor\n",
            " |      \n",
            " |      see :func:`torch.ravel`\n",
            " |  \n",
            " |  reciprocal(...)\n",
            " |      reciprocal() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.reciprocal`\n",
            " |  \n",
            " |  reciprocal_(...)\n",
            " |      reciprocal_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.reciprocal`\n",
            " |  \n",
            " |  record_stream(...)\n",
            " |      record_stream(stream)\n",
            " |      \n",
            " |      Ensures that the tensor memory is not reused for another tensor until all\n",
            " |      current work queued on :attr:`stream` are complete.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          The caching allocator is aware of only the stream where a tensor was\n",
            " |          allocated. Due to the awareness, it already correctly manages the life\n",
            " |          cycle of tensors on only one stream. But if a tensor is used on a stream\n",
            " |          different from the stream of origin, the allocator might reuse the memory\n",
            " |          unexpectedly. Calling this method lets the allocator know which streams\n",
            " |          have used the tensor.\n",
            " |  \n",
            " |  relu(...)\n",
            " |  \n",
            " |  relu_(...)\n",
            " |  \n",
            " |  remainder(...)\n",
            " |      remainder(divisor) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.remainder`\n",
            " |  \n",
            " |  remainder_(...)\n",
            " |      remainder_(divisor) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.remainder`\n",
            " |  \n",
            " |  renorm(...)\n",
            " |      renorm(p, dim, maxnorm) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.renorm`\n",
            " |  \n",
            " |  renorm_(...)\n",
            " |      renorm_(p, dim, maxnorm) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.renorm`\n",
            " |  \n",
            " |  repeat(...)\n",
            " |      repeat(*sizes) -> Tensor\n",
            " |      \n",
            " |      Repeats this tensor along the specified dimensions.\n",
            " |      \n",
            " |      Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          :meth:`~Tensor.repeat` behaves differently from\n",
            " |          `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,\n",
            " |          but is more similar to\n",
            " |          `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.\n",
            " |          For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.\n",
            " |      \n",
            " |      Args:\n",
            " |          sizes (torch.Size or int...): The number of times to repeat this tensor along each\n",
            " |              dimension\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([1, 2, 3])\n",
            " |          >>> x.repeat(4, 2)\n",
            " |          tensor([[ 1,  2,  3,  1,  2,  3],\n",
            " |                  [ 1,  2,  3,  1,  2,  3],\n",
            " |                  [ 1,  2,  3,  1,  2,  3],\n",
            " |                  [ 1,  2,  3,  1,  2,  3]])\n",
            " |          >>> x.repeat(4, 2, 1).size()\n",
            " |          torch.Size([4, 2, 3])\n",
            " |  \n",
            " |  repeat_interleave(...)\n",
            " |      repeat_interleave(repeats, dim=None, *, output_size=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.repeat_interleave`.\n",
            " |  \n",
            " |  requires_grad_(...)\n",
            " |      requires_grad_(requires_grad=True) -> Tensor\n",
            " |      \n",
            " |      Change if autograd should record operations on this tensor: sets this tensor's\n",
            " |      :attr:`requires_grad` attribute in-place. Returns this tensor.\n",
            " |      \n",
            " |      :func:`requires_grad_`'s main use case is to tell autograd to begin recording\n",
            " |      operations on a Tensor ``tensor``. If ``tensor`` has ``requires_grad=False``\n",
            " |      (because it was obtained through a DataLoader, or required preprocessing or\n",
            " |      initialization), ``tensor.requires_grad_()`` makes it so that autograd will\n",
            " |      begin to record operations on ``tensor``.\n",
            " |      \n",
            " |      Args:\n",
            " |          requires_grad (bool): If autograd should record operations on this tensor.\n",
            " |              Default: ``True``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # Let's say we want to preprocess some saved weights and use\n",
            " |          >>> # the result as new weights.\n",
            " |          >>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n",
            " |          >>> loaded_weights = torch.tensor(saved_weights)\n",
            " |          >>> weights = preprocess(loaded_weights)  # some function\n",
            " |          >>> weights\n",
            " |          tensor([-0.5503,  0.4926, -2.1158, -0.8303])\n",
            " |      \n",
            " |          >>> # Now, start to record operations done to weights\n",
            " |          >>> weights.requires_grad_()\n",
            " |          >>> out = weights.pow(2).sum()\n",
            " |          >>> out.backward()\n",
            " |          >>> weights.grad\n",
            " |          tensor([-1.1007,  0.9853, -4.2316, -1.6606])\n",
            " |  \n",
            " |  reshape(...)\n",
            " |      reshape(*shape) -> Tensor\n",
            " |      \n",
            " |      Returns a tensor with the same data and number of elements as :attr:`self`\n",
            " |      but with the specified shape. This method returns a view if :attr:`shape` is\n",
            " |      compatible with the current shape. See :meth:`torch.Tensor.view` on when it is\n",
            " |      possible to return a view.\n",
            " |      \n",
            " |      See :func:`torch.reshape`\n",
            " |      \n",
            " |      Args:\n",
            " |          shape (tuple of ints or int...): the desired shape\n",
            " |  \n",
            " |  reshape_as(...)\n",
            " |      reshape_as(other) -> Tensor\n",
            " |      \n",
            " |      Returns this tensor as the same shape as :attr:`other`.\n",
            " |      ``self.reshape_as(other)`` is equivalent to ``self.reshape(other.sizes())``.\n",
            " |      This method returns a view if ``other.sizes()`` is compatible with the current\n",
            " |      shape. See :meth:`torch.Tensor.view` on when it is possible to return a view.\n",
            " |      \n",
            " |      Please see :meth:`reshape` for more information about ``reshape``.\n",
            " |      \n",
            " |      Args:\n",
            " |          other (:class:`torch.Tensor`): The result tensor has the same shape\n",
            " |              as :attr:`other`.\n",
            " |  \n",
            " |  resize_(...)\n",
            " |      resize_(*sizes, memory_format=torch.contiguous_format) -> Tensor\n",
            " |      \n",
            " |      Resizes :attr:`self` tensor to the specified size. If the number of elements is\n",
            " |      larger than the current storage size, then the underlying storage is resized\n",
            " |      to fit the new number of elements. If the number of elements is smaller, the\n",
            " |      underlying storage is not changed. Existing elements are preserved but any new\n",
            " |      memory is uninitialized.\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          This is a low-level method. The storage is reinterpreted as C-contiguous,\n",
            " |          ignoring the current strides (unless the target size equals the current\n",
            " |          size, in which case the tensor is left unchanged). For most purposes, you\n",
            " |          will instead want to use :meth:`~Tensor.view()`, which checks for\n",
            " |          contiguity, or :meth:`~Tensor.reshape()`, which copies data if needed. To\n",
            " |          change the size in-place with custom strides, see :meth:`~Tensor.set_()`.\n",
            " |      \n",
            " |      Args:\n",
            " |          sizes (torch.Size or int...): the desired size\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
            " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``sizes``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
            " |          >>> x.resize_(2, 2)\n",
            " |          tensor([[ 1,  2],\n",
            " |                  [ 3,  4]])\n",
            " |  \n",
            " |  resize_as_(...)\n",
            " |      resize_as_(tensor, memory_format=torch.contiguous_format) -> Tensor\n",
            " |      \n",
            " |      Resizes the :attr:`self` tensor to be the same size as the specified\n",
            " |      :attr:`tensor`. This is equivalent to ``self.resize_(tensor.size())``.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
            " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``.\n",
            " |  \n",
            " |  resolve_conj(...)\n",
            " |      resolve_conj() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.resolve_conj`\n",
            " |  \n",
            " |  resolve_neg(...)\n",
            " |      resolve_neg() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.resolve_neg`\n",
            " |  \n",
            " |  retain_grad(...)\n",
            " |      retain_grad() -> None\n",
            " |      \n",
            " |      Enables this Tensor to have their :attr:`grad` populated during\n",
            " |      :func:`backward`. This is a no-op for leaf tensors.\n",
            " |  \n",
            " |  roll(...)\n",
            " |      roll(shifts, dims) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.roll`\n",
            " |  \n",
            " |  rot90(...)\n",
            " |      rot90(k, dims) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.rot90`\n",
            " |  \n",
            " |  round(...)\n",
            " |      round() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.round`\n",
            " |  \n",
            " |  round_(...)\n",
            " |      round_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.round`\n",
            " |  \n",
            " |  rsqrt(...)\n",
            " |      rsqrt() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.rsqrt`\n",
            " |  \n",
            " |  rsqrt_(...)\n",
            " |      rsqrt_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.rsqrt`\n",
            " |  \n",
            " |  scatter(...)\n",
            " |      scatter(dim, index, src) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.scatter_`\n",
            " |  \n",
            " |  scatter_(...)\n",
            " |      scatter_(dim, index, src, reduce=None) -> Tensor\n",
            " |      \n",
            " |      Writes all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
            " |      specified in the :attr:`index` tensor. For each value in :attr:`src`, its output\n",
            " |      index is specified by its index in :attr:`src` for ``dimension != dim`` and by\n",
            " |      the corresponding value in :attr:`index` for ``dimension = dim``.\n",
            " |      \n",
            " |      For a 3-D tensor, :attr:`self` is updated as::\n",
            " |      \n",
            " |          self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
            " |          self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
            " |          self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
            " |      \n",
            " |      This is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n",
            " |      \n",
            " |      :attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have\n",
            " |      the same number of dimensions. It is also required that\n",
            " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
            " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
            " |      Note that ``index`` and ``src`` do not broadcast.\n",
            " |      \n",
            " |      Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\n",
            " |      between ``0`` and ``self.size(dim) - 1`` inclusive.\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          When indices are not unique, the behavior is non-deterministic (one of the\n",
            " |          values from ``src`` will be picked arbitrarily) and the gradient will be\n",
            " |          incorrect (it will be propagated to all locations in the source that\n",
            " |          correspond to the same index)!\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
            " |      \n",
            " |      Additionally accepts an optional :attr:`reduce` argument that allows\n",
            " |      specification of an optional reduction operation, which is applied to all\n",
            " |      values in the tensor :attr:`src` into :attr:`self` at the indicies\n",
            " |      specified in the :attr:`index`. For each value in :attr:`src`, the reduction\n",
            " |      operation is applied to an index in :attr:`self` which is specified by\n",
            " |      its index in :attr:`src` for ``dimension != dim`` and by the corresponding\n",
            " |      value in :attr:`index` for ``dimension = dim``.\n",
            " |      \n",
            " |      Given a 3-D tensor and reduction using the multiplication operation, :attr:`self`\n",
            " |      is updated as::\n",
            " |      \n",
            " |          self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n",
            " |          self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n",
            " |          self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
            " |      \n",
            " |      Reducing with the addition operation is the same as using\n",
            " |      :meth:`~torch.Tensor.scatter_add_`.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int): the axis along which to index\n",
            " |          index (LongTensor): the indices of elements to scatter, can be either empty\n",
            " |              or of the same dimensionality as ``src``. When empty, the operation\n",
            " |              returns ``self`` unchanged.\n",
            " |          src (Tensor or float): the source element(s) to scatter.\n",
            " |          reduce (str, optional): reduction operation to apply, can be either\n",
            " |              ``'add'`` or ``'multiply'``.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> src = torch.arange(1, 11).reshape((2, 5))\n",
            " |          >>> src\n",
            " |          tensor([[ 1,  2,  3,  4,  5],\n",
            " |                  [ 6,  7,  8,  9, 10]])\n",
            " |          >>> index = torch.tensor([[0, 1, 2, 0]])\n",
            " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
            " |          tensor([[1, 0, 0, 4, 0],\n",
            " |                  [0, 2, 0, 0, 0],\n",
            " |                  [0, 0, 3, 0, 0]])\n",
            " |          >>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
            " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
            " |          tensor([[1, 2, 3, 0, 0],\n",
            " |                  [6, 7, 0, 0, 8],\n",
            " |                  [0, 0, 0, 0, 0]])\n",
            " |      \n",
            " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
            " |          ...            1.23, reduce='multiply')\n",
            " |          tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
            " |                  [2.0000, 2.0000, 2.0000, 2.4600]])\n",
            " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
            " |          ...            1.23, reduce='add')\n",
            " |          tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
            " |                  [2.0000, 2.0000, 2.0000, 3.2300]])\n",
            " |  \n",
            " |  scatter_add(...)\n",
            " |      scatter_add(dim, index, src) -> Tensor\n",
            " |      \n",
            " |      Out-of-place version of :meth:`torch.Tensor.scatter_add_`\n",
            " |  \n",
            " |  scatter_add_(...)\n",
            " |      scatter_add_(dim, index, src) -> Tensor\n",
            " |      \n",
            " |      Adds all values from the tensor :attr:`other` into :attr:`self` at the indices\n",
            " |      specified in the :attr:`index` tensor in a similar fashion as\n",
            " |      :meth:`~torch.Tensor.scatter_`. For each value in :attr:`src`, it is added to\n",
            " |      an index in :attr:`self` which is specified by its index in :attr:`src`\n",
            " |      for ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
            " |      ``dimension = dim``.\n",
            " |      \n",
            " |      For a 3-D tensor, :attr:`self` is updated as::\n",
            " |      \n",
            " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
            " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
            " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
            " |      \n",
            " |      :attr:`self`, :attr:`index` and :attr:`src` should have same number of\n",
            " |      dimensions. It is also required that ``index.size(d) <= src.size(d)`` for all\n",
            " |      dimensions ``d``, and that ``index.size(d) <= self.size(d)`` for all dimensions\n",
            " |      ``d != dim``. Note that ``index`` and ``src`` do not broadcast.\n",
            " |      \n",
            " |      Note:\n",
            " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int): the axis along which to index\n",
            " |          index (LongTensor): the indices of elements to scatter and add, can be\n",
            " |              either empty or of the same dimensionality as ``src``. When empty, the\n",
            " |              operation returns ``self`` unchanged.\n",
            " |          src (Tensor): the source elements to scatter and add\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> src = torch.ones((2, 5))\n",
            " |          >>> index = torch.tensor([[0, 1, 2, 0, 0]])\n",
            " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
            " |          tensor([[1., 0., 0., 1., 1.],\n",
            " |                  [0., 1., 0., 0., 0.],\n",
            " |                  [0., 0., 1., 0., 0.]])\n",
            " |          >>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n",
            " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
            " |          tensor([[2., 0., 0., 1., 1.],\n",
            " |                  [0., 2., 0., 0., 0.],\n",
            " |                  [0., 0., 2., 1., 1.]])\n",
            " |  \n",
            " |  select(...)\n",
            " |      select(dim, index) -> Tensor\n",
            " |      \n",
            " |      Slices the :attr:`self` tensor along the selected dimension at the given index.\n",
            " |      This function returns a view of the original tensor with the given dimension removed.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int): the dimension to slice\n",
            " |          index (int): the index to select with\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          :meth:`select` is equivalent to slicing. For example,\n",
            " |          ``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and\n",
            " |          ``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.\n",
            " |  \n",
            " |  set_(...)\n",
            " |      set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor\n",
            " |      \n",
            " |      Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
            " |      :attr:`self` tensor will share the same storage and have the same size and\n",
            " |      strides as :attr:`source`. Changes to elements in one tensor will be reflected\n",
            " |      in the other.\n",
            " |      \n",
            " |      If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
            " |      storage, offset, size, and stride.\n",
            " |      \n",
            " |      Args:\n",
            " |          source (Tensor or Storage): the tensor or storage to use\n",
            " |          storage_offset (int, optional): the offset in the storage\n",
            " |          size (torch.Size, optional): the desired size. Defaults to the size of the source.\n",
            " |          stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.\n",
            " |  \n",
            " |  sgn(...)\n",
            " |      sgn() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sgn`\n",
            " |  \n",
            " |  sgn_(...)\n",
            " |      sgn_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sgn`\n",
            " |  \n",
            " |  short(...)\n",
            " |      short(memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      ``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.\n",
            " |      \n",
            " |      Args:\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  sigmoid(...)\n",
            " |      sigmoid() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sigmoid`\n",
            " |  \n",
            " |  sigmoid_(...)\n",
            " |      sigmoid_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sigmoid`\n",
            " |  \n",
            " |  sign(...)\n",
            " |      sign() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sign`\n",
            " |  \n",
            " |  sign_(...)\n",
            " |      sign_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sign`\n",
            " |  \n",
            " |  signbit(...)\n",
            " |      signbit() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.signbit`\n",
            " |  \n",
            " |  sin(...)\n",
            " |      sin() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sin`\n",
            " |  \n",
            " |  sin_(...)\n",
            " |      sin_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sin`\n",
            " |  \n",
            " |  sinc(...)\n",
            " |      sinc() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sinc`\n",
            " |  \n",
            " |  sinc_(...)\n",
            " |      sinc_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sinc`\n",
            " |  \n",
            " |  sinh(...)\n",
            " |      sinh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sinh`\n",
            " |  \n",
            " |  sinh_(...)\n",
            " |      sinh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sinh`\n",
            " |  \n",
            " |  size(...)\n",
            " |      size(dim=None) -> torch.Size or int\n",
            " |      \n",
            " |      Returns the size of the :attr:`self` tensor. If ``dim`` is not specified,\n",
            " |      the returned value is a :class:`torch.Size`, a subclass of :class:`tuple`.\n",
            " |      If ``dim`` is specified, returns an int holding the size of that dimension.\n",
            " |      \n",
            " |      Args:\n",
            " |        dim (int, optional): The dimension for which to retrieve the size.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> t = torch.empty(3, 4, 5)\n",
            " |          >>> t.size()\n",
            " |          torch.Size([3, 4, 5])\n",
            " |          >>> t.size(dim=1)\n",
            " |          4\n",
            " |  \n",
            " |  slogdet(...)\n",
            " |      slogdet() -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.slogdet`\n",
            " |  \n",
            " |  smm(...)\n",
            " |      smm(mat) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.smm`\n",
            " |  \n",
            " |  softmax(...)\n",
            " |  \n",
            " |  solve(...)\n",
            " |      solve(A) -> Tensor, Tensor\n",
            " |      \n",
            " |      See :func:`torch.solve`\n",
            " |  \n",
            " |  sort(...)\n",
            " |      sort(dim=-1, descending=False) -> (Tensor, LongTensor)\n",
            " |      \n",
            " |      See :func:`torch.sort`\n",
            " |  \n",
            " |  sparse_dim(...)\n",
            " |      sparse_dim() -> int\n",
            " |      \n",
            " |      Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
            " |      \n",
            " |      See also :meth:`Tensor.dense_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
            " |  \n",
            " |  sparse_mask(...)\n",
            " |      sparse_mask(mask) -> Tensor\n",
            " |      \n",
            " |      Returns a new :ref:`sparse tensor <sparse-docs>` with values from a\n",
            " |      strided tensor :attr:`self` filtered by the indices of the sparse\n",
            " |      tensor :attr:`mask`. The values of :attr:`mask` sparse tensor are\n",
            " |      ignored. :attr:`self` and :attr:`mask` tensors must have the same\n",
            " |      shape.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |        The returned sparse tensor has the same indices as the sparse tensor\n",
            " |        :attr:`mask`, even when the corresponding values in :attr:`self` are\n",
            " |        zeros.\n",
            " |      \n",
            " |      Args:\n",
            " |          mask (Tensor): a sparse tensor whose indices are used as a filter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> nse = 5\n",
            " |          >>> dims = (5, 5, 2, 2)\n",
            " |          >>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n",
            " |          ...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n",
            " |          >>> V = torch.randn(nse, dims[2], dims[3])\n",
            " |          >>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n",
            " |          >>> D = torch.randn(dims)\n",
            " |          >>> D.sparse_mask(S)\n",
            " |          tensor(indices=tensor([[0, 0, 0, 2],\n",
            " |                                 [0, 1, 4, 3]]),\n",
            " |                 values=tensor([[[ 1.6550,  0.2397],\n",
            " |                                 [-0.1611, -0.0779]],\n",
            " |      \n",
            " |                                [[ 0.2326, -1.0558],\n",
            " |                                 [ 1.4711,  1.9678]],\n",
            " |      \n",
            " |                                [[-0.5138, -0.0411],\n",
            " |                                 [ 1.9417,  0.5158]],\n",
            " |      \n",
            " |                                [[ 0.0793,  0.0036],\n",
            " |                                 [-0.2569, -0.1055]]]),\n",
            " |                 size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n",
            " |  \n",
            " |  sparse_resize_(...)\n",
            " |      sparse_resize_(size, sparse_dim, dense_dim) -> Tensor\n",
            " |      \n",
            " |      Resizes :attr:`self` :ref:`sparse tensor <sparse-docs>` to the desired\n",
            " |      size and the number of sparse and dense dimensions.\n",
            " |      \n",
            " |      .. note::\n",
            " |        If the number of specified elements in :attr:`self` is zero, then\n",
            " |        :attr:`size`, :attr:`sparse_dim`, and :attr:`dense_dim` can be any\n",
            " |        size and positive integers such that ``len(size) == sparse_dim +\n",
            " |        dense_dim``.\n",
            " |      \n",
            " |        If :attr:`self` specifies one or more elements, however, then each\n",
            " |        dimension in :attr:`size` must not be smaller than the corresponding\n",
            " |        dimension of :attr:`self`, :attr:`sparse_dim` must equal the number\n",
            " |        of sparse dimensions in :attr:`self`, and :attr:`dense_dim` must\n",
            " |        equal the number of dense dimensions in :attr:`self`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          size (torch.Size): the desired size. If :attr:`self` is non-empty\n",
            " |            sparse tensor, the desired size cannot be smaller than the\n",
            " |            original size.\n",
            " |          sparse_dim (int): the number of sparse dimensions\n",
            " |          dense_dim (int): the number of dense dimensions\n",
            " |  \n",
            " |  sparse_resize_and_clear_(...)\n",
            " |      sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor\n",
            " |      \n",
            " |      Removes all specified elements from a :ref:`sparse tensor\n",
            " |      <sparse-docs>` :attr:`self` and resizes :attr:`self` to the desired\n",
            " |      size and the number of sparse and dense dimensions.\n",
            " |      \n",
            " |      .. warning:\n",
            " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          size (torch.Size): the desired size.\n",
            " |          sparse_dim (int): the number of sparse dimensions\n",
            " |          dense_dim (int): the number of dense dimensions\n",
            " |  \n",
            " |  split_with_sizes(...)\n",
            " |  \n",
            " |  sqrt(...)\n",
            " |      sqrt() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sqrt`\n",
            " |  \n",
            " |  sqrt_(...)\n",
            " |      sqrt_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sqrt`\n",
            " |  \n",
            " |  square(...)\n",
            " |      square() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.square`\n",
            " |  \n",
            " |  square_(...)\n",
            " |      square_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.square`\n",
            " |  \n",
            " |  squeeze(...)\n",
            " |      squeeze(dim=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.squeeze`\n",
            " |  \n",
            " |  squeeze_(...)\n",
            " |      squeeze_(dim=None) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.squeeze`\n",
            " |  \n",
            " |  sspaddmm(...)\n",
            " |      sspaddmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sspaddmm`\n",
            " |  \n",
            " |  std(...)\n",
            " |      std(dim, unbiased=True, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.std`\n",
            " |      \n",
            " |      .. function:: std(unbiased=True) -> Tensor\n",
            " |         :noindex:\n",
            " |      \n",
            " |      See :func:`torch.std`\n",
            " |  \n",
            " |  storage(...)\n",
            " |      storage() -> torch.Storage\n",
            " |      \n",
            " |      Returns the underlying storage.\n",
            " |  \n",
            " |  storage_offset(...)\n",
            " |      storage_offset() -> int\n",
            " |      \n",
            " |      Returns :attr:`self` tensor's offset in the underlying storage in terms of\n",
            " |      number of storage elements (not bytes).\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([1, 2, 3, 4, 5])\n",
            " |          >>> x.storage_offset()\n",
            " |          0\n",
            " |          >>> x[3:].storage_offset()\n",
            " |          3\n",
            " |  \n",
            " |  storage_type(...)\n",
            " |      storage_type() -> type\n",
            " |      \n",
            " |      Returns the type of the underlying storage.\n",
            " |  \n",
            " |  stride(...)\n",
            " |      stride(dim) -> tuple or int\n",
            " |      \n",
            " |      Returns the stride of :attr:`self` tensor.\n",
            " |      \n",
            " |      Stride is the jump necessary to go from one element to the next one in the\n",
            " |      specified dimension :attr:`dim`. A tuple of all strides is returned when no\n",
            " |      argument is passed in. Otherwise, an integer value is returned as the stride in\n",
            " |      the particular dimension :attr:`dim`.\n",
            " |      \n",
            " |      Args:\n",
            " |          dim (int, optional): the desired dimension in which stride is required\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
            " |          >>> x.stride()\n",
            " |          (5, 1)\n",
            " |          >>> x.stride(0)\n",
            " |          5\n",
            " |          >>> x.stride(-1)\n",
            " |          1\n",
            " |  \n",
            " |  sub(...)\n",
            " |      sub(other, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sub`.\n",
            " |  \n",
            " |  sub_(...)\n",
            " |      sub_(other, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.sub`\n",
            " |  \n",
            " |  subtract(...)\n",
            " |      subtract(other, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.subtract`.\n",
            " |  \n",
            " |  subtract_(...)\n",
            " |      subtract_(other, *, alpha=1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.subtract`.\n",
            " |  \n",
            " |  sum(...)\n",
            " |      sum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.sum`\n",
            " |  \n",
            " |  sum_to_size(...)\n",
            " |      sum_to_size(*size) -> Tensor\n",
            " |      \n",
            " |      Sum ``this`` tensor to :attr:`size`.\n",
            " |      :attr:`size` must be broadcastable to ``this`` tensor size.\n",
            " |      \n",
            " |      Args:\n",
            " |          size (int...): a sequence of integers defining the shape of the output tensor.\n",
            " |  \n",
            " |  svd(...)\n",
            " |      svd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.svd`\n",
            " |  \n",
            " |  swapaxes(...)\n",
            " |      swapaxes(axis0, axis1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.swapaxes`\n",
            " |  \n",
            " |  swapaxes_(...)\n",
            " |      swapaxes_(axis0, axis1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.swapaxes`\n",
            " |  \n",
            " |  swapdims(...)\n",
            " |      swapdims(dim0, dim1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.swapdims`\n",
            " |  \n",
            " |  swapdims_(...)\n",
            " |      swapdims_(dim0, dim1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.swapdims`\n",
            " |  \n",
            " |  symeig(...)\n",
            " |      symeig(eigenvectors=False, upper=True) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.symeig`\n",
            " |  \n",
            " |  t(...)\n",
            " |      t() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.t`\n",
            " |  \n",
            " |  t_(...)\n",
            " |      t_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.t`\n",
            " |  \n",
            " |  take(...)\n",
            " |      take(indices) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.take`\n",
            " |  \n",
            " |  take_along_dim(...)\n",
            " |      take_along_dim(indices, dim) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.take_along_dim`\n",
            " |  \n",
            " |  tan(...)\n",
            " |      tan() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.tan`\n",
            " |  \n",
            " |  tan_(...)\n",
            " |      tan_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.tan`\n",
            " |  \n",
            " |  tanh(...)\n",
            " |      tanh() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.tanh`\n",
            " |  \n",
            " |  tanh_(...)\n",
            " |      tanh_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.tanh`\n",
            " |  \n",
            " |  tensor_split(...)\n",
            " |      tensor_split(indices_or_sections, dim=0) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.tensor_split`\n",
            " |  \n",
            " |  tile(...)\n",
            " |      tile(*reps) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.tile`\n",
            " |  \n",
            " |  to(...)\n",
            " |      to(*args, **kwargs) -> Tensor\n",
            " |      \n",
            " |      Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are\n",
            " |      inferred from the arguments of ``self.to(*args, **kwargs)``.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          If the ``self`` Tensor already\n",
            " |          has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned.\n",
            " |          Otherwise, the returned tensor is a copy of ``self`` with the desired\n",
            " |          :class:`torch.dtype` and :class:`torch.device`.\n",
            " |      \n",
            " |      Here are the ways to call ``to``:\n",
            " |      \n",
            " |      .. method:: to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
            " |         :noindex:\n",
            " |      \n",
            " |          Returns a Tensor with the specified :attr:`dtype`\n",
            " |      \n",
            " |          Args:\n",
            " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |      \n",
            " |      .. method:: to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
            " |         :noindex:\n",
            " |      \n",
            " |          Returns a Tensor with the specified :attr:`device` and (optional)\n",
            " |          :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``.\n",
            " |          When :attr:`non_blocking`, tries to convert asynchronously with respect to\n",
            " |          the host if possible, e.g., converting a CPU Tensor with pinned memory to a\n",
            " |          CUDA Tensor.\n",
            " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
            " |          already matches the desired conversion.\n",
            " |      \n",
            " |          Args:\n",
            " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |      \n",
            " |      .. method:: to(other, non_blocking=False, copy=False) -> Tensor\n",
            " |         :noindex:\n",
            " |      \n",
            " |          Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as\n",
            " |          the Tensor :attr:`other`. When :attr:`non_blocking`, tries to convert\n",
            " |          asynchronously with respect to the host if possible, e.g., converting a CPU\n",
            " |          Tensor with pinned memory to a CUDA Tensor.\n",
            " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
            " |          already matches the desired conversion.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n",
            " |          >>> tensor.to(torch.float64)\n",
            " |          tensor([[-0.5044,  0.0005],\n",
            " |                  [ 0.3310, -0.0584]], dtype=torch.float64)\n",
            " |      \n",
            " |          >>> cuda0 = torch.device('cuda:0')\n",
            " |          >>> tensor.to(cuda0)\n",
            " |          tensor([[-0.5044,  0.0005],\n",
            " |                  [ 0.3310, -0.0584]], device='cuda:0')\n",
            " |      \n",
            " |          >>> tensor.to(cuda0, dtype=torch.float64)\n",
            " |          tensor([[-0.5044,  0.0005],\n",
            " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
            " |      \n",
            " |          >>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n",
            " |          >>> tensor.to(other, non_blocking=True)\n",
            " |          tensor([[-0.5044,  0.0005],\n",
            " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
            " |  \n",
            " |  to_dense(...)\n",
            " |      to_dense() -> Tensor\n",
            " |      \n",
            " |      Creates a strided copy of :attr:`self`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is a strided tensor.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> s = torch.sparse_coo_tensor(\n",
            " |          ...        torch.tensor([[1, 1],\n",
            " |          ...                      [0, 2]]),\n",
            " |          ...        torch.tensor([9, 10]),\n",
            " |          ...        size=(3, 3))\n",
            " |          >>> s.to_dense()\n",
            " |          tensor([[ 0,  0,  0],\n",
            " |                  [ 9,  0, 10],\n",
            " |                  [ 0,  0,  0]])\n",
            " |  \n",
            " |  to_mkldnn(...)\n",
            " |      to_mkldnn() -> Tensor\n",
            " |      Returns a copy of the tensor in ``torch.mkldnn`` layout.\n",
            " |  \n",
            " |  to_sparse(...)\n",
            " |      to_sparse(sparseDims) -> Tensor\n",
            " |      Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in\n",
            " |      :ref:`coordinate format <sparse-coo-docs>`.\n",
            " |      \n",
            " |      Args:\n",
            " |          sparseDims (int, optional): the number of sparse dimensions to include in the new sparse tensor\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n",
            " |          >>> d\n",
            " |          tensor([[ 0,  0,  0],\n",
            " |                  [ 9,  0, 10],\n",
            " |                  [ 0,  0,  0]])\n",
            " |          >>> d.to_sparse()\n",
            " |          tensor(indices=tensor([[1, 1],\n",
            " |                                 [0, 2]]),\n",
            " |                 values=tensor([ 9, 10]),\n",
            " |                 size=(3, 3), nnz=2, layout=torch.sparse_coo)\n",
            " |          >>> d.to_sparse(1)\n",
            " |          tensor(indices=tensor([[1]]),\n",
            " |                 values=tensor([[ 9,  0, 10]]),\n",
            " |                 size=(3, 3), nnz=1, layout=torch.sparse_coo)\n",
            " |  \n",
            " |  tolist(...)\n",
            " |      tolist() -> list or number\n",
            " |      \n",
            " |      Returns the tensor as a (nested) list. For scalars, a standard\n",
            " |      Python number is returned, just like with :meth:`~Tensor.item`.\n",
            " |      Tensors are automatically moved to the CPU first if necessary.\n",
            " |      \n",
            " |      This operation is not differentiable.\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> a = torch.randn(2, 2)\n",
            " |          >>> a.tolist()\n",
            " |          [[0.012766935862600803, 0.5415473580360413],\n",
            " |           [-0.08909505605697632, 0.7729271650314331]]\n",
            " |          >>> a[0,0].tolist()\n",
            " |          0.012766935862600803\n",
            " |  \n",
            " |  topk(...)\n",
            " |      topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
            " |      \n",
            " |      See :func:`torch.topk`\n",
            " |  \n",
            " |  trace(...)\n",
            " |      trace() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.trace`\n",
            " |  \n",
            " |  transpose(...)\n",
            " |      transpose(dim0, dim1) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.transpose`\n",
            " |  \n",
            " |  transpose_(...)\n",
            " |      transpose_(dim0, dim1) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.transpose`\n",
            " |  \n",
            " |  triangular_solve(...)\n",
            " |      triangular_solve(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)\n",
            " |      \n",
            " |      See :func:`torch.triangular_solve`\n",
            " |  \n",
            " |  tril(...)\n",
            " |      tril(k=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.tril`\n",
            " |  \n",
            " |  tril_(...)\n",
            " |      tril_(k=0) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.tril`\n",
            " |  \n",
            " |  triu(...)\n",
            " |      triu(k=0) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.triu`\n",
            " |  \n",
            " |  triu_(...)\n",
            " |      triu_(k=0) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.triu`\n",
            " |  \n",
            " |  true_divide(...)\n",
            " |      true_divide(value) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.true_divide`\n",
            " |  \n",
            " |  true_divide_(...)\n",
            " |      true_divide_(value) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.true_divide_`\n",
            " |  \n",
            " |  trunc(...)\n",
            " |      trunc() -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.trunc`\n",
            " |  \n",
            " |  trunc_(...)\n",
            " |      trunc_() -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.trunc`\n",
            " |  \n",
            " |  type(...)\n",
            " |      type(dtype=None, non_blocking=False, **kwargs) -> str or Tensor\n",
            " |      Returns the type if `dtype` is not provided, else casts this object to\n",
            " |      the specified type.\n",
            " |      \n",
            " |      If this is already of the correct type, no copy is performed and the\n",
            " |      original object is returned.\n",
            " |      \n",
            " |      Args:\n",
            " |          dtype (type or string): The desired type\n",
            " |          non_blocking (bool): If ``True``, and the source is in pinned memory\n",
            " |              and destination is on the GPU or vice versa, the copy is performed\n",
            " |              asynchronously with respect to the host. Otherwise, the argument\n",
            " |              has no effect.\n",
            " |          **kwargs: For compatibility, may contain the key ``async`` in place of\n",
            " |              the ``non_blocking`` argument. The ``async`` arg is deprecated.\n",
            " |  \n",
            " |  type_as(...)\n",
            " |      type_as(tensor) -> Tensor\n",
            " |      \n",
            " |      Returns this tensor cast to the type of the given tensor.\n",
            " |      \n",
            " |      This is a no-op if the tensor is already of the correct type. This is\n",
            " |      equivalent to ``self.type(tensor.type())``\n",
            " |      \n",
            " |      Args:\n",
            " |          tensor (Tensor): the tensor which has the desired type\n",
            " |  \n",
            " |  unbind(...)\n",
            " |      unbind(dim=0) -> seq\n",
            " |      \n",
            " |      See :func:`torch.unbind`\n",
            " |  \n",
            " |  unfold(...)\n",
            " |      unfold(dimension, size, step) -> Tensor\n",
            " |      \n",
            " |      Returns a view of the original tensor which contains all slices of size :attr:`size` from\n",
            " |      :attr:`self` tensor in the dimension :attr:`dimension`.\n",
            " |      \n",
            " |      Step between two slices is given by :attr:`step`.\n",
            " |      \n",
            " |      If `sizedim` is the size of dimension :attr:`dimension` for :attr:`self`, the size of\n",
            " |      dimension :attr:`dimension` in the returned tensor will be\n",
            " |      `(sizedim - size) / step + 1`.\n",
            " |      \n",
            " |      An additional dimension of size :attr:`size` is appended in the returned tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          dimension (int): dimension in which unfolding happens\n",
            " |          size (int): the size of each slice that is unfolded\n",
            " |          step (int): the step between each slice\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.arange(1., 8)\n",
            " |          >>> x\n",
            " |          tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n",
            " |          >>> x.unfold(0, 2, 1)\n",
            " |          tensor([[ 1.,  2.],\n",
            " |                  [ 2.,  3.],\n",
            " |                  [ 3.,  4.],\n",
            " |                  [ 4.,  5.],\n",
            " |                  [ 5.,  6.],\n",
            " |                  [ 6.,  7.]])\n",
            " |          >>> x.unfold(0, 2, 2)\n",
            " |          tensor([[ 1.,  2.],\n",
            " |                  [ 3.,  4.],\n",
            " |                  [ 5.,  6.]])\n",
            " |  \n",
            " |  uniform_(...)\n",
            " |      uniform_(from=0, to=1) -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with numbers sampled from the continuous uniform\n",
            " |      distribution:\n",
            " |      \n",
            " |      .. math::\n",
            " |          P(x) = \\dfrac{1}{\\text{to} - \\text{from}}\n",
            " |  \n",
            " |  unsafe_chunk(...)\n",
            " |      unsafe_chunk(chunks, dim=0) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.unsafe_chunk`\n",
            " |  \n",
            " |  unsafe_split(...)\n",
            " |      unsafe_split(split_size, dim=0) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.unsafe_split`\n",
            " |  \n",
            " |  unsafe_split_with_sizes(...)\n",
            " |  \n",
            " |  unsqueeze(...)\n",
            " |      unsqueeze(dim) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.unsqueeze`\n",
            " |  \n",
            " |  unsqueeze_(...)\n",
            " |      unsqueeze_(dim) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.unsqueeze`\n",
            " |  \n",
            " |  values(...)\n",
            " |      values() -> Tensor\n",
            " |      \n",
            " |      Return the values tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
            " |      \n",
            " |      .. warning::\n",
            " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
            " |      \n",
            " |      See also :meth:`Tensor.indices`.\n",
            " |      \n",
            " |      .. note::\n",
            " |        This method can only be called on a coalesced sparse tensor. See\n",
            " |        :meth:`Tensor.coalesce` for details.\n",
            " |  \n",
            " |  var(...)\n",
            " |      var(dim, unbiased=True, keepdim=False) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.var`\n",
            " |      \n",
            " |      .. function:: var(unbiased=True) -> Tensor\n",
            " |         :noindex:\n",
            " |      \n",
            " |      See :func:`torch.var`\n",
            " |  \n",
            " |  vdot(...)\n",
            " |      vdot(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.vdot`\n",
            " |  \n",
            " |  view(...)\n",
            " |      view(*shape) -> Tensor\n",
            " |      \n",
            " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
            " |      different :attr:`shape`.\n",
            " |      \n",
            " |      The returned tensor shares the same data and must have the same number\n",
            " |      of elements, but may have a different size. For a tensor to be viewed, the new\n",
            " |      view size must be compatible with its original size and stride, i.e., each new\n",
            " |      view dimension must either be a subspace of an original dimension, or only span\n",
            " |      across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
            " |      contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
            " |      \n",
            " |      .. math::\n",
            " |      \n",
            " |        \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
            " |      \n",
            " |      Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
            " |      without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
            " |      :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
            " |      returns a view if the shapes are compatible, and copies (equivalent to calling\n",
            " |      :meth:`contiguous`) otherwise.\n",
            " |      \n",
            " |      Args:\n",
            " |          shape (torch.Size or int...): the desired size\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.randn(4, 4)\n",
            " |          >>> x.size()\n",
            " |          torch.Size([4, 4])\n",
            " |          >>> y = x.view(16)\n",
            " |          >>> y.size()\n",
            " |          torch.Size([16])\n",
            " |          >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
            " |          >>> z.size()\n",
            " |          torch.Size([2, 8])\n",
            " |      \n",
            " |          >>> a = torch.randn(1, 2, 3, 4)\n",
            " |          >>> a.size()\n",
            " |          torch.Size([1, 2, 3, 4])\n",
            " |          >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
            " |          >>> b.size()\n",
            " |          torch.Size([1, 3, 2, 4])\n",
            " |          >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
            " |          >>> c.size()\n",
            " |          torch.Size([1, 3, 2, 4])\n",
            " |          >>> torch.equal(b, c)\n",
            " |          False\n",
            " |      \n",
            " |      \n",
            " |      .. method:: view(dtype) -> Tensor\n",
            " |         :noindex:\n",
            " |      \n",
            " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
            " |      different :attr:`dtype`. :attr:`dtype` must have the same number of bytes per\n",
            " |      element as :attr:`self`'s dtype.\n",
            " |      \n",
            " |      .. warning::\n",
            " |      \n",
            " |          This overload is not supported by TorchScript, and using it in a Torchscript\n",
            " |          program will cause undefined behavior.\n",
            " |      \n",
            " |      \n",
            " |      Args:\n",
            " |          dtype (:class:`torch.dtype`): the desired dtype\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> x = torch.randn(4, 4)\n",
            " |          >>> x\n",
            " |          tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
            " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
            " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
            " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
            " |          >>> x.dtype\n",
            " |          torch.float32\n",
            " |      \n",
            " |          >>> y = x.view(torch.int32)\n",
            " |          >>> y\n",
            " |          tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
            " |                  [-1105482831,  1061112040,  1057999968, -1084397505],\n",
            " |                  [-1071760287, -1123489973, -1097310419, -1084649136],\n",
            " |                  [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
            " |              dtype=torch.int32)\n",
            " |          >>> y[0, 0] = 1000000000\n",
            " |          >>> x\n",
            " |          tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
            " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
            " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
            " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
            " |      \n",
            " |          >>> x.view(torch.int16)\n",
            " |          Traceback (most recent call last):\n",
            " |            File \"<stdin>\", line 1, in <module>\n",
            " |          RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.\n",
            " |  \n",
            " |  view_as(...)\n",
            " |      view_as(other) -> Tensor\n",
            " |      \n",
            " |      View this tensor as the same size as :attr:`other`.\n",
            " |      ``self.view_as(other)`` is equivalent to ``self.view(other.size())``.\n",
            " |      \n",
            " |      Please see :meth:`~Tensor.view` for more information about ``view``.\n",
            " |      \n",
            " |      Args:\n",
            " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
            " |              as :attr:`other`.\n",
            " |  \n",
            " |  vsplit(...)\n",
            " |      vsplit(split_size_or_sections) -> List of Tensors\n",
            " |      \n",
            " |      See :func:`torch.vsplit`\n",
            " |  \n",
            " |  where(...)\n",
            " |      where(condition, y) -> Tensor\n",
            " |      \n",
            " |      ``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``.\n",
            " |      See :func:`torch.where`\n",
            " |  \n",
            " |  xlogy(...)\n",
            " |      xlogy(other) -> Tensor\n",
            " |      \n",
            " |      See :func:`torch.xlogy`\n",
            " |  \n",
            " |  xlogy_(...)\n",
            " |      xlogy_(other) -> Tensor\n",
            " |      \n",
            " |      In-place version of :meth:`~Tensor.xlogy`\n",
            " |  \n",
            " |  xpu(...)\n",
            " |      xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
            " |      \n",
            " |      Returns a copy of this object in XPU memory.\n",
            " |      \n",
            " |      If this object is already in XPU memory and on the correct device,\n",
            " |      then no copy is performed and the original object is returned.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The destination XPU device.\n",
            " |              Defaults to the current XPU device.\n",
            " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
            " |              the copy will be asynchronous with respect to the host.\n",
            " |              Otherwise, the argument has no effect. Default: ``False``.\n",
            " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
            " |              returned Tensor. Default: ``torch.preserve_format``.\n",
            " |  \n",
            " |  zero_(...)\n",
            " |      zero_() -> Tensor\n",
            " |      \n",
            " |      Fills :attr:`self` tensor with zeros.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch._C._TensorBase:\n",
            " |  \n",
            " |  T\n",
            " |      Is this Tensor with its dimensions reversed.\n",
            " |      \n",
            " |      If ``n`` is the number of dimensions in ``x``,\n",
            " |      ``x.T`` is equivalent to ``x.permute(n-1, n-2, ..., 0)``.\n",
            " |  \n",
            " |  data\n",
            " |  \n",
            " |  device\n",
            " |      Is the :class:`torch.device` where this Tensor is.\n",
            " |  \n",
            " |  dtype\n",
            " |  \n",
            " |  grad_fn\n",
            " |  \n",
            " |  imag\n",
            " |      Returns a new tensor containing imaginary values of the :attr:`self` tensor.\n",
            " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          :func:`imag` is only supported for tensors with complex dtypes.\n",
            " |      \n",
            " |      Example::\n",
            " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
            " |          >>> x\n",
            " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
            " |          >>> x.imag\n",
            " |          tensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n",
            " |  \n",
            " |  is_cuda\n",
            " |      Is ``True`` if the Tensor is stored on the GPU, ``False`` otherwise.\n",
            " |  \n",
            " |  is_leaf\n",
            " |      All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.\n",
            " |      \n",
            " |      For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were\n",
            " |      created by the user. This means that they are not the result of an operation and so\n",
            " |      :attr:`grad_fn` is None.\n",
            " |      \n",
            " |      Only leaf Tensors will have their :attr:`grad` populated during a call to :func:`backward`.\n",
            " |      To get :attr:`grad` populated for non-leaf Tensors, you can use :func:`retain_grad`.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> a = torch.rand(10, requires_grad=True)\n",
            " |          >>> a.is_leaf\n",
            " |          True\n",
            " |          >>> b = torch.rand(10, requires_grad=True).cuda()\n",
            " |          >>> b.is_leaf\n",
            " |          False\n",
            " |          # b was created by the operation that cast a cpu Tensor into a cuda Tensor\n",
            " |          >>> c = torch.rand(10, requires_grad=True) + 2\n",
            " |          >>> c.is_leaf\n",
            " |          False\n",
            " |          # c was created by the addition operation\n",
            " |          >>> d = torch.rand(10).cuda()\n",
            " |          >>> d.is_leaf\n",
            " |          True\n",
            " |          # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n",
            " |          >>> e = torch.rand(10).cuda().requires_grad_()\n",
            " |          >>> e.is_leaf\n",
            " |          True\n",
            " |          # e requires gradients and has no operations creating it\n",
            " |          >>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
            " |          >>> f.is_leaf\n",
            " |          True\n",
            " |          # f requires grad, has no operation creating it\n",
            " |  \n",
            " |  is_meta\n",
            " |      Is ``True`` if the Tensor is a meta tensor, ``False`` otherwise.  Meta tensors\n",
            " |      are like normal tensors, but they carry no data.\n",
            " |  \n",
            " |  is_mkldnn\n",
            " |  \n",
            " |  is_mlc\n",
            " |  \n",
            " |  is_ort\n",
            " |  \n",
            " |  is_quantized\n",
            " |      Is ``True`` if the Tensor is quantized, ``False`` otherwise.\n",
            " |  \n",
            " |  is_sparse\n",
            " |      Is ``True`` if the Tensor uses sparse storage layout, ``False`` otherwise.\n",
            " |  \n",
            " |  is_sparse_csr\n",
            " |      Is ``True`` if the Tensor uses sparse CSR storage layout, ``False`` otherwise.\n",
            " |  \n",
            " |  is_vulkan\n",
            " |  \n",
            " |  is_xpu\n",
            " |      Is ``True`` if the Tensor is stored on the XPU, ``False`` otherwise.\n",
            " |  \n",
            " |  layout\n",
            " |  \n",
            " |  name\n",
            " |  \n",
            " |  names\n",
            " |      Stores names for each of this tensor's dimensions.\n",
            " |      \n",
            " |      ``names[idx]`` corresponds to the name of tensor dimension ``idx``.\n",
            " |      Names are either a string if the dimension is named or ``None`` if the\n",
            " |      dimension is unnamed.\n",
            " |      \n",
            " |      Dimension names may contain characters or underscore. Furthermore, a dimension\n",
            " |      name must be a valid Python variable name (i.e., does not start with underscore).\n",
            " |      \n",
            " |      Tensors may not have two named dimensions with the same name.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          The named tensor API is experimental and subject to change.\n",
            " |  \n",
            " |  ndim\n",
            " |      Alias for :meth:`~Tensor.dim()`\n",
            " |  \n",
            " |  output_nr\n",
            " |  \n",
            " |  real\n",
            " |      Returns a new tensor containing real values of the :attr:`self` tensor.\n",
            " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          :func:`real` is only supported for tensors with complex dtypes.\n",
            " |      \n",
            " |      Example::\n",
            " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
            " |          >>> x\n",
            " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
            " |          >>> x.real\n",
            " |          tensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n",
            " |  \n",
            " |  requires_grad\n",
            " |      Is ``True`` if gradients need to be computed for this Tensor, ``False`` otherwise.\n",
            " |      \n",
            " |      .. note::\n",
            " |      \n",
            " |          The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`\n",
            " |          attribute will be populated, see :attr:`is_leaf` for more details.\n",
            " |  \n",
            " |  retains_grad\n",
            " |      Is ``True`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be\n",
            " |      populated during :func:`backward`, ``False`` otherwise.\n",
            " |  \n",
            " |  shape\n",
            " |  \n",
            " |  volatile\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL1Q418fCjJx",
        "outputId": "968d9bad-87e8-4317-b8e3-0d955e54065f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 2.4252e-05, -3.5940e-03, -1.1231e-02, -1.3950e-02,  1.2392e-02,\n",
            "        -2.2924e-03, -5.5472e-03,  6.4447e-03,  5.3315e-03, -1.0412e-02,\n",
            "        -9.0748e-03, -5.0182e-03, -9.0289e-03, -1.1763e-02, -1.2532e-02,\n",
            "        -1.3428e-02, -9.4909e-03, -1.2215e-02,  1.7320e-03, -1.0958e-02,\n",
            "         5.8611e-03,  1.3109e-02, -3.9436e-04,  1.1868e-03, -1.9697e-03,\n",
            "         3.4549e-03,  1.2648e-02, -9.3525e-03,  1.6830e-03, -2.6058e-03,\n",
            "        -7.1322e-03, -2.1987e-03, -5.6401e-04, -4.7328e-03, -4.6197e-03,\n",
            "        -1.3864e-02, -1.1094e-03, -1.3028e-02,  4.6251e-03, -2.2866e-03,\n",
            "        -2.6645e-03, -1.6517e-03,  3.4688e-03, -1.1326e-02,  1.1942e-02,\n",
            "        -7.5590e-04,  1.6800e-02, -1.9134e-02, -1.7240e-02, -3.0990e-03,\n",
            "         1.4421e-02, -1.9972e-02,  1.4029e-02,  9.2369e-03,  1.3318e-02,\n",
            "         5.3759e-03,  1.9041e-02, -1.8332e-03,  6.9949e-03,  1.6575e-02,\n",
            "        -4.4368e-03,  7.7840e-03,  6.2909e-04,  9.6192e-03,  1.9924e-02,\n",
            "         1.0974e-02, -8.5309e-03, -9.9378e-04,  9.3166e-03, -3.6413e-03,\n",
            "        -1.4380e-02,  3.1796e-03,  2.9409e-03, -3.5264e-03, -4.1359e-03,\n",
            "         1.5779e-02,  1.7120e-03,  5.8895e-03,  7.8900e-03,  8.6789e-03,\n",
            "        -1.9626e-02, -2.8065e-03,  2.1981e-03, -3.9247e-02, -2.2997e-02,\n",
            "        -4.3423e-03, -8.8585e-03,  5.6942e-04,  6.1691e-03,  1.2612e-02,\n",
            "        -2.2874e-03, -1.6251e-02,  5.7904e-03, -1.0203e-02, -1.1790e-02,\n",
            "        -1.6283e-02, -5.4399e-04, -1.3277e-02, -2.1789e-02, -9.8468e-03,\n",
            "        -1.0435e-02, -1.7446e-02, -9.0786e-03,  1.2816e-02,  5.1896e-03,\n",
            "        -9.4637e-03, -8.6194e-04, -2.1099e-02, -1.9515e-02, -2.6629e-02,\n",
            "        -3.0134e-02,  2.2964e-02,  6.0908e-03, -1.1035e-02, -3.5738e-03,\n",
            "        -3.4416e-02, -1.6728e-02, -1.0660e-02,  7.1807e-03, -5.2475e-03,\n",
            "        -6.6159e-03, -9.1879e-03,  4.8890e-03, -1.3734e-02,  4.8014e-04,\n",
            "        -1.0382e-02, -1.8849e-02,  4.7004e-04,  1.3089e-03,  1.2087e-03,\n",
            "        -2.0215e-02, -7.9890e-04, -1.7436e-04, -2.9265e-04, -1.8087e-02,\n",
            "        -2.0709e-02, -1.2156e-02, -1.7392e-02, -1.0500e-02, -1.7532e-02,\n",
            "        -2.1847e-02, -7.4788e-03, -1.9455e-02, -4.8343e-03, -1.0975e-02,\n",
            "        -1.0617e-02,  3.8287e-03, -1.8242e-02, -5.5177e-03, -1.6590e-02,\n",
            "        -9.6180e-04,  1.3919e-04, -1.8838e-02,  1.3107e-02,  2.3464e-03,\n",
            "         6.0978e-03,  1.3889e-02, -8.9563e-03, -7.6170e-03,  2.6191e-03,\n",
            "        -2.8309e-03,  2.1522e-02,  2.8302e-02,  1.5458e-03,  2.8069e-03,\n",
            "        -7.7488e-03,  7.8462e-03, -1.6368e-02,  1.0075e-02, -1.8265e-03,\n",
            "         1.0666e-02,  1.7061e-02,  7.0695e-03,  6.6410e-03,  1.7955e-02,\n",
            "        -1.6233e-02, -1.9520e-03,  6.2590e-03,  1.6799e-02,  5.7298e-03,\n",
            "         4.3504e-03, -1.3386e-03,  1.6826e-02, -9.0032e-03,  1.2068e-02,\n",
            "         4.9916e-03,  7.0952e-04,  8.3123e-03,  8.4228e-03, -1.7427e-03,\n",
            "         8.9653e-04,  1.8632e-02,  1.9952e-02, -1.0340e-02,  2.6529e-03,\n",
            "         3.3417e-02,  2.1528e-02,  1.2192e-02, -2.2368e-02,  1.7489e-02,\n",
            "        -1.3797e-02,  7.9001e-03,  1.3338e-02,  2.4298e-02,  1.8266e-02,\n",
            "         9.4643e-03, -1.5262e-02,  4.7201e-03, -5.9716e-03,  1.7075e-02,\n",
            "        -8.4122e-03,  8.0611e-03, -6.4746e-03, -6.1366e-03,  2.2611e-03,\n",
            "         1.0709e-02, -4.7339e-03,  2.6434e-02,  6.2820e-03,  1.9674e-03,\n",
            "         8.4281e-03, -1.0898e-02,  2.9999e-03,  2.0527e-02, -6.2776e-03,\n",
            "         2.7121e-02, -4.8017e-03,  8.3912e-03, -1.1393e-02,  1.8458e-02,\n",
            "         1.0556e-02, -3.1013e-03,  1.5550e-02,  1.2332e-03,  1.0569e-02,\n",
            "        -2.0841e-03,  2.2023e-02,  1.5703e-02,  9.4840e-03,  3.0995e-02,\n",
            "         4.0859e-03,  3.4727e-03, -1.5836e-03, -8.0780e-03,  1.0261e-02,\n",
            "         1.0709e-02, -1.9952e-05,  1.6524e-02,  8.6138e-03, -5.9071e-03,\n",
            "         9.6737e-03,  1.5961e-02,  7.7633e-03, -1.0075e-02,  1.5992e-02,\n",
            "        -8.4181e-03,  1.2642e-02,  1.6330e-03,  1.3277e-02,  3.7909e-03,\n",
            "        -5.1555e-04, -1.0970e-02,  6.7811e-03,  8.1709e-03,  7.3557e-03,\n",
            "         2.8707e-03,  1.9046e-02, -3.2315e-03, -1.9688e-02,  7.0356e-03,\n",
            "         1.9835e-02, -1.0458e-02, -7.5881e-03,  1.5889e-02, -3.9910e-03,\n",
            "        -1.0690e-02, -4.2523e-03, -3.5353e-03, -3.3934e-03,  9.9788e-03,\n",
            "        -8.2160e-04,  3.0616e-02, -1.3495e-02, -1.1781e-02,  1.5927e-02,\n",
            "         2.0827e-02,  2.5908e-03, -5.2178e-04,  6.6020e-04, -2.9913e-04,\n",
            "        -7.6404e-03, -3.7934e-03, -4.1162e-04, -1.2141e-02, -2.9156e-03,\n",
            "         1.2609e-03, -5.6924e-03, -1.2762e-02,  8.8818e-03,  1.5859e-02,\n",
            "        -1.6116e-02,  1.2641e-04, -8.5219e-03, -3.8118e-03, -1.2280e-02,\n",
            "         4.5196e-03,  1.0948e-02, -7.6788e-04, -1.1976e-02,  7.9920e-04,\n",
            "        -5.7732e-03, -3.6485e-03,  1.8193e-02,  1.4027e-02,  2.4666e-02,\n",
            "        -1.2133e-03, -1.0950e-03, -9.8918e-03,  3.0323e-03, -1.0450e-02,\n",
            "        -1.4582e-02, -2.6244e-02, -2.4208e-02, -1.2951e-02, -2.2502e-02,\n",
            "        -2.2424e-02, -2.4724e-02, -2.3349e-04, -1.1919e-02,  4.0288e-04,\n",
            "        -1.4117e-03, -8.7185e-03,  6.0223e-04,  4.9141e-03,  7.9781e-03,\n",
            "        -6.1760e-03,  8.6264e-03, -1.0127e-02, -3.2377e-03, -4.3149e-03,\n",
            "        -1.0065e-02, -1.7396e-02, -1.7922e-04, -1.5321e-03, -8.9861e-03,\n",
            "        -1.0396e-02,  5.2122e-03, -2.8116e-03,  3.6887e-03, -6.5339e-03,\n",
            "         8.8409e-03, -2.3880e-03, -8.2064e-03, -3.8404e-04, -4.3732e-03,\n",
            "        -6.5039e-03, -2.2234e-03, -1.2836e-02,  2.4946e-03,  1.9472e-02,\n",
            "         4.6813e-03,  1.9694e-02,  4.7335e-04,  4.3889e-03, -1.7679e-04,\n",
            "        -1.0124e-03, -5.6189e-04,  1.2376e-02,  3.5225e-03,  2.4079e-03,\n",
            "        -4.3731e-03, -7.3055e-03,  1.8952e-03,  6.0451e-03, -5.1815e-04,\n",
            "        -6.1728e-03,  2.9907e-03, -8.4507e-03,  9.5554e-03,  8.3636e-03,\n",
            "        -1.0260e-02, -3.6365e-03,  1.1095e-02,  8.5657e-03, -1.8831e-02,\n",
            "        -5.7321e-03,  6.6138e-03, -2.2510e-03, -1.4998e-02,  3.1096e-03,\n",
            "        -6.2999e-03,  2.1549e-02, -2.3045e-02, -2.0920e-02, -1.9505e-03,\n",
            "        -1.9023e-03, -1.9746e-02,  3.5935e-04, -6.3647e-03,  2.5273e-03,\n",
            "        -1.2591e-02,  1.2967e-02, -2.3687e-03, -1.1616e-04, -2.0456e-02,\n",
            "         1.6681e-02,  2.2321e-03, -5.4100e-04, -2.4726e-02, -9.9807e-04,\n",
            "        -6.1287e-03,  4.8659e-03,  6.8403e-03,  1.3745e-02,  1.4913e-02,\n",
            "         2.4851e-03,  1.7672e-02, -2.0400e-02,  1.0747e-03,  7.7393e-03,\n",
            "        -4.4207e-04, -3.2128e-03, -4.5307e-03,  2.0171e-02,  1.8880e-02,\n",
            "        -7.9142e-03, -2.3399e-02, -2.5061e-02, -6.1341e-04,  2.4715e-04,\n",
            "        -4.1907e-03,  1.0106e-02, -1.5686e-03,  9.1214e-03,  5.8142e-04,\n",
            "        -1.7671e-02, -4.3118e-03, -5.0399e-03,  5.3879e-03, -2.4175e-03,\n",
            "         6.7997e-03,  1.1474e-02, -6.1668e-03, -8.2019e-03,  8.4150e-03,\n",
            "         2.1473e-02,  1.4416e-02,  1.5283e-02,  2.0346e-02,  9.3179e-03,\n",
            "         3.6597e-03, -6.2927e-03, -1.4550e-02, -9.2840e-04,  7.8252e-03,\n",
            "         4.2396e-04,  1.0804e-02,  1.8530e-02, -1.2433e-02,  1.7047e-02,\n",
            "        -5.4940e-03, -4.3557e-03,  7.7890e-03, -1.0873e-03,  2.6188e-02,\n",
            "         5.0758e-03, -6.0164e-03, -9.2281e-04,  1.2511e-02, -1.6320e-03,\n",
            "         3.0462e-02, -7.4788e-03, -1.1677e-03,  1.6432e-02, -5.2766e-03,\n",
            "        -7.0639e-03, -2.6963e-02,  5.8037e-03, -1.2931e-02,  2.4290e-04,\n",
            "         1.2201e-02, -6.0824e-03,  6.7681e-03, -3.6644e-03, -1.1694e-02,\n",
            "         6.6555e-03,  7.0564e-03,  1.8190e-02,  1.2388e-04,  2.9247e-03,\n",
            "        -2.4335e-02,  6.4842e-03, -4.1662e-03, -4.5997e-03, -4.9631e-03,\n",
            "         4.6956e-03, -1.3185e-02, -1.0681e-02, -2.0974e-03, -6.0719e-04,\n",
            "        -2.0665e-02,  1.6446e-03,  5.0329e-03,  1.4857e-03,  1.4333e-02,\n",
            "         6.2904e-03,  6.7921e-03, -6.9877e-03,  9.9422e-03, -1.2908e-02,\n",
            "        -2.1284e-02,  9.7616e-03,  4.3549e-03,  1.8512e-04, -6.2173e-03,\n",
            "         1.4965e-02, -6.4137e-03, -2.7370e-03, -1.4029e-02,  4.6953e-03,\n",
            "        -2.2300e-02,  6.3346e-03,  8.6146e-03,  1.7977e-02,  4.0361e-03,\n",
            "        -2.2926e-02,  1.3547e-02,  1.3790e-02,  3.9968e-03,  1.0192e-02,\n",
            "         1.1821e-02, -2.7620e-03,  6.5637e-03,  2.5191e-03,  1.1222e-02,\n",
            "         8.5225e-03, -1.2849e-02,  1.0288e-02,  1.2823e-02,  1.8432e-02,\n",
            "        -2.4742e-02,  7.4899e-03,  4.5767e-03, -1.3433e-03,  6.4392e-03,\n",
            "         9.2021e-03, -2.9991e-03, -8.6641e-03, -4.9366e-03, -4.7412e-03,\n",
            "        -4.7364e-03, -5.5233e-03, -1.4910e-03,  6.8172e-03, -5.5429e-03,\n",
            "        -7.4333e-03, -2.0865e-02,  3.1114e-03, -1.6105e-03,  1.2573e-03,\n",
            "        -3.8264e-03,  1.9472e-03, -1.1139e-02,  2.0478e-02, -8.5620e-03,\n",
            "        -7.7096e-03, -4.2763e-03,  1.1771e-02,  2.7107e-03, -4.6697e-03,\n",
            "        -4.0106e-04, -2.5750e-04,  4.4108e-03, -7.9638e-03,  9.3712e-03,\n",
            "         2.1877e-03,  6.5822e-04,  2.4323e-03,  3.7965e-03, -1.4986e-03,\n",
            "        -1.0956e-02, -1.6900e-03,  9.0248e-03,  3.0059e-03,  1.3138e-02,\n",
            "         7.3929e-04, -1.3527e-02,  6.5021e-03,  7.5280e-03,  7.1134e-04,\n",
            "        -8.6665e-03, -6.7316e-03,  6.3290e-03,  8.6945e-03, -4.5834e-03,\n",
            "        -8.4885e-03, -6.2502e-03,  1.2319e-02, -2.2845e-02,  5.6906e-03,\n",
            "        -6.7671e-03,  6.1049e-03,  6.1069e-03, -3.3679e-03, -9.6948e-03,\n",
            "        -9.5861e-04, -1.3742e-02,  7.3685e-03,  4.3420e-03, -2.3789e-03,\n",
            "         3.1937e-02, -2.3378e-02, -1.8225e-02, -4.7975e-03, -2.0125e-02,\n",
            "         1.5192e-03,  2.9967e-03, -2.3205e-03,  1.6031e-02, -1.5818e-02,\n",
            "         2.3625e-02, -4.4691e-03,  1.8422e-02,  1.9771e-02,  8.6937e-03,\n",
            "        -5.1004e-03,  1.6390e-02,  6.5689e-03, -4.4770e-03,  1.6528e-02,\n",
            "         4.6706e-03, -1.6011e-02,  2.1123e-02,  2.0665e-03, -1.4340e-02,\n",
            "        -3.6791e-03,  1.1681e-02, -8.6034e-03, -3.7005e-03, -9.8828e-03,\n",
            "        -2.8177e-02,  1.1018e-02,  7.3978e-03, -3.9675e-03,  9.5272e-03,\n",
            "        -9.3897e-03,  2.3795e-02, -2.0138e-02, -3.8751e-05,  3.6110e-03,\n",
            "         2.5813e-02, -1.6963e-02, -5.2506e-03,  3.9263e-03,  2.8595e-03,\n",
            "         1.1377e-02,  4.2302e-03,  9.2759e-03, -7.1187e-04,  1.1753e-02,\n",
            "         1.1695e-02, -1.3433e-02,  1.8342e-02,  7.9681e-03,  4.9778e-03,\n",
            "         2.1344e-02,  2.6596e-03,  1.0253e-02, -1.7581e-02, -2.3243e-02,\n",
            "        -1.2141e-02,  2.5121e-02,  1.0480e-02,  1.8971e-02, -3.1279e-03,\n",
            "        -4.9105e-03, -3.1892e-04,  1.7844e-02,  5.3361e-03,  8.5304e-03,\n",
            "         1.0655e-02,  1.7125e-02,  8.4594e-04,  3.4907e-02,  1.7075e-02,\n",
            "         4.5380e-03,  2.0944e-02, -1.3233e-02, -1.0381e-02, -2.5319e-03,\n",
            "        -9.6155e-03, -8.3108e-03,  7.1197e-03, -1.8625e-02, -8.8235e-03,\n",
            "         1.5253e-02,  1.4265e-02, -1.1014e-02, -8.3049e-03,  2.4490e-03,\n",
            "         7.4176e-03, -6.3967e-03, -1.2777e-02, -1.0907e-03,  2.0378e-02,\n",
            "        -1.0309e-02, -1.3703e-02, -1.8929e-03,  5.6021e-03, -2.0090e-02,\n",
            "        -7.7949e-03,  4.6328e-03, -5.8752e-03,  1.0793e-02, -5.2322e-03,\n",
            "         4.6862e-03,  1.7604e-02, -9.0392e-03,  7.0002e-03,  1.9267e-02,\n",
            "         5.0500e-03, -8.2290e-03,  1.5804e-02, -1.3129e-02, -1.2823e-02,\n",
            "         9.0285e-03,  5.9746e-03, -1.8075e-02,  4.3077e-03, -5.1537e-03,\n",
            "        -5.1214e-03,  1.8718e-02,  8.6789e-03,  6.3730e-03, -3.6153e-03,\n",
            "        -1.2539e-05, -7.3871e-03,  1.6148e-04, -4.7486e-04,  2.0585e-02,\n",
            "        -3.7756e-03, -9.1458e-03, -6.0767e-03,  1.0571e-02,  1.1660e-02,\n",
            "         2.0457e-02, -1.3896e-02,  2.0097e-03, -1.6030e-02,  1.4201e-02,\n",
            "         1.4318e-03,  6.9700e-03,  7.2493e-03, -8.8155e-03, -7.5986e-04,\n",
            "        -6.2068e-03,  4.6980e-03, -1.2116e-02,  1.5526e-02,  3.6744e-03,\n",
            "         2.1390e-03,  1.2713e-02, -1.4046e-02,  7.8081e-03,  1.1671e-02,\n",
            "        -7.6532e-04, -6.7240e-03,  7.4256e-03,  6.0897e-03,  1.5856e-02,\n",
            "        -6.1552e-03, -1.5173e-02,  2.9578e-03,  1.2653e-02,  2.5143e-02,\n",
            "        -8.3284e-03,  1.0812e-02,  2.3351e-02, -4.3691e-03, -1.1206e-02,\n",
            "        -1.2275e-02, -1.1636e-02,  2.8387e-03, -1.3286e-03,  1.6191e-02,\n",
            "        -2.7693e-03, -5.8401e-03, -7.9729e-03, -2.5239e-04,  7.3041e-04,\n",
            "        -2.3827e-02,  1.1467e-02, -3.4023e-03,  3.5950e-03, -1.5251e-02,\n",
            "        -4.1666e-03,  3.8388e-03, -7.2495e-03, -1.0165e-03, -5.9426e-03,\n",
            "        -2.0284e-02,  7.6189e-04, -1.4483e-02, -7.6558e-03, -1.1235e-02,\n",
            "         2.1771e-02, -1.4183e-02, -1.6805e-02,  5.9281e-03,  5.7980e-03,\n",
            "        -8.4900e-03, -1.3254e-02,  7.0720e-03,  1.1140e-02, -3.0270e-03,\n",
            "        -2.1668e-02, -5.4213e-03, -3.2337e-04, -2.1949e-02,  1.4540e-03,\n",
            "        -1.3664e-02, -3.0199e-04,  9.6518e-03,  9.7072e-03,  3.5469e-03,\n",
            "         1.9376e-03, -2.2104e-03, -8.7742e-03,  3.4301e-03, -1.1887e-02,\n",
            "         6.8920e-03,  4.2577e-04, -5.0457e-03,  3.0193e-04,  1.2898e-02,\n",
            "        -1.6378e-02,  1.8975e-02,  1.4279e-02,  2.9860e-02,  1.4504e-02,\n",
            "         1.4177e-02, -9.1635e-03,  1.1251e-02,  1.0649e-02,  4.3311e-03,\n",
            "         1.6509e-02,  4.0793e-03,  2.6462e-03,  1.9313e-02,  3.8682e-03,\n",
            "         1.5431e-02, -1.5367e-02,  6.1679e-03, -4.7630e-03, -5.7637e-03,\n",
            "        -2.7962e-03, -2.7935e-02, -1.8167e-02,  2.4512e-03,  1.0520e-02,\n",
            "        -9.3206e-03,  1.4029e-02, -4.7169e-03,  8.8799e-03,  7.9136e-03,\n",
            "        -1.7232e-03, -3.7590e-03,  8.2311e-03,  3.4176e-02, -8.6008e-03,\n",
            "         1.6287e-02, -6.1445e-03,  1.9867e-02, -5.9042e-03, -1.1210e-02,\n",
            "         1.5526e-02,  8.4137e-03,  1.7622e-03, -1.9220e-02,  2.0403e-02,\n",
            "         1.3634e-02,  1.0097e-03,  5.2698e-03, -2.0850e-03, -3.7647e-03,\n",
            "         2.9588e-02, -1.3835e-02, -1.8501e-02,  5.5159e-03,  4.7691e-03,\n",
            "        -6.9867e-03, -3.9273e-03,  2.4381e-03,  8.9150e-03, -1.3040e-02,\n",
            "         9.7832e-05,  1.5156e-02,  9.9801e-03,  2.6794e-02, -1.3405e-02,\n",
            "         7.5620e-04,  1.1605e-03,  2.8392e-03,  1.3482e-02, -1.2688e-02,\n",
            "        -1.2171e-02,  9.7302e-03,  1.6751e-03, -1.2205e-02,  1.7420e-02,\n",
            "         7.4913e-03,  5.1893e-03, -1.4867e-02, -2.4888e-05, -5.9493e-04,\n",
            "        -7.2555e-04, -2.5126e-02, -7.9649e-03, -3.9167e-03,  2.9422e-02,\n",
            "         2.1165e-02, -1.6826e-02, -6.9749e-03,  7.6042e-03, -1.9988e-02,\n",
            "        -6.2428e-03,  6.0117e-04, -3.0261e-03, -6.2471e-03,  2.8081e-02,\n",
            "         3.7688e-03, -3.9736e-03, -5.0048e-04, -9.9148e-03,  8.5269e-03,\n",
            "        -6.5521e-03, -3.6637e-03, -1.7470e-02, -5.3914e-03, -9.0411e-03,\n",
            "         2.8903e-05, -3.2868e-03, -2.4899e-03, -1.7261e-02, -9.6195e-03,\n",
            "        -3.2696e-03, -4.1874e-03, -1.2062e-02,  4.1027e-03, -2.4554e-04,\n",
            "         1.0415e-02,  7.5885e-05,  3.8138e-03, -4.9690e-03, -2.9998e-04,\n",
            "         1.2338e-03, -8.5460e-03, -2.2829e-03, -1.4905e-02, -1.8677e-02,\n",
            "         4.8100e-03,  4.4534e-03, -1.2100e-02, -7.6854e-03, -1.4470e-02,\n",
            "         3.8002e-03,  1.6028e-02,  7.2447e-03, -5.9827e-03,  4.7748e-03,\n",
            "        -5.4465e-03,  7.7402e-03, -8.0067e-03, -1.3618e-02, -3.9643e-03,\n",
            "        -2.8264e-03,  1.1944e-02,  1.2888e-02, -4.9227e-03, -2.5245e-02,\n",
            "         1.4617e-02, -4.2137e-03,  1.9924e-02, -1.2365e-02, -3.2730e-03,\n",
            "         6.7351e-03, -1.7962e-02,  8.5531e-03, -9.4513e-03,  5.1416e-03,\n",
            "         1.5143e-04, -1.2370e-02, -1.3060e-02, -4.6236e-02, -1.7051e-02,\n",
            "        -2.3103e-02,  6.0341e-03, -1.0229e-02, -3.5864e-03,  2.1600e-03])\n"
          ]
        }
      ],
      "source": [
        "print(param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdualDFLCuZ5",
        "outputId": "60cc22fd-0cc6-4f1a-b390-4909189c71b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([ 2.4252e-05, -3.5940e-03, -1.1231e-02, -1.3950e-02,  1.2392e-02,\n",
            "        -2.2924e-03, -5.5472e-03,  6.4447e-03,  5.3315e-03, -1.0412e-02,\n",
            "        -9.0748e-03, -5.0182e-03, -9.0289e-03, -1.1763e-02, -1.2532e-02,\n",
            "        -1.3428e-02, -9.4909e-03, -1.2215e-02,  1.7320e-03, -1.0958e-02,\n",
            "         5.8611e-03,  1.3109e-02, -3.9436e-04,  1.1868e-03, -1.9697e-03,\n",
            "         3.4549e-03,  1.2648e-02, -9.3525e-03,  1.6830e-03, -2.6058e-03,\n",
            "        -7.1322e-03, -2.1987e-03, -5.6401e-04, -4.7328e-03, -4.6197e-03,\n",
            "        -1.3864e-02, -1.1094e-03, -1.3028e-02,  4.6251e-03, -2.2866e-03,\n",
            "        -2.6645e-03, -1.6517e-03,  3.4688e-03, -1.1326e-02,  1.1942e-02,\n",
            "        -7.5590e-04,  1.6800e-02, -1.9134e-02, -1.7240e-02, -3.0990e-03,\n",
            "         1.4421e-02, -1.9972e-02,  1.4029e-02,  9.2369e-03,  1.3318e-02,\n",
            "         5.3759e-03,  1.9041e-02, -1.8332e-03,  6.9949e-03,  1.6575e-02,\n",
            "        -4.4368e-03,  7.7840e-03,  6.2909e-04,  9.6192e-03,  1.9924e-02,\n",
            "         1.0974e-02, -8.5309e-03, -9.9378e-04,  9.3166e-03, -3.6413e-03,\n",
            "        -1.4380e-02,  3.1796e-03,  2.9409e-03, -3.5264e-03, -4.1359e-03,\n",
            "         1.5779e-02,  1.7120e-03,  5.8895e-03,  7.8900e-03,  8.6789e-03,\n",
            "        -1.9626e-02, -2.8065e-03,  2.1981e-03, -3.9247e-02, -2.2997e-02,\n",
            "        -4.3423e-03, -8.8585e-03,  5.6942e-04,  6.1691e-03,  1.2612e-02,\n",
            "        -2.2874e-03, -1.6251e-02,  5.7904e-03, -1.0203e-02, -1.1790e-02,\n",
            "        -1.6283e-02, -5.4399e-04, -1.3277e-02, -2.1789e-02, -9.8468e-03,\n",
            "        -1.0435e-02, -1.7446e-02, -9.0786e-03,  1.2816e-02,  5.1896e-03,\n",
            "        -9.4637e-03, -8.6194e-04, -2.1099e-02, -1.9515e-02, -2.6629e-02,\n",
            "        -3.0134e-02,  2.2964e-02,  6.0908e-03, -1.1035e-02, -3.5738e-03,\n",
            "        -3.4416e-02, -1.6728e-02, -1.0660e-02,  7.1807e-03, -5.2475e-03,\n",
            "        -6.6159e-03, -9.1879e-03,  4.8890e-03, -1.3734e-02,  4.8014e-04,\n",
            "        -1.0382e-02, -1.8849e-02,  4.7004e-04,  1.3089e-03,  1.2087e-03,\n",
            "        -2.0215e-02, -7.9890e-04, -1.7436e-04, -2.9265e-04, -1.8087e-02,\n",
            "        -2.0709e-02, -1.2156e-02, -1.7392e-02, -1.0500e-02, -1.7532e-02,\n",
            "        -2.1847e-02, -7.4788e-03, -1.9455e-02, -4.8343e-03, -1.0975e-02,\n",
            "        -1.0617e-02,  3.8287e-03, -1.8242e-02, -5.5177e-03, -1.6590e-02,\n",
            "        -9.6180e-04,  1.3919e-04, -1.8838e-02,  1.3107e-02,  2.3464e-03,\n",
            "         6.0978e-03,  1.3889e-02, -8.9563e-03, -7.6170e-03,  2.6191e-03,\n",
            "        -2.8309e-03,  2.1522e-02,  2.8302e-02,  1.5458e-03,  2.8069e-03,\n",
            "        -7.7488e-03,  7.8462e-03, -1.6368e-02,  1.0075e-02, -1.8265e-03,\n",
            "         1.0666e-02,  1.7061e-02,  7.0695e-03,  6.6410e-03,  1.7955e-02,\n",
            "        -1.6233e-02, -1.9520e-03,  6.2590e-03,  1.6799e-02,  5.7298e-03,\n",
            "         4.3504e-03, -1.3386e-03,  1.6826e-02, -9.0032e-03,  1.2068e-02,\n",
            "         4.9916e-03,  7.0952e-04,  8.3123e-03,  8.4228e-03, -1.7427e-03,\n",
            "         8.9653e-04,  1.8632e-02,  1.9952e-02, -1.0340e-02,  2.6529e-03,\n",
            "         3.3417e-02,  2.1528e-02,  1.2192e-02, -2.2368e-02,  1.7489e-02,\n",
            "        -1.3797e-02,  7.9001e-03,  1.3338e-02,  2.4298e-02,  1.8266e-02,\n",
            "         9.4643e-03, -1.5262e-02,  4.7201e-03, -5.9716e-03,  1.7075e-02,\n",
            "        -8.4122e-03,  8.0611e-03, -6.4746e-03, -6.1366e-03,  2.2611e-03,\n",
            "         1.0709e-02, -4.7339e-03,  2.6434e-02,  6.2820e-03,  1.9674e-03,\n",
            "         8.4281e-03, -1.0898e-02,  2.9999e-03,  2.0527e-02, -6.2776e-03,\n",
            "         2.7121e-02, -4.8017e-03,  8.3912e-03, -1.1393e-02,  1.8458e-02,\n",
            "         1.0556e-02, -3.1013e-03,  1.5550e-02,  1.2332e-03,  1.0569e-02,\n",
            "        -2.0841e-03,  2.2023e-02,  1.5703e-02,  9.4840e-03,  3.0995e-02,\n",
            "         4.0859e-03,  3.4727e-03, -1.5836e-03, -8.0780e-03,  1.0261e-02,\n",
            "         1.0709e-02, -1.9952e-05,  1.6524e-02,  8.6138e-03, -5.9071e-03,\n",
            "         9.6737e-03,  1.5961e-02,  7.7633e-03, -1.0075e-02,  1.5992e-02,\n",
            "        -8.4181e-03,  1.2642e-02,  1.6330e-03,  1.3277e-02,  3.7909e-03,\n",
            "        -5.1555e-04, -1.0970e-02,  6.7811e-03,  8.1709e-03,  7.3557e-03,\n",
            "         2.8707e-03,  1.9046e-02, -3.2315e-03, -1.9688e-02,  7.0356e-03,\n",
            "         1.9835e-02, -1.0458e-02, -7.5881e-03,  1.5889e-02, -3.9910e-03,\n",
            "        -1.0690e-02, -4.2523e-03, -3.5353e-03, -3.3934e-03,  9.9788e-03,\n",
            "        -8.2160e-04,  3.0616e-02, -1.3495e-02, -1.1781e-02,  1.5927e-02,\n",
            "         2.0827e-02,  2.5908e-03, -5.2178e-04,  6.6020e-04, -2.9913e-04,\n",
            "        -7.6404e-03, -3.7934e-03, -4.1162e-04, -1.2141e-02, -2.9156e-03,\n",
            "         1.2609e-03, -5.6924e-03, -1.2762e-02,  8.8818e-03,  1.5859e-02,\n",
            "        -1.6116e-02,  1.2641e-04, -8.5219e-03, -3.8118e-03, -1.2280e-02,\n",
            "         4.5196e-03,  1.0948e-02, -7.6788e-04, -1.1976e-02,  7.9920e-04,\n",
            "        -5.7732e-03, -3.6485e-03,  1.8193e-02,  1.4027e-02,  2.4666e-02,\n",
            "        -1.2133e-03, -1.0950e-03, -9.8918e-03,  3.0323e-03, -1.0450e-02,\n",
            "        -1.4582e-02, -2.6244e-02, -2.4208e-02, -1.2951e-02, -2.2502e-02,\n",
            "        -2.2424e-02, -2.4724e-02, -2.3349e-04, -1.1919e-02,  4.0288e-04,\n",
            "        -1.4117e-03, -8.7185e-03,  6.0223e-04,  4.9141e-03,  7.9781e-03,\n",
            "        -6.1760e-03,  8.6264e-03, -1.0127e-02, -3.2377e-03, -4.3149e-03,\n",
            "        -1.0065e-02, -1.7396e-02, -1.7922e-04, -1.5321e-03, -8.9861e-03,\n",
            "        -1.0396e-02,  5.2122e-03, -2.8116e-03,  3.6887e-03, -6.5339e-03,\n",
            "         8.8409e-03, -2.3880e-03, -8.2064e-03, -3.8404e-04, -4.3732e-03,\n",
            "        -6.5039e-03, -2.2234e-03, -1.2836e-02,  2.4946e-03,  1.9472e-02,\n",
            "         4.6813e-03,  1.9694e-02,  4.7335e-04,  4.3889e-03, -1.7679e-04,\n",
            "        -1.0124e-03, -5.6189e-04,  1.2376e-02,  3.5225e-03,  2.4079e-03,\n",
            "        -4.3731e-03, -7.3055e-03,  1.8952e-03,  6.0451e-03, -5.1815e-04,\n",
            "        -6.1728e-03,  2.9907e-03, -8.4507e-03,  9.5554e-03,  8.3636e-03,\n",
            "        -1.0260e-02, -3.6365e-03,  1.1095e-02,  8.5657e-03, -1.8831e-02,\n",
            "        -5.7321e-03,  6.6138e-03, -2.2510e-03, -1.4998e-02,  3.1096e-03,\n",
            "        -6.2999e-03,  2.1549e-02, -2.3045e-02, -2.0920e-02, -1.9505e-03,\n",
            "        -1.9023e-03, -1.9746e-02,  3.5935e-04, -6.3647e-03,  2.5273e-03,\n",
            "        -1.2591e-02,  1.2967e-02, -2.3687e-03, -1.1616e-04, -2.0456e-02,\n",
            "         1.6681e-02,  2.2321e-03, -5.4100e-04, -2.4726e-02, -9.9807e-04,\n",
            "        -6.1287e-03,  4.8659e-03,  6.8403e-03,  1.3745e-02,  1.4913e-02,\n",
            "         2.4851e-03,  1.7672e-02, -2.0400e-02,  1.0747e-03,  7.7393e-03,\n",
            "        -4.4207e-04, -3.2128e-03, -4.5307e-03,  2.0171e-02,  1.8880e-02,\n",
            "        -7.9142e-03, -2.3399e-02, -2.5061e-02, -6.1341e-04,  2.4715e-04,\n",
            "        -4.1907e-03,  1.0106e-02, -1.5686e-03,  9.1214e-03,  5.8142e-04,\n",
            "        -1.7671e-02, -4.3118e-03, -5.0399e-03,  5.3879e-03, -2.4175e-03,\n",
            "         6.7997e-03,  1.1474e-02, -6.1668e-03, -8.2019e-03,  8.4150e-03,\n",
            "         2.1473e-02,  1.4416e-02,  1.5283e-02,  2.0346e-02,  9.3179e-03,\n",
            "         3.6597e-03, -6.2927e-03, -1.4550e-02, -9.2840e-04,  7.8252e-03,\n",
            "         4.2396e-04,  1.0804e-02,  1.8530e-02, -1.2433e-02,  1.7047e-02,\n",
            "        -5.4940e-03, -4.3557e-03,  7.7890e-03, -1.0873e-03,  2.6188e-02,\n",
            "         5.0758e-03, -6.0164e-03, -9.2281e-04,  1.2511e-02, -1.6320e-03,\n",
            "         3.0462e-02, -7.4788e-03, -1.1677e-03,  1.6432e-02, -5.2766e-03,\n",
            "        -7.0639e-03, -2.6963e-02,  5.8037e-03, -1.2931e-02,  2.4290e-04,\n",
            "         1.2201e-02, -6.0824e-03,  6.7681e-03, -3.6644e-03, -1.1694e-02,\n",
            "         6.6555e-03,  7.0564e-03,  1.8190e-02,  1.2388e-04,  2.9247e-03,\n",
            "        -2.4335e-02,  6.4842e-03, -4.1662e-03, -4.5997e-03, -4.9631e-03,\n",
            "         4.6956e-03, -1.3185e-02, -1.0681e-02, -2.0974e-03, -6.0719e-04,\n",
            "        -2.0665e-02,  1.6446e-03,  5.0329e-03,  1.4857e-03,  1.4333e-02,\n",
            "         6.2904e-03,  6.7921e-03, -6.9877e-03,  9.9422e-03, -1.2908e-02,\n",
            "        -2.1284e-02,  9.7616e-03,  4.3549e-03,  1.8512e-04, -6.2173e-03,\n",
            "         1.4965e-02, -6.4137e-03, -2.7370e-03, -1.4029e-02,  4.6953e-03,\n",
            "        -2.2300e-02,  6.3346e-03,  8.6146e-03,  1.7977e-02,  4.0361e-03,\n",
            "        -2.2926e-02,  1.3547e-02,  1.3790e-02,  3.9968e-03,  1.0192e-02,\n",
            "         1.1821e-02, -2.7620e-03,  6.5637e-03,  2.5191e-03,  1.1222e-02,\n",
            "         8.5225e-03, -1.2849e-02,  1.0288e-02,  1.2823e-02,  1.8432e-02,\n",
            "        -2.4742e-02,  7.4899e-03,  4.5767e-03, -1.3433e-03,  6.4392e-03,\n",
            "         9.2021e-03, -2.9991e-03, -8.6641e-03, -4.9366e-03, -4.7412e-03,\n",
            "        -4.7364e-03, -5.5233e-03, -1.4910e-03,  6.8172e-03, -5.5429e-03,\n",
            "        -7.4333e-03, -2.0865e-02,  3.1114e-03, -1.6105e-03,  1.2573e-03,\n",
            "        -3.8264e-03,  1.9472e-03, -1.1139e-02,  2.0478e-02, -8.5620e-03,\n",
            "        -7.7096e-03, -4.2763e-03,  1.1771e-02,  2.7107e-03, -4.6697e-03,\n",
            "        -4.0106e-04, -2.5750e-04,  4.4108e-03, -7.9638e-03,  9.3712e-03,\n",
            "         2.1877e-03,  6.5822e-04,  2.4323e-03,  3.7965e-03, -1.4986e-03,\n",
            "        -1.0956e-02, -1.6900e-03,  9.0248e-03,  3.0059e-03,  1.3138e-02,\n",
            "         7.3929e-04, -1.3527e-02,  6.5021e-03,  7.5280e-03,  7.1134e-04,\n",
            "        -8.6665e-03, -6.7316e-03,  6.3290e-03,  8.6945e-03, -4.5834e-03,\n",
            "        -8.4885e-03, -6.2502e-03,  1.2319e-02, -2.2845e-02,  5.6906e-03,\n",
            "        -6.7671e-03,  6.1049e-03,  6.1069e-03, -3.3679e-03, -9.6948e-03,\n",
            "        -9.5861e-04, -1.3742e-02,  7.3685e-03,  4.3420e-03, -2.3789e-03,\n",
            "         3.1937e-02, -2.3378e-02, -1.8225e-02, -4.7975e-03, -2.0125e-02,\n",
            "         1.5192e-03,  2.9967e-03, -2.3205e-03,  1.6031e-02, -1.5818e-02,\n",
            "         2.3625e-02, -4.4691e-03,  1.8422e-02,  1.9771e-02,  8.6937e-03,\n",
            "        -5.1004e-03,  1.6390e-02,  6.5689e-03, -4.4770e-03,  1.6528e-02,\n",
            "         4.6706e-03, -1.6011e-02,  2.1123e-02,  2.0665e-03, -1.4340e-02,\n",
            "        -3.6791e-03,  1.1681e-02, -8.6034e-03, -3.7005e-03, -9.8828e-03,\n",
            "        -2.8177e-02,  1.1018e-02,  7.3978e-03, -3.9675e-03,  9.5272e-03,\n",
            "        -9.3897e-03,  2.3795e-02, -2.0138e-02, -3.8751e-05,  3.6110e-03,\n",
            "         2.5813e-02, -1.6963e-02, -5.2506e-03,  3.9263e-03,  2.8595e-03,\n",
            "         1.1377e-02,  4.2302e-03,  9.2759e-03, -7.1187e-04,  1.1753e-02,\n",
            "         1.1695e-02, -1.3433e-02,  1.8342e-02,  7.9681e-03,  4.9778e-03,\n",
            "         2.1344e-02,  2.6596e-03,  1.0253e-02, -1.7581e-02, -2.3243e-02,\n",
            "        -1.2141e-02,  2.5121e-02,  1.0480e-02,  1.8971e-02, -3.1279e-03,\n",
            "        -4.9105e-03, -3.1892e-04,  1.7844e-02,  5.3361e-03,  8.5304e-03,\n",
            "         1.0655e-02,  1.7125e-02,  8.4594e-04,  3.4907e-02,  1.7075e-02,\n",
            "         4.5380e-03,  2.0944e-02, -1.3233e-02, -1.0381e-02, -2.5319e-03,\n",
            "        -9.6155e-03, -8.3108e-03,  7.1197e-03, -1.8625e-02, -8.8235e-03,\n",
            "         1.5253e-02,  1.4265e-02, -1.1014e-02, -8.3049e-03,  2.4490e-03,\n",
            "         7.4176e-03, -6.3967e-03, -1.2777e-02, -1.0907e-03,  2.0378e-02,\n",
            "        -1.0309e-02, -1.3703e-02, -1.8929e-03,  5.6021e-03, -2.0090e-02,\n",
            "        -7.7949e-03,  4.6328e-03, -5.8752e-03,  1.0793e-02, -5.2322e-03,\n",
            "         4.6862e-03,  1.7604e-02, -9.0392e-03,  7.0002e-03,  1.9267e-02,\n",
            "         5.0500e-03, -8.2290e-03,  1.5804e-02, -1.3129e-02, -1.2823e-02,\n",
            "         9.0285e-03,  5.9746e-03, -1.8075e-02,  4.3077e-03, -5.1537e-03,\n",
            "        -5.1214e-03,  1.8718e-02,  8.6789e-03,  6.3730e-03, -3.6153e-03,\n",
            "        -1.2539e-05, -7.3871e-03,  1.6148e-04, -4.7486e-04,  2.0585e-02,\n",
            "        -3.7756e-03, -9.1458e-03, -6.0767e-03,  1.0571e-02,  1.1660e-02,\n",
            "         2.0457e-02, -1.3896e-02,  2.0097e-03, -1.6030e-02,  1.4201e-02,\n",
            "         1.4318e-03,  6.9700e-03,  7.2493e-03, -8.8155e-03, -7.5986e-04,\n",
            "        -6.2068e-03,  4.6980e-03, -1.2116e-02,  1.5526e-02,  3.6744e-03,\n",
            "         2.1390e-03,  1.2713e-02, -1.4046e-02,  7.8081e-03,  1.1671e-02,\n",
            "        -7.6532e-04, -6.7240e-03,  7.4256e-03,  6.0897e-03,  1.5856e-02,\n",
            "        -6.1552e-03, -1.5173e-02,  2.9578e-03,  1.2653e-02,  2.5143e-02,\n",
            "        -8.3284e-03,  1.0812e-02,  2.3351e-02, -4.3691e-03, -1.1206e-02,\n",
            "        -1.2275e-02, -1.1636e-02,  2.8387e-03, -1.3286e-03,  1.6191e-02,\n",
            "        -2.7693e-03, -5.8401e-03, -7.9729e-03, -2.5239e-04,  7.3041e-04,\n",
            "        -2.3827e-02,  1.1467e-02, -3.4023e-03,  3.5950e-03, -1.5251e-02,\n",
            "        -4.1666e-03,  3.8388e-03, -7.2495e-03, -1.0165e-03, -5.9426e-03,\n",
            "        -2.0284e-02,  7.6189e-04, -1.4483e-02, -7.6558e-03, -1.1235e-02,\n",
            "         2.1771e-02, -1.4183e-02, -1.6805e-02,  5.9281e-03,  5.7980e-03,\n",
            "        -8.4900e-03, -1.3254e-02,  7.0720e-03,  1.1140e-02, -3.0270e-03,\n",
            "        -2.1668e-02, -5.4213e-03, -3.2337e-04, -2.1949e-02,  1.4540e-03,\n",
            "        -1.3664e-02, -3.0199e-04,  9.6518e-03,  9.7072e-03,  3.5469e-03,\n",
            "         1.9376e-03, -2.2104e-03, -8.7742e-03,  3.4301e-03, -1.1887e-02,\n",
            "         6.8920e-03,  4.2577e-04, -5.0457e-03,  3.0193e-04,  1.2898e-02,\n",
            "        -1.6378e-02,  1.8975e-02,  1.4279e-02,  2.9860e-02,  1.4504e-02,\n",
            "         1.4177e-02, -9.1635e-03,  1.1251e-02,  1.0649e-02,  4.3311e-03,\n",
            "         1.6509e-02,  4.0793e-03,  2.6462e-03,  1.9313e-02,  3.8682e-03,\n",
            "         1.5431e-02, -1.5367e-02,  6.1679e-03, -4.7630e-03, -5.7637e-03,\n",
            "        -2.7962e-03, -2.7935e-02, -1.8167e-02,  2.4512e-03,  1.0520e-02,\n",
            "        -9.3206e-03,  1.4029e-02, -4.7169e-03,  8.8799e-03,  7.9136e-03,\n",
            "        -1.7232e-03, -3.7590e-03,  8.2311e-03,  3.4176e-02, -8.6008e-03,\n",
            "         1.6287e-02, -6.1445e-03,  1.9867e-02, -5.9042e-03, -1.1210e-02,\n",
            "         1.5526e-02,  8.4137e-03,  1.7622e-03, -1.9220e-02,  2.0403e-02,\n",
            "         1.3634e-02,  1.0097e-03,  5.2698e-03, -2.0850e-03, -3.7647e-03,\n",
            "         2.9588e-02, -1.3835e-02, -1.8501e-02,  5.5159e-03,  4.7691e-03,\n",
            "        -6.9867e-03, -3.9273e-03,  2.4381e-03,  8.9150e-03, -1.3040e-02,\n",
            "         9.7832e-05,  1.5156e-02,  9.9801e-03,  2.6794e-02, -1.3405e-02,\n",
            "         7.5620e-04,  1.1605e-03,  2.8392e-03,  1.3482e-02, -1.2688e-02,\n",
            "        -1.2171e-02,  9.7302e-03,  1.6751e-03, -1.2205e-02,  1.7420e-02,\n",
            "         7.4913e-03,  5.1893e-03, -1.4867e-02, -2.4888e-05, -5.9493e-04,\n",
            "        -7.2555e-04, -2.5126e-02, -7.9649e-03, -3.9167e-03,  2.9422e-02,\n",
            "         2.1165e-02, -1.6826e-02, -6.9749e-03,  7.6042e-03, -1.9988e-02,\n",
            "        -6.2428e-03,  6.0117e-04, -3.0261e-03, -6.2471e-03,  2.8081e-02,\n",
            "         3.7688e-03, -3.9736e-03, -5.0048e-04, -9.9148e-03,  8.5269e-03,\n",
            "        -6.5521e-03, -3.6637e-03, -1.7470e-02, -5.3914e-03, -9.0411e-03,\n",
            "         2.8903e-05, -3.2868e-03, -2.4899e-03, -1.7261e-02, -9.6195e-03,\n",
            "        -3.2696e-03, -4.1874e-03, -1.2062e-02,  4.1027e-03, -2.4554e-04,\n",
            "         1.0415e-02,  7.5885e-05,  3.8138e-03, -4.9690e-03, -2.9998e-04,\n",
            "         1.2338e-03, -8.5460e-03, -2.2829e-03, -1.4905e-02, -1.8677e-02,\n",
            "         4.8100e-03,  4.4534e-03, -1.2100e-02, -7.6854e-03, -1.4470e-02,\n",
            "         3.8002e-03,  1.6028e-02,  7.2447e-03, -5.9827e-03,  4.7748e-03,\n",
            "        -5.4465e-03,  7.7402e-03, -8.0067e-03, -1.3618e-02, -3.9643e-03,\n",
            "        -2.8264e-03,  1.1944e-02,  1.2888e-02, -4.9227e-03, -2.5245e-02,\n",
            "         1.4617e-02, -4.2137e-03,  1.9924e-02, -1.2365e-02, -3.2730e-03,\n",
            "         6.7351e-03, -1.7962e-02,  8.5531e-03, -9.4513e-03,  5.1416e-03,\n",
            "         1.5143e-04, -1.2370e-02, -1.3060e-02, -4.6236e-02, -1.7051e-02,\n",
            "        -2.3103e-02,  6.0341e-03, -1.0229e-02, -3.5864e-03,  2.1600e-03],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLHCGjDdClBI",
        "outputId": "654bbd4e-6470-4e18-a3b7-09fd33f783b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(param.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGDvEOcCnpP"
      },
      "source": [
        "We remark that gradients are intialized to None initially. Let's have a look on what happens after one forward pass and what happens after one backward pass. \n",
        "\n",
        "**Exercice:** generate a random batch $(X, y)$, where $X$ has shape `(8, 3, 224, 224)` and $y$ has shape `(8, 1)`. Run a forward pass on the generated batch then a backward pass on the generated batch (use `nn.CrossEntropyLoss` as a loss function), and print a parameter (for example the bias of the output layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "a3EW29wbEqJj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(7.8546, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data = torch.randn(size=(8, 3 , 224, 224))\n",
        "targets = torch.randint(low=0, high=999, size=(8,))\n",
        "\n",
        "out = net(data)\n",
        "loss = criterion(out, targets)\n",
        "\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbBogiDhFotL",
        "outputId": "da7ee253-4bbb-4772-e9f2-2f81ab2cc2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 7, 7])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([128, 64, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 64, 1, 1])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([256, 128, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 128, 1, 1])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([512, 256, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 256, 1, 1])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([1000, 512])\n",
            "torch.Size([1000])\n"
          ]
        }
      ],
      "source": [
        "for param in net.parameters():\n",
        "    print(param.grad.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weVTRiNYHJdU",
        "outputId": "512a3777-e6b6-4b31-d5b1-56de29d4a21d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(param.is_leaf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-FoesTTMUie",
        "outputId": "e225bcc9-8326-4491-dbac-f6251062c98c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(loss.is_leaf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Handle neural networks with PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
