\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}

% Importing settings from setup.sty
\usepackage{setup}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{glossaries}
% \makenoidxglossaries
% \newcommand{\thetahat}{\hat{\theta}}

% \newacronym{bmi}{BMI}{Body Mass Index}


% \pagenumbering{roman}
\begin{document}

% Inserting title page
\import{./}{title}

\pagenumbering{gobble}
\tableofcontents
% \listoffigures
% \listoftables



\newgeometry{
  left=25mm,
  right=25mm,
  top=25mm,
  bottom=25mm}
\pagenumbering{arabic}

\section{Exercise 1}

Let $f$ be given by:
\begin{equation}
  f(x) = \frac{1}{2} \| Ax - y \|_2^2
\end{equation}
we want to compute the gradient of $f$. We have for a given direction $v \in \R^n$:
\begin{align*}
  \nabla_v f(x)
   & =\lim_{\varepsilon \to 0} \frac{f(x+\varepsilon v)-f(x)}{\varepsilon}                                                      \\
   & =\lim_{\varepsilon \to 0} \frac{\|A(x+\varepsilon v)-y\|^2-\|A x-y\|^2}{2 \varepsilon}                                     \\
   & =\lim_{\varepsilon \to 0} \frac{\|(Ax - y) + A \varepsilon v \|^2-\|A x-y\|^2}{2 \varepsilon}                              \\
   & =\lim_{\varepsilon \to 0} \frac{\|A x-y\|^2+2 \varepsilon<A x-y, A v> + \varepsilon^2\|A v\|^2-\|A x-y\|^2}{2 \varepsilon} \\
   & =\lim_{\varepsilon \to 0}(A x-y)^T A v + \underbrace{\frac{\varepsilon}{2}\|A v\|^2}_{\to 0}                               \\
   & =\left\langle A^T(A x-y), v\right\rangle,
\end{align*}
We see that we obtain a scalar product with $v$ on one side, as we wished. The other side of the scalar product (\ie $A^T (Ax - y)$) corresponds to the gradient of $f$ in the direction $v$. In other words, this is the change that will occur when we take a small step in the direction of $v$.


\section{Exercise 2}
\subsection{Proof of the Lipschitz continuity of the gradient}
Recall that a function $g: $ is said to be $L$-Lipschitz continuous if $\forall\ x_1, x_2 \in \R^n$:
\begin{equation}
  \|g(x_1) - g(x_2)\| \leq L \|x_1 - x_2\|.
\end{equation}
In particular for $g \equiv \nabla f$, we have:
\begin{align*}
  \| \nabla f (x_1) - \nabla f (x_2) \|
   & = \| A^T(Ax_1 - y) - A^T(Ax_2 - y) \|  \\
   & = \| A^T A(x_1 - x_2) \|               \\
   & \leq \| A^T A \| \cdot \| x_1 - x_2 \|
\end{align*}
We therefore have that that $\nabla f$ is $L$-Lipschitz continuous, with $L := \| A^T A \|$.

\subsection{Computation of the Lipschitz constant}
Note that $\| A^T A \|$ is the matrix norm of $A^T A$, which is the largest singular value of $A^T A$. Let us call $\sigma_{max}(M)$ the largest singular value of a given matrix $M$. Recall that the singular value decomposition of $M$ is given by:
\begin{equation}
  M = U \Sigma V^T
\end{equation}
where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix with the singular values of $M$ on the diagonal, which we assume to be sorted in decreasing order (without loss of generality, thanks to potential reordering of the rows, columns of U, V respectively). We have:
\begin{align*}
  \| A^T A \|
   & = \sigma_{max}(A^T A)                                        \\
   & = \sigma_{max}(U \Sigma \underbrace{V^T V}_{=Id} \Sigma U^T) \\
   & = \sigma_{max}(U \Sigma^2 U^T)                               \\
   & = \sigma_{max}(\Sigma^2)                                     \\
   & = \sigma_{max}(\Sigma)^2                                     \\
   & = \sigma_{max}(U \Sigma V^T)^2                               \\
   & = \| A \| ^2                                                 \\
\end{align*}
Thus we have that the Lipschitz constant $L$ is given by:
$$
  L = \| A^T A \| = \| A \| ^2.
$$

\section{Exercise 3}
\label{ex3}
We must split the problem into two subproblems: $x = 0$ and $x \neq 0$. \\
We will first consider the case $x = 0$. In this case, we have that the subdifferential of $f$ at a point $x$ is given by:
\begin{equation}
  \label{eq: subdifferential}
  \partial f(x) := \left\{ c \in \R \mid \forall\ y \in \R, \quad f(y) \geq f(x) + \langle c, y - x \rangle \right\}
\end{equation}
We compute the following:
\begin{align*}
  \partial f(0)
   & = \left\{ c \in \R \mid \forall\ y \in \R, \quad f(y) \geq f(0) + \langle c, y - 0 \rangle \right\} \\
   & = \left\{ c \in \R \mid \forall\ y \in \R, \quad |y| \geq \langle c, y \rangle \right\}             \\
   & = \left\{ c \in \R \mid \forall\ y \in \R, \quad |y| \geq cy \right\}                               \\
   & =
  \begin{cases}
    \left\{ c \in \R \mid \forall\ y \in \R, \quad y \geq cy \right\},  & y \geq 0 \\
    \left\{ c \in \R \mid \forall\ y \in \R, \quad -y \geq cy \right\}, & y < 0
  \end{cases}                                                                              \\
   & =
  \begin{cases}
    \left\{ c \in \R \mid \forall\ y \in \R, \quad c \leq 1 \right\},  & y \geq 0 \\
    \left\{ c \in \R \mid \forall\ y \in \R, \quad c \geq -1 \right\}, & y < 0
  \end{cases}                                                                              \\
   & = \left\{ c \in \R \mid \forall\ y \in \R, \quad c \in [-1, 1] \right\}                             \\
   & = [-1, 1]
\end{align*}
Now for the case $x \neq 0$, we have
$$
  x \neq 0 \implies \partial f(x) = \left\{ \operatorname{sign}(x) \right\}
$$



\section{Exercise 4}
We want to compute the subdifferential of the following function:
\begin{equation}
  F(x) := \frac{1}{2} \| Ax - y \|^2 + \lambda \| x \|_1^2
\end{equation}
We define $F_1$ and $F_2$ as follows:
\begin{align*}
  F_1(x) & := \frac{1}{2} \| Ax - y \|^2 \\
  F_2(x) & := \lambda \| x \|_1^2
\end{align*}
and we note that $F(x) = F_1(x) + F_2(x)$. \\
We also note that $F_1$ is the function seen in exercise 1, while $F_2$ is somewhat of a generalization of the function seen in exercise 3 to the $n$-dimensional case (we will expand on that later). Furthermore, by proposition 1, we have that $\partial F(x) = \partial F_1(x) + F_2(x)$.  \\
Let us first compute the subdifferential of $F_1$. By exercise 1, we know that $F_1$ is differentiable and we have:
\begin{align*}
  \partial F_1(x)
  &= \left\{ \nabla F_1(x) \right\} \\
  &= \left\{ A^T(Ax - y) \right\}
\end{align*}
Now let us compute the subdifferential of $F_2$. By proposition 2, we have:
\begin{align*}
  \partial F_2(x)
  &= \partial \left( \lambda \| x \|_1 \right) \\
  &= \lambda \partial \| x \|_1 \\
\end{align*}
We notice that $\| x \|_1$ is the sum of the absolute values of the components of $x$, which is a convex separable function. We can therefore apply proposition 3 to compute the subdifferential of $\| x \|_1$:
\begin{align*}
  &
  \partial \| x \|_1 = \partial \sum_{i=1}^n |x_i| \\
  \implies &
  \partial F_2(x) = \left\{ \lambda (p_1, \ldots, p_n) \mid \forall i = 1, \ldots, n,\ p_i \in \partial |x_i| \right\}
\end{align*}
with $\partial |x_i|$ as defined in exercise \ref{ex3}. \\
Combining the results of the previous two computations, we have:
\begin{align*}
  \partial F(x)
  &= \partial F_1(x) + \partial F_2(x) \\
  &= \left\{ A^T(Ax - y) \right\} + \left\{ \lambda (p_1, \ldots, p_n) \mid \forall i = 1, \ldots, n,\ p_i \in \partial |x_i| \right\} \\
  &= \left\{ A^T(Ax - y) + \lambda (p_1, \ldots, p_n) \mid \forall i = 1, \ldots, n,\ p_i \in \partial |x_i| \right\}
\end{align*}


% \clearpage
% \printnoidxglossaries

\end{document}