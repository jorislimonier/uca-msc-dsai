\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue, 
}
\usepackage[a4paper, total={15.8cm, 26cm}]{geometry}


\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\thetahat}{\hat\theta}
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\so}{\ensuremath{\mathcal{S}_1} }
\newcommand{\st}{\ensuremath{\mathcal{S}_2} }
\newcommand{\mco}{\ensuremath{\mathcal{MC}_1} }
\newcommand{\mct}{\ensuremath{\mathcal{MC}_2} }
\newcommand{\mdft}{\ensuremath{\mathcal{MDFT}} }

\title{Report of ``Think Fast and Slow in AI", by Francesca ROSSI}
\author{Joris LIMONIER}
\date{November 17, 2021}

\begin{document}
\maketitle
\pagenumbering{gobble}
\vspace{.5cm}
Francesca ROSSI is an Italian IBM researcher focusing on ethics in Artificial Intelligence (AI). Her talk ``Think Fast and Slow in AI" is named after the book \textit{Thinking, Fast and Slow}, by Daniel KAHNEMAN. This book, as well as \textit{Sapiens} (HARARI), \textit{Society of Mind} (MINSKY) and \textit{Rethinking Consciousness} (GRAZIANO) are four pillars of the theory about Human minds. The framework used in this field dates back to Descartes who separated, in the 17th century, intuitive and conscious reasoning. However, it was brought back to life by Posner and Synder in 1975 and was reformulated to a more recent description by Kahneman, who introduced the so-called System 1 (\so)and System 2 (\st).

\so is fast, unconscious, automatic, error prone and is responsible for everyday decisions. \st on the other hand is slow, conscious, effortful, reliable and responsible for complex decisions. It may appear that these systems work in opposite manners, but they are actually complementary, as we shall show below. \so quickly reacts in a local and parrallel fashion, almost like a reflex, whereas \st takes time to analyze and consider its environment before performing a more accurate response. It is important to understand that the border between the two systems is blurred. Some actions may initially be managed by \so, before moving onto \st once they become more natural, more intuitive. Complex arithmetic operations for example require a lot of cognitive intensity at first, but may become routine given enough practice.

The link with AI is now straightforward, one may easily imagine a multitude of problems where a quick decision is needed, even suboptimal, before performing a more accurate decision. This however leads to several questions: Do we always need a more accurate decision from \st? Who should decide whether such a decision is required? Is the use of \st worth it given the associated costs? The key to those questions lays in meta-cognitive agents. They take as input the world (problems, actions, environment), the two systems (past decisions, rewards, cost) and other considerations such as knowledge and beliefs about other agents. Then they should assess available resources, the expected cost of using \st and the expected reward associated with getting a correct solution. Their (broad) role is to determine how to best use \so and \st. To perform this task, meta-cognitive agents take a two-phase approach. In phase one, the meta-cognitive agent (\mco) asserts whether there are enough resources to perform both \so and \st. If not, \so is chosen. Then \mco determines the confidence of \so. If it is higher than the expected reward, then \so is chosen. The second meta-cognitive agent (\mct) then comes into play. It computes the gain of using \st (reward minus cost) and compares it to the expected reward of \so. If the former is greater than the latter, \st should be used, otherwise \so suffices.

Meta-cognitive agents are also responsible for a major topic in AI: trustworthiness. They can take into consideration fairness, robustness, explainability before choosing whether to apply \st. In summary, they must be logic-based so that a reasoning can be extracted from them. The clearer this reasoning, the better.

What we presented is actually a simplified model and more sophisticated ones exist, \textit{e.g.} where there are multiple \so's and \st's. Some more sophisticated models are part of Multi-alternative Decision Field Theory (\mdft). As the name hints, \mdft considers multiple options as an input and outputs one of these options, ideally the best one. This model is weighted over attributes, where the weights are updated according to the similarity between multiple options. Eventually, either when the maximum number of iterations is reached, or when a given value of a quality metric is obtained, the algorithm should stop.

The \so- \st framework is an interesting framework, not only because it works in practice, but also because it models the way of thinking of the human mind. Both this framework and its more sophisticated counterpart, the \mdft, arise from natural concepts in cognitive theory. Another field inspired by cognitive and neural theory is deep learning. Merging the \so- \st framework with deep learning seems to be an interesting idea. Maybe it could lead to more explainability in the deep learning so-called ``black-box'', and therefore to improvements with respect to fairness in AI?







% \newpage

% \begin{itemize}
%   \item AI lacks capabilities that humans have. See how humans can help and bring them in machine world
%   \item Focus on human capabilities that machines do not have, exploit cognitive theories of human decision making, identify causes enabling such capabilities, [fourth point]
%   \item Four books:
%         \subitem TFTS
%         \subitem Sapiens: historian point of view
%         \subitem Society of mind: how we interact collectively
%         \subitem theory of consciousness: how humans model the world
%   \item System 1, 2 = TF, TS (respectively)
%         \subitem TF: automatic, fast thinking, error prone, unconscious bias, used when problem solved is easy, reactive mode, make quick decisions
%         \subitem TS: slow, deliberate decision making, effortful, requires all attention, not many parrallel decisions, used when problem is cognitively difficult or we really care to solve it correctly (because many things depend on the problem). Sometimes override TF.
%         \subitem We usually go from system 2 (TS) to system 1 (TF). Example: with child, start with TS, then go to TF when they can read. But not always: arithmetic operations.
%   \item TFTS inspires AI. AAAI 2020 panel, CERN SPARK podcast of Francesca.
%   \item Different approach
%         \subitem Multi-agent architecture:
%         \subsubitem TF solvers: rely on past experience, don't look at caracteristics of the problem / similar problems and what we know about the environment. Very fast, react (activate automatically) when new problems arise if their skills are relevant for the problem to be solved. Propose solution and assert confidence.
%         \subsubitem TS solvers: Don't rely too much on past experience, even though they have access to it. Computational complexity can be slow depending on size of input. Activate by meta-cognition.
%         \subsubitem Model solvers / updaters: Act in the backgorund to update model used by agents to do their job.
%   \item Matacognition: cognition about cognition. Improves the quality of the system's decision. Choice of author is to use a centralized meta-cognitive agent. Assert TF vs TS.
%   \item S1 solvers do not wait to be asked to start solving a problem + give confidence. Model solvers assert quality of S1 agent and decide if activation of S2 is required. Model solvers assesses available resources, expected cost of using S2, expected reward for correct solution and solvers. 2 phases: quick assesment, then more reliable (longer one) if needed.
%   \item Two metacognitive phases:
%         \subitem Goal: avoid using S2 when unnecessary (i.e. expected reward - expected code $>$ what S1 can provide)
%         \subitem 2 phases: 1) check if enough resources for S1 and S2, if not, choose TF
%   \item Design choices:
%         \subitem S1 by default
%         \subitem S2 may not be better than S1
%         \subitem In more complex scenarios: there are several S1 and S2. MC need to choose among them
%         \subitem AI trustworthiness: take into consideration fairness, robustness, explainability, ...etc and MC must be explainable and logic-based.
%   \item SOFAI vs neuro-symbolic
%         \subitem We do not assume that S1 are data-driven and S2 are logic-based.
%   \item Human reasoning
%         \subitem Human deliberation (Multi dimensional theory: MDFT)
%         \subitem Results: learn the model of the world fril human deomonstrations, comparisaon of MDFT and RL
%   \item MDFT: model how people make decisions about a set of options by choosing an option based on their preference and accumulation depending on discount factors and option similarity. Then stop criteria (number of iterations (ie time) or satisfaction with solution). Stopping times are used (with time and upper bound)
%   \item MDFT vs RL: Machine is given a state and has to build a path towards a goal. There are penalties for violating constraints, for going for state where the agent is not supposed to go.
%   \item S1 uses probability distribution based on history. S2 uses MDFT. MC decides between move proposed by S1 with confidence level and activating and MDFT (S2).
%   \item Initially, system has no S1 and only S2, then at some point there is an option to use S1.
% \end{itemize}

% \paragraph{Cem's notes}

% Thinking fast and slow AI \\
% Francesca Rossi IBM research \\
%  \\
% How human make decision to advance aı capability. \\
% Currently Aı lack of these slay-2 \\
% How humans can thise, what causes these abilities, \\
% Slide-3 \\
% A machine is compliantly different platform \\
% Slide-4 \\
% Kahneman= human decide, think fast or slow, how combine each other to make decisions \\
%  \\
% Harari=Tries to nderstand how humans got this decision making over evolutionary. How we got here in decision making. How evolve? \\
% Minsky= Focus on the idea our mind is not single box as collection of many specialities work together. \\
% Graziano= How humans model world, awere in their… Metacognition? \\
%  \\
% Slide-5 \\
% System1= fast modaltity in terms of time. Almost automatic decision everyday life. Error prone, embeds uncousnious bias. \\
% Cognitively easy, we don’t need to pay too much attention to decide \\
%  \\
% System-2 \\
% Slow?= Effortful, requires attention, we use it when the problem is cognitively difficult. We really care when w care to solve it correctly. \\
%  \\
% System2--$>$SYSTEM1: When we don’t know problem we use system2 then we get use to  it and go system1. \\
% For ex: Child reads letter then without effort read book. \\
%  \\
% Slide-6 These theory .. \\
% Slide-7 \\
% We use multi agent arkitectute where we have several agents. Some of them solve problems, some use system1 some system2. Some update the world.What is solver--$>$  \\
% System1 solver-$>$ They just rely on past experience /similar problems. They’re working constant time (very fast). They activate themselves if they think problem is relevant their skills \\
% System-2 solver-$>$ They cannot start working unless meta cognitive agent activate them.  \\
% System updater-$>$ update \\
%  \\
% Slide-8 \\
% Any cog process that is reason of another meta cognition? \\
% Main goal-$>$ to improve the quality of decision. To assess the … \\
% Slide-9 \\
% System-1 solver does not wait meta cognitive agent. They start moving when problem come \\
% Doesnot trust system1 solution so it activate system2 solver (meta cog agent) \\
%  \\
% Meta cog agent-$>$ 1. P… \\
%  \\
% Slide-10 \\
% THE SOFAI architecture \\
% Meta cog agent--$>$ choose system1 solution or activate system2 solver. \\
%  \\
% Slide11 \\
% Avoid activate system2 solver because it is more costly.  \\
% Pros executed reward \\
% Cons: \\
% Slide12 \\
% S1 agent start immediately like human.  If S1 didn’t start and propose to meta agent  \\
%  \\
% There is no assumption that s1$>$s2 or s2$>$s1. \\
% Perception task generally s1$>$s2 \\
%  \\
% In this environment \\
%  \\
% Slide13 \\
% Slide-14 \\
% Project  \\
% Slide-15  \\
% Scnerio 1- human resoning. \\
% Whether this system behaves better  \\
% Slide-16 \\
% MDFT \\
% Slide17 \\
% MDFT example: 3 options, 2attributes \\
% w=[0.45, 0.55] \\
% Slide18 \\
% MDFT vs RL \\
% Slide19 \\
%  meta agent, setting weights In a way tries \\
% …time is out. \\
% This architecture can create … \\
%  \\
% In begging use s2, and at some point switch s1 because it get use to solve problems. \\
% S2 helps exploration \\




\end{document}