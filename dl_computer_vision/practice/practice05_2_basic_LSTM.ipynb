{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "qsbaXgie_mfj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "\n",
        "def split_sequence(sequence, n_steps):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequence)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the sequence\n",
        "    if end_ix > len(sequence) - 1:\n",
        "      break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return array(X), array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Q19Nxoak_7ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6, 3)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define input sequence\n",
        "raw_seq = [*range(10, 100, 10)]\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "num_classes = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6, 3)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape[0], X.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "QnIbea-yAA_m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_44 (LSTM)              (None, 100)               40800     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,901\n",
            "Trainable params: 40,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# To_Do: define a single layer LSTM model with 100 neurons, with relu activation and its input shape and compile it\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=100, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJdP_vT1kQUR"
      },
      "source": [
        "Stacked LSTM below!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "DQHurNdmol08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_14 (Bidirecti  (None, 3, 100)           20800     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " lstm_46 (LSTM)              (None, 3, 50)             30200     \n",
            "                                                                 \n",
            " lstm_47 (LSTM)              (None, 50)                20200     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 71,251\n",
            "Trainable params: 71,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# To_Do: define a stack of 3 layers of stacked LSTMs, each with 50 neurons  and compile it\n",
        "model = Sequential()\n",
        "model.add(\n",
        "  Bidirectional(\n",
        "    LSTM(\n",
        "      50, activation=\"relu\", input_shape=(n_steps, n_features), return_sequences=True\n",
        "    )\n",
        "  )\n",
        ")\n",
        "model.add(LSTM(50, activation=\"relu\", return_sequences=True))\n",
        "model.add(LSTM(50, activation=\"relu\"))\n",
        "model.add(Dense(num_classes))\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.build(input_shape=[None, n_steps, n_features])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15f7843d60>"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, epochs=500, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 461ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[69.22727]], dtype=float32)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test = np.array([[60, 50, 40]]).reshape((1, n_steps, n_features))\n",
        "model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPj5k2KskUIT"
      },
      "source": [
        "Bidirectional LSTM below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "CfbEIILIpbdr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_15 (Bidirecti  (None, 3, 100)           20800     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_16 (Bidirecti  (None, 3, 100)           60400     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " lstm_50 (LSTM)              (None, 3, 50)             30200     \n",
            "                                                                 \n",
            " lstm_51 (LSTM)              (None, 50)                20200     \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,651\n",
            "Trainable params: 131,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Flatten\n",
        "# To_Do: define a bidirectional LSTM model with one forward and one backward LSTM layers, each with 50 neurons\n",
        "# To_Do: define a stack of 3 layers of stacked LSTMs, each with 50 neurons  and compile it\n",
        "model = Sequential()\n",
        "model.add(\n",
        "  Bidirectional(\n",
        "    LSTM(\n",
        "      50, activation=\"relu\", input_shape=(n_steps, n_features), return_sequences=True\n",
        "    )\n",
        "  )\n",
        ")\n",
        "model.add(\n",
        "  Bidirectional(\n",
        "    LSTM(\n",
        "      50, activation=\"relu\", input_shape=(n_steps, n_features), return_sequences=True\n",
        "    )\n",
        "  )\n",
        ")\n",
        "model.add(LSTM(50, activation=\"relu\", return_sequences=True))\n",
        "model.add(LSTM(50, activation=\"relu\"))\n",
        "model.add(Dense(num_classes))\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.build(input_shape=[None, n_steps, n_features])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gut1RTrEAHlp",
        "outputId": "ad5ed74d-a3ef-4892-bf47-6c1b5fe61e48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15f5fd5600>"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_5qfQlOAXn-",
        "outputId": "4fe5f637-9641-4564-d974-de62cb2f6a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[68.86922]]\n"
          ]
        }
      ],
      "source": [
        "# demonstrate prediction\n",
        "y_hat = model.predict(X_test, verbose=0)\n",
        "print(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvDMTgPHnsT9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f0df963a0b00f23561625d2ba0a0b8b2d4dd4941b6b854c35050f99f4efed218"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
