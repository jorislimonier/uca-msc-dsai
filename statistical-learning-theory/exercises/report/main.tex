\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue, 
}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\thetahat}{\hat\theta}
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\var}{\operatorname{Var}}


\title{Statistical inference practice}
\author{Joris LIMONIER}
\begin{document}
\maketitle

\tableofcontents

\section{Inclass exercise January 12, 2022}
\subsection{Exercise 1}
Show that
\begin{equation}
  \E \left[ \hat{\mathcal{R}}_S (h) \right] = \mathcal{R}_{D, f} (h)
\end{equation}
\begin{align*}
  \E \left[ \hat{\mathcal{R}}_S (h) \right]
   & = \E \left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq y_i} \right] \\
   & = \frac{1}{n} \sum_{i=1}^n \E \left[ \mathbf{1}_{h(x_i) \neq y_i} \right] \\
   & = \frac{1}{n} \sum_{i=1}^n \P \left(h(x_i) \neq y_i \right)               \\
   & = \frac{1}{n} n  \P \left(h(x_i) \neq y_i \right)                         \\
   & = \P \left(h(x_i) \neq y_i \right)                                        \\
   & = \P \left(h(x_i) \neq f(x) \right)                                       \\
   & = \mathcal{R}_{D, f} (h)
\end{align*}

\subsection{Exercise 2}
We must prove that the variance of \(\hat{\mathcal{R}}_S (h) \to 0\)
\begin{align*}
  Var \left[ \hat{\mathcal{R}}_S (h) \right]
   & = Var \left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq y_i} \right]   \\
   & = Var \frac{1}{n^2} \left[ \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq y_i} \right] \\
\end{align*}

Let the \(Z_i\) be defined as follows:
\[\frac{1}{n} \sum_{i=1}^{n} \mathbf{1}_{h(x_i) \neq f(x_i)} =: \frac{1}{n} \sum_{i=1}^{n} Z_i\]
(not finished, see lecture 1 slides)

\section{Inclass exercise January 21, 2022}
\subsection{Exercise 1}
Set \(g(x) = \P(Y=1 \mid X=x)\). We define the Bayes optimal predictor as:
\begin{equation*}
  f_\mathcal{D}(x) =
  \begin{cases}
    1 & g(x) \geq 1/2    \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}

\paragraph{Question 1.}
Let \(h: \mathcal{X} \to \{0,1\}\) be a classifier. Show that
\begin{align*}
   & \P(h(X) \neq Y \mid X = x )                                                      \\
   & = g(x) \cdot \P(h(X) = 0 \mid X = x)) + (1 - g(x)) \cdot \P(h(X) = 1 \mid X = x)
\end{align*}
\begin{align*}
   & g(x) \cdot \P(h(X) = 0 \mid X = x)) + (1 - g(x)) \cdot \P(h(X) = 1 \mid X = x) \\
   & = \P(Y=1 \mid X = x) \cdot \P(h(X) = 0 \mid X = x))                            \\
   & + (1 - \P(Y=1 \mid X = x)) \cdot \P(h(X) = 1 \mid X = x)                       \\
   & = \P(Y=1 \cap h(X) = 0 \mid X = x))                                            \\
   & + \P(h(X) = 1 \mid X = x) - \P(Y=1 \cap h(X) = 1 \mid X = x)                   \\
   & = \P(Y=1 \cap h(X) = 0 \mid X = x)) + \P(Y=0 \cap h(X) = 1 \mid X = x)         \\
   & =\P(h(X) \neq Y \mid X = x)
\end{align*}

\paragraph{Question 2.}
Deduce that
\[
  \P(f_D (X) \neq Y \mid X = x) = \min (g(x), 1 - g(x))
\]

\begin{align*}
   & \P(f_D (X) \neq Y \mid X = x) \\
   & =
  \begin{cases}
    \P(1 \neq Y \mid X = x), & g(x) \geq 1/2 \\
    \P(0 \neq Y \mid X = x), & g(x) < 1/2    \\
  \end{cases}        \\
   & =
  \begin{cases}
    1 - g(x), & g(x) \geq 1-g(x) \\
    g(x),     & g(x) < 1 - g(x)  \\
  \end{cases}        \\
   & = \min (g(x), 1-g(x))
\end{align*}

\paragraph{Question 3.}
Show that
\begin{equation*}
  \P(h(X) \neq Y \mid X=x) \geq \P (f_D(x) \neq Y \mid X=x)
\end{equation*}

\begin{align*}
  \P (f_D(x) \neq Y \mid X=x)
   & = \min (g(x), 1-g(x))                             \\
   & = \min (g(x), 1-g(x))                             \\
   & \cdot (\P(h(X)=0 \mid X=x) + \P(h(X)=1 \mid X=x)) \\
   & \leq g(x) \cdot (\P(h(X)=0 \mid X=x)              \\
   & + (1-g(x)) \cdot \P(h(X)=1 \mid X=x))             \\
   & = \P(h(X) \neq Y \mid X=x)
\end{align*}

\paragraph{Question 4.}
Prove that
\begin{equation*}
  \mathcal{R}_\mathcal{D} (f_\mathcal{D}) \leq \mathcal{R}_\mathcal{D} (h)
\end{equation*}

\begin{align*}
           &
  \P (f_D(x) \neq Y \mid X=x) \leq \P(h(X) \neq Y \mid X=x)                               \\
  \implies &
  \E\left[\P (f_D(x) \neq Y \mid X=x)\right] \leq \E\left[\P(h(X) \neq Y \mid X=x)\right] \\
  \implies &
  \mathcal{R}_\mathcal{D} (f_\mathcal{D}) \leq \mathcal{R}_\mathcal{D} (h)
\end{align*}

\section{Inclass exercise January 28, 2022}
\subsection{Exercise 1}
Let \(Z\) be a random variable with a second moment such that \(\E[Z] = \mu\) and \(\var(Z) = \sigma^2\).
\subsubsection{Question 1}
Let \(g : t \mapsto \E[(Z-t)^2]\). Show that \(g\) is minimum at \(t = \mu\).
\begin{align*}
  g(t)
   & =
  \E\left[ (Z-t)^2 \right]                                       \\
   & =
  \E\left[ Z^2 + t^2 - 2tZ \right]                               \\
   & =
  \E\left[ Z^2\right] + \E\left[t^2\right] - \E\left[2tZ \right] \\
   & =
  \E\left[ Z^2\right] + t^2 - 2t\E\left[Z \right]                \\
   & =
  \sigma^2 - \mu^2 + t^2 - 2t\mu                                 \\
   & =
  \sigma^2 - \mu^2 + t^2 - 2t\mu                                 \\
\end{align*}
We differentiate with respect to \(t\):
\begin{align*}
           & \frac{\partial}{\partial t} g(t) = 0                             \\
  \implies &
  \frac{\partial}{\partial t} \left[\sigma^2 - \mu^2 + t^2 - 2t\mu\right] = 0 \\
  \implies &
  2t - 2\mu  = 0                                                              \\
  \implies &
  t = \mu                                                                     \\
\end{align*}


\subsubsection{Question 2}
Assume \(Z \in \left[a,b\right]\) almost surely. Use the previous question to show that
\begin{equation*}
  \var(Z) \leq \frac{(b-a)^2}{4}
\end{equation*}
\begin{align*}
           &
  g(\mu) \leq g(t)                                                          \\
  \implies &
  \var(Z) \leq \E\left[(Z-t)^2\right]                                       \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left(2Z - a - b \right)^2\right]        \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left((Z - a) + (Z - b) \right)^2\right] \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left((Z - a) - (b - Z) \right)^2\right] \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left(|Z - a| - |b - Z| \right)^2\right] \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left|(Z - a) - (Z - b)\right|^2\right]  \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left|b-a\right|^2\right]                \\
  \implies &
  \var(Z) \leq \frac{\left(b-a\right)^2}{4}                                 \\
\end{align*}


\subsubsection{Question 3}
Let \(Z_1, \ldots, Z_n \sim Z\) be i.i.d. Use Chebyshev inequality to obtain a concentration inequality for
\begin{equation*}
  Z := \frac{1}{n} \sum_{i=1}^n Z_i
\end{equation*}
Chebyshev inequality:
\begin{equation}
  \label{eqn: chebyshev inequality}
  \P\left( |Z - \E[Z]| \geq a\right) \leq \frac{\var Z}{a^2}
\end{equation}
\begin{align*}
  \var\left(Z\right)
                                                  & =
  \var\left(\frac{1}{n^2}\sum_{i=1}^n Z_i\right)                                     \\
                                                  & =
  \frac{1}{n^2} \sum_{i=1}^n \var\left(Z_i\right) & \textit{(\(Z_i\)'s independent)} \\
                                                  & \leq
  \frac{1}{n^2} \sum_{i=1}^n \frac{\left(b-a\right)^2}{4}                            \\
                                                  & \leq
  \frac{\left(b-a\right)^2}{4n}                                                      \\
\end{align*}
Then we apply \eqref{eqn: chebyshev inequality}:
\begin{align*}
           &
  \P\left( |Z - \E[Z]| \geq \varepsilon \right) \leq \frac{\var Z}{\varepsilon^2}                                                    \\
  \implies &
  \P\left( \left| \frac{1}{n} \sum_{i=1}^n Z_i - \mu\right| \geq \varepsilon \right) \leq \frac{\left(b-a\right)^2}{4n\varepsilon^2} \\
\end{align*}

\subsubsection{Question 4}




\end{document}