\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue, 
}

\newcommand{\1}{\mathbf{1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rcal}{\mathcal{R}}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\thetahat}{\hat\theta}
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\var}{\operatorname{Var}}


\title{Statistical learning theory}
\author{Joris LIMONIER}
\begin{document}
\maketitle

\tableofcontents

\section{Inclass exercise January 12, 2022}
\subsection{Exercise 1}
Show that
\begin{equation}
  \E \left[ \hat{\mathcal{R}}_S (h) \right] = \mathcal{R}_{D, f} (h)
\end{equation}
\begin{align*}
  \E \left[ \hat{\mathcal{R}}_S (h) \right]
   & = \E \left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq y_i} \right] \\
   & = \frac{1}{n} \sum_{i=1}^n \E \left[ \mathbf{1}_{h(x_i) \neq y_i} \right] \\
   & = \frac{1}{n} \sum_{i=1}^n \P \left(h(x_i) \neq y_i \right)               \\
   & = \frac{1}{n} n  \P \left(h(x_i) \neq y_i \right)                         \\
   & = \P \left(h(x_i) \neq y_i \right)                                        \\
   & = \P \left(h(x_i) \neq f(x) \right)                                       \\
   & = \mathcal{R}_{D, f} (h)
\end{align*}

\subsection{Exercise 2}
We must prove that the variance of \(\hat{\mathcal{R}}_S (h) \to 0\)
\begin{align*}
  Var \left[ \hat{\mathcal{R}}_S (h) \right]
   & = Var \left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq y_i} \right]   \\
   & = Var \frac{1}{n^2} \left[ \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq y_i} \right] \\
\end{align*}

Let the \(Z_i\) be defined as follows:
\[\frac{1}{n} \sum_{i=1}^{n} \mathbf{1}_{h(x_i) \neq f(x_i)} =: \frac{1}{n} \sum_{i=1}^{n} Z_i\]
(not finished, see lecture 1 slides)

\section{Inclass exercise January 21, 2022}
\subsection{Exercise 1}
Set \(g(x) = \P(Y=1 \mid X=x)\). We define the Bayes optimal predictor as:
\begin{equation*}
  f_\mathcal{D}(x) =
  \begin{cases}
    1 & g(x) \geq 1/2    \\
    0 & \text{otherwise}
  \end{cases}
\end{equation*}

\paragraph{Question 1.}
Let \(h: \mathcal{X} \to \{0,1\}\) be a classifier. Show that
\begin{align*}
   & \P(h(X) \neq Y \mid X = x )                                                      \\
   & = g(x) \cdot \P(h(X) = 0 \mid X = x)) + (1 - g(x)) \cdot \P(h(X) = 1 \mid X = x)
\end{align*}
\begin{align*}
   & g(x) \cdot \P(h(X) = 0 \mid X = x)) + (1 - g(x)) \cdot \P(h(X) = 1 \mid X = x) \\
   & = \P(Y=1 \mid X = x) \cdot \P(h(X) = 0 \mid X = x))                            \\
   & + (1 - \P(Y=1 \mid X = x)) \cdot \P(h(X) = 1 \mid X = x)                       \\
   & = \P(Y=1 \cap h(X) = 0 \mid X = x))                                            \\
   & + \P(h(X) = 1 \mid X = x) - \P(Y=1 \cap h(X) = 1 \mid X = x)                   \\
   & = \P(Y=1 \cap h(X) = 0 \mid X = x)) + \P(Y=0 \cap h(X) = 1 \mid X = x)         \\
   & =\P(h(X) \neq Y \mid X = x)
\end{align*}

\paragraph{Question 2.}
Deduce that
\[
  \P(f_D (X) \neq Y \mid X = x) = \min (g(x), 1 - g(x))
\]

\begin{align*}
   & \P(f_D (X) \neq Y \mid X = x) \\
   & =
  \begin{cases}
    \P(1 \neq Y \mid X = x), & g(x) \geq 1/2 \\
    \P(0 \neq Y \mid X = x), & g(x) < 1/2    \\
  \end{cases}        \\
   & =
  \begin{cases}
    1 - g(x), & g(x) \geq 1-g(x) \\
    g(x),     & g(x) < 1 - g(x)  \\
  \end{cases}        \\
   & = \min (g(x), 1-g(x))
\end{align*}

\paragraph{Question 3.}
Show that
\begin{equation*}
  \P(h(X) \neq Y \mid X=x) \geq \P (f_D(x) \neq Y \mid X=x)
\end{equation*}

\begin{align*}
  \P (f_D(x) \neq Y \mid X=x)
   & = \min (g(x), 1-g(x))                             \\
   & = \min (g(x), 1-g(x))                             \\
   & \cdot (\P(h(X)=0 \mid X=x) + \P(h(X)=1 \mid X=x)) \\
   & \leq g(x) \cdot (\P(h(X)=0 \mid X=x)              \\
   & + (1-g(x)) \cdot \P(h(X)=1 \mid X=x))             \\
   & = \P(h(X) \neq Y \mid X=x)
\end{align*}

\paragraph{Question 4.}
Prove that
\begin{equation*}
  \mathcal{R}_\mathcal{D} (f_\mathcal{D}) \leq \mathcal{R}_\mathcal{D} (h)
\end{equation*}

\begin{align*}
           &
  \P (f_D(x) \neq Y \mid X=x) \leq \P(h(X) \neq Y \mid X=x)                               \\
  \implies &
  \E\left[\P (f_D(x) \neq Y \mid X=x)\right] \leq \E\left[\P(h(X) \neq Y \mid X=x)\right] \\
  \implies &
  \mathcal{R}_\mathcal{D} (f_\mathcal{D}) \leq \mathcal{R}_\mathcal{D} (h)
\end{align*}

\section{Inclass exercise January 28, 2022}
\subsection{Exercise 1}
Let \(Z\) be a random variable with a second moment such that \(\E[Z] = \mu\) and \(\var(Z) = \sigma^2\).
\subsubsection{Question 1}
Let \(g : t \mapsto \E[(Z-t)^2]\). Show that \(g\) is minimum at \(t = \mu\).
\begin{align*}
  g(t)
   & =
  \E\left[ (Z-t)^2 \right]                                       \\
   & =
  \E\left[ Z^2 + t^2 - 2tZ \right]                               \\
   & =
  \E\left[ Z^2\right] + \E\left[t^2\right] - \E\left[2tZ \right] \\
   & =
  \E\left[ Z^2\right] + t^2 - 2t\E\left[Z \right]                \\
   & =
  \sigma^2 - \mu^2 + t^2 - 2t\mu                                 \\
   & =
  \sigma^2 - \mu^2 + t^2 - 2t\mu                                 \\
\end{align*}
We differentiate with respect to \(t\):
\begin{align*}
           & \frac{\partial}{\partial t} g(t) = 0                             \\
  \implies &
  \frac{\partial}{\partial t} \left[\sigma^2 - \mu^2 + t^2 - 2t\mu\right] = 0 \\
  \implies &
  2t - 2\mu  = 0                                                              \\
  \implies &
  t = \mu                                                                     \\
\end{align*}


\subsubsection{Question 2}
Assume \(Z \in \left[a,b\right]\) almost surely. Use the previous question to show that
\begin{equation*}
  \var(Z) \leq \frac{(b-a)^2}{4}
\end{equation*}
\begin{align*}
           &
  g(\mu) \leq g(t)                                                          \\
  \implies &
  \var(Z) \leq \E\left[(Z-t)^2\right]                                       \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left(2Z - a - b \right)^2\right]        \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left((Z - a) + (Z - b) \right)^2\right] \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left((Z - a) - (b - Z) \right)^2\right] \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left(|Z - a| - |b - Z| \right)^2\right] \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left|(Z - a) - (Z - b)\right|^2\right]  \\
  \implies &
  \var(Z) \leq \frac{1}{4} \E\left[\left|b-a\right|^2\right]                \\
  \implies &
  \var(Z) \leq \frac{\left(b-a\right)^2}{4}                                 \\
\end{align*}


\subsubsection{Question 3}
Let \(Z_1, \ldots, Z_n \sim Z\) be i.i.d. Use Chebyshev inequality to obtain a concentration inequality for
\begin{equation*}
  Z := \frac{1}{n} \sum_{i=1}^n Z_i
\end{equation*}
Chebyshev inequality:
\begin{equation}
  \label{eqn: chebyshev inequality}
  \P\left( |Z - \E[Z]| \geq a\right) \leq \frac{\var Z}{a^2}
\end{equation}
\begin{align*}
  \var\left(Z\right)
                                                  & =
  \var\left(\frac{1}{n^2}\sum_{i=1}^n Z_i\right)                                     \\
                                                  & =
  \frac{1}{n^2} \sum_{i=1}^n \var\left(Z_i\right) & \textit{(\(Z_i\)'s independent)} \\
                                                  & \leq
  \frac{1}{n^2} \sum_{i=1}^n \frac{\left(b-a\right)^2}{4}                            \\
                                                  & \leq
  \frac{\left(b-a\right)^2}{4n}                                                      \\
\end{align*}
Then we apply \eqref{eqn: chebyshev inequality}:
\begin{align*}
           &
  \P\left( |Z - \E[Z]| \geq \varepsilon \right) \leq \frac{\var Z}{\varepsilon^2}                                                    \\
  \implies &
  \P\left( \left| \frac{1}{n} \sum_{i=1}^n Z_i - \mu\right| \geq \varepsilon \right) \leq \frac{\left(b-a\right)^2}{4n\varepsilon^2} \\
\end{align*}

\section{In-class exercise February 4, 2022}
\subsection{Question 1}
We define our loss as:
\begin{equation*}
  \ell(y, y') = |y - y'|
\end{equation*}
Show:
\begin{equation*}
  \forall\ c \in \R,\ \begin{cases}
    |c| = \min_{a \geq 0} a \\
    s.t. \quad a \geq c     \\
    \qquad a \geq -c
  \end{cases}
\end{equation*}
A function study of \(x \mapsto |x|\) gives the result.

\subsection{Question 2}
ERM consists in finding the following quantity:
\begin{align*}
  \min_{w \in \R} \mathcal{R}_S (w)
   & = \min_{w \in \R} \frac{1}{n} \sum_{i=1}^n |\langle w_i x_i \rangle - y_i| \\
\end{align*}
% \begin{align*}
%   \ell(y, y')
%    & = |y - y'|                                       \\
%    & \forall\ w \in \R^n,\ \begin{cases}
%     |c| = \min_{a \geq 0} a \\
%     s.t. \quad a \geq c     \\
%     \qquad a \geq -c
%   \end{cases}
% \end{align*}





\section{In-class exercise February 22, 2022}
\subsection{Question 1}
Show that ERM with the logistic loss is equivalent to minimizing
\begin{equation*}
  F(w) = \sum_{i=1}^{n} \log \left( 1 + \exp \left( - \tilde{y}_{i}\left\langle w, x_{i}\right\rangle\right)\right)
\end{equation*}
with \(\tilde{y}_i = \operatorname{sign}(y_i - 0.5)\). Deduce that \(\hat{\Rcal}\) is a convex function of \(w\). \\
We have:
\begin{align*}
  \ell \left(y, y_i\right)
   & =
  \begin{cases}
    - \log (1 - \hat y) & y = 0 \\
    - \log (\hat y)     & y = 1
  \end{cases}                      \\
   & =
  \begin{cases}
    - \log \left(1 - \frac{1}{1 + e^{-w^T x_i}}\right) & y = 0 \\
    - \log \left(\frac{1}{1 + e^{-w^T x_i}}\right)     & y = 1
  \end{cases}                      \\
   & =
  \begin{cases}
    - \log \left(\frac{e^{-w^T x_i}}{1 + e^{-w^T x_i}}\right) & y = 0 \\
    - \log \left(\frac{1}{1 + e^{-w^T x_i}}\right)            & y = 1
  \end{cases}                      \\
   & =
  \begin{cases}
    \log \left(\frac{1 + e^{-w^T x_i}}{e^{-w^T x_i}}\right) & y = 0 \\
    \log \left( 1 + e^{-w^T x_i} \right)                    & y = 1
  \end{cases}                      \\
   & =
  \begin{cases}
    \log \left( 1 + \frac{1}{e^{-w^T x_i}}\right) & y = 0 \\
    \log \left( 1 + e^{-w^T x_i} \right)          & y = 1
  \end{cases}                      \\
   & =
  \begin{cases}
    \log \left(1 + e^{w^T x_i} \right)   & y = 0 \\
    \log \left( 1 + e^{-w^T x_i} \right) & y = 1
  \end{cases}                      \\
   & =
  \log \left(1 + e^{-\tilde{y_i} w^T x_i} \right) \\
   & =
  \log \left(1 + e^{(-1)^{y_i} w^T x_i} \right)   \\
\end{align*}
% Convex: \(t f(w_1) + (1-t) f(w_2) \geq f(tw_1 + (1-t)w_2)\)
\begin{align*}
   & \frac{\partial^2}{\partial w^2} \log \left(1 + e^{(-1)^{y_i} w^T x_i} \right)                                                                          \\
   & = \frac{\partial}{\partial w} \frac{-y_i x_i e^{-\tilde{y_i} w^T x_i}}{1 + e^{-\tilde{y_i} w^T x_i}}                                                   \\
   & = \frac{(y_i x_i)^2 e^{-\tilde{y_i} w^T x_i} (1 + e^{-\tilde{y_i} w^T x_i}) - (-y_i x_i e^{-\tilde{y_i} w^T x_i})^2}{(1 + e^{-\tilde{y_i} w^T x_i})^2} \\
   & = \frac{x_i^2 e^{-\tilde{y_i} w^T x_i} (1 + e^{-\tilde{y_i} w^T x_i}) - x_i^2 e^{-2\tilde{y_i} w^T x_i}}{(1 + e^{-\tilde{y_i} w^T x_i})^2}             \\
   & = \frac{x_i^2 e^{-\tilde{y_i} w^T x_i}}{(1 + e^{-\tilde{y_i} w^T x_i})^2}                                                                              \\
   & \geq 0
\end{align*}

\subsection{Question 2}
Compute the gradient of \(\hat\Rcal\) with respect to \(w\). \\
Hint: show that \(\phi'(z) = \phi(z)(1 - \phi(z))\)
\begin{align*}
  \frac{d}{dz} \phi(z)
   & = \frac{d}{dz} \frac{1}{1 + e^{-z}}                      \\
   & = -\frac{-e^{-z}}{\left(1 + e^{-z}\right)^2}             \\
   & = \frac{1 - 1 + e^{-z}}{\left(1 + e^{-z}\right)^2}       \\
   & = \frac{1}{1 + e^{-z}} \frac{- 1 + e^{-z}}{1 + e^{-z}}   \\
   & = \frac{1}{1 + e^{-z}} \frac{e^{-z}}{1 + e^{-z}}         \\
   & = \frac{1}{1 + e^{-z}} \frac{1 + e^{-z} - 1}{1 + e^{-z}} \\
   & = \phi(z)(1 - \phi(z))                                   \\
\end{align*}
\begin{align*}
  \hat\Rcal (w)
   & = \sum_{i=1}^n \ell(y_i, \hat y_i)                                                 \\
   & = \sum_{i=1}^n -(1-y) \log \left(1 - \hat{y_i}\right) - y_i \log \hat{y_i}         \\
   & = \sum_{i=1}^n -(1-y) \log \left(1 - \phi(w^T x_i)\right) - y_i \log \phi(w^T x_i) \\
\end{align*}
For some \(1 \leq j \leq n\):
\begin{align*}
  \frac{\partial}{\partial w_j} \ell\left(y, \hat{y}\right)
   & = \frac{\partial}{\partial w_j} -(1-y) \log \left(1 - \phi(w^T x_i)\right) - y_i \log \phi(w^T x_i) \\
\end{align*}
Final result:
\begin{equation*}
  \hat\Rcal = \sum_{i=1}^n \left(\phi(w^T x_i) - y_i\right)x_{ij}
\end{equation*}

\section{In class exercises February 25, 2022}
\subsection{Question 1}
Prove that the following function is a kernel:
\begin{equation*}
  k(x,y) = 2^{x+y}
\end{equation*}
\paragraph{Symmetry} Trivial
\paragraph{Positive definiteness}
\begin{align*}
  \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i, x_j)
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j 2^{x_i + x_j}   \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j 2^{x_i} 2^{x_j} \\
   & = \sum_{i=1}^n c_i 2^{x_i} \sum_{j=1}^n c_j 2^{x_j} \\
   & = \left[\sum_{i=1}^n c_i 2^{x_i}\right]^2           \\
  \geq 0
\end{align*}

\subsection{Question 2}
Prove that the following function is a kernel:
\begin{equation*}
  k(x,y) = (x^T y)^2
\end{equation*}
\paragraph{Symmetry} Trivial

\paragraph{Positive definiteness}
\begin{align*}
  \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i, x_j)
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j (x_i^T x_j)^2                           \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j (x_i^T x_j) (x_i^T x_j)                 \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j (x_j^T x_i) (x_i^T x_j)                 \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j \operatorname{tr} (x_j^T x_i x_i^T x_j) \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j \operatorname{tr} (x_i x_i^T x_j x_j^T) \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j \langle x_i x_i^T, x_j x_j^T \rangle    \\
   & = \sum_{i=1}^n c_i x_i x_i^T \sum_{j=1}^n c_j x_j x_j^T                     \\
   & = \left[\sum_{i=1}^n c_i x_i x_i^T\right]^2                                 \\
   & \geq 0
\end{align*}


\subsection{Question 3}
Prove that the following function is a kernel:
\begin{equation*}
  k(x,y) = \cos(x-y)
\end{equation*}
\paragraph{Symmetry} Trivial

\paragraph{Positive definiteness}

\begin{align*}
   & \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i, x_j)                                                                   \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j \cos(x_i - x_j)                                                             \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j \left[\cos(x_i) \cos(x_j) + \sin(x_i) \sin(x_j)\right]                      \\
   & = \sum_{i=1}^n \sum_{j=1}^n c_i c_j \cos(x_i) \cos(x_j) + \sum_{i=1}^n \sum_{j=1}^n c_i c_j \sin(x_i) \sin(x_j) \\
   & = \left[\sum_{i=1}^n c_i \cos(x_i)\right]^2 + \left[\sum_{i=1}^n c_i \sin(x_i)\right]^2                         \\
   & \geq 0
\end{align*}

\subsection{Exercise}
Show that the Gaussian kernel, which is given by:
\begin{equation*}
  k(x,y) := \exp \left(- \frac{\|x-y\|^2}{2\nu^2}\right)
\end{equation*}
is actually a kernel. \\
We have that:
\begin{equation*}
  \exp \left(- \frac{\|x-y\|^2}{2\nu^2}\right) = \lim_{n \to \infty} \sum_{p=1}^n \frac{1}{p!} \left(\frac{-\|x-y\|^2}{2\nu^2}\right)^p
\end{equation*}
\begin{align*}
           & \|x-y\|^2 = (x-y)^T (x-y)                                  \\
  \implies &
  \|x-y\|^2 = x^T x - x^T y - y^T x + y^T y                             \\
  \implies &
  \|x-y\|^2 = \|x\|^2 - 2x^T y + \|y\|^2                                \\
  \implies &
  - \|x-y\|^2 = \underbrace{2x^T y}_{\text{kernel}} - \|x\|^2 - \|y\|^2 \\
\end{align*}

Let us show that \(\exp\left(- \|x\|^2 - \|y\|^2\right)\) is a kernel:
\begin{align*}
  \exp\left(- \|x\|^2 - \|y\|^2\right)
   & = \exp\left(- \|x\|^2 \right) \exp \left(- \|y\|^2\right)                  \\
   & = \langle \exp\left(- \|x\|^2 \right), \exp \left(- \|y\|^2\right) \rangle \\
\end{align*}

\section{In-class exercises March 9, 2022}
\subsection{Exercise}
Let \(S = \{ x_1, \ldots, x_n\}\) be a finite set of points in \(\mathcal{X}\). Compute the distance to the barycenter of \(S\) in the RKHS. \\
The distance to the barycenter is given by:
\begin{align*}
  d^2
   & := \left\| \Phi(x_{i_0}) - \frac{1}{n} \sum_{i=1}^n \Phi(x_i) \right\|^2                                                                                                                                                                       \\
   & = \left\langle \Phi(x_{i_0}) - \frac{1}{n} \sum_{i=1}^n \Phi(x_i), \Phi(x_{i_0}) - \frac{1}{n} \sum_{i=1}^n \Phi(x_i) \right\rangle                                                                                                            \\
   & = \left\langle \Phi(x_{i_0}), \Phi(x_{i_0}) \right\rangle + \left\langle \frac{1}{n} \sum_{i=1}^n \Phi(x_i), \frac{1}{n} \sum_{i=1}^n \Phi(x_i) \right\rangle - 2 \left\langle \Phi(x_{i_0}), \frac{1}{n} \sum_{i=1}^n \Phi(x_i) \right\rangle \\
   & = \left\langle k(x_{i_0}), k(x_{i_0}) \right\rangle + \frac{1}{n^2} \left\langle \sum_{i=1}^n \Phi(x_i), \sum_{i=1}^n \Phi(x_i) \right\rangle - 2 \left\langle \Phi(x_{i_0}), \frac{1}{n} \sum_{i=1}^n \Phi(x_i) \right\rangle                 \\
\end{align*}

\subsection{Exercise}
Show that for any matrices \(A, B\), \(A\left(BA + \gamma I\right)^{-1} = \left(AB + \gamma I\right)^{-1} A\).
\begin{align*}
  &
  A\left(BA + \gamma I\right)^{-1} = \left(AB + \gamma I\right)^{-1} A \\
  \implies &
  \left(AB + \gamma I\right) A = A \left(BA + \gamma I\right) \\
  \implies &
  ABA + \gamma A = ABA + \gamma A \\
\end{align*}
The linear kernel is given by \(k(x, y) = x^T y\), so \(K\)


\end{document}