---
title: "Untitled"
author: "CB"
date: "2022-10-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The EM algorithm for the univariate Gaussian mixture model

The EM algorithm will be used here to infer (find the best set of parameters) an univariate GMM on a set of data.

Let's remind that the equations for the E and M steps are as follows:

-   E step: $$t_{ik} = E[y_{ik}|\theta^*,X] \propto \pi^*_k \mathcal{N}(x_i;\mu^*_k,\sigma^{2*}_k)$$ and $t_{ik} \in [0,1]$.

-   M step: we optimize $Q(\theta,\theta^*)$ to get the updated parameters:

    -   $\pi_k = \sum_{i=1}^n t_{ik} / n$,
    -   $\mu_k = \frac{1}{n_k}\sum_{i=1}^n t_{ik} x_i$,
    -   $\sigma^{2*}_k = \frac{1}{n_k}\sum_{i=1}^n t_{ik} (x_i - \mu_k)^2$, where $n_k = \sum_{i=1}^n t_{ik}$.

Let's now code this:

```{r}
# Code for the EM algo for an univaraite (p=1) GMM
myEM <- function(X,K,maxit=50,disp=FALSE,...){
   n = length(X); Lik = rep(0,maxit)
   T = matrix(NA,nrow=n,ncol=K)
   
   # Initialization of \theta
   prop = rep(1/K,K)
   sigma = rep(1,K)
   mu = rnorm(K,mean=mean(X),sd=1)
   
   # The main loop of the EM algo
   for (it in 1:maxit){
      # The E step
      for (k in 1:K){
         T[,k] = prop[k] * dnorm(X,mean=mu[k],sd=sqrt(sigma[k]))
      }
      T = T / rowSums(T) %*% matrix(1,nrow=1,ncol=K) # for normalizing
      
      # The M step
      for (k in 1:K){
         prop[k] = sum(T[,k]) / n
         mu[k] = sum(T[,k] * X) / sum(T[,k])
         sigma[k] = sum(T[,k] * (X-mu[k])^2) / sum(T[,k])
      }
      
      # Visualization
      grp = max.col(T)
      if (disp){ plot(X,rep(1,n),col=grp);
         points(mu,rep(1,3),pch=19,cex=2,col=1:3); Sys.sleep(0.5)}
      
      # Likelihood evaluation
      # To do at home using the sum-log-exp trick explained last week!
   }
   # Returning the results
   list(prop=prop,mu=mu,sigma=sigma,T=T,grp=grp,Lik=Lik)
}
```

This code can be then tested on some simulated data set:

```{r}
mu = c(0,2,-2)
sigma = c(0.2,0.3,0.2)
X = c(rnorm(100,mu[1],sqrt(sigma[1])),
      rnorm(100,mu[2],sqrt(sigma[2])),
      rnorm(100,mu[3],sqrt(sigma[3])))
cls = rep(1:3,rep(100,3))
plot(X,col = cls)

out = myEM(X,K=3)
plot(X,col = out$grp)
out$mu
```

The values stored in `out$T` allow to identify the observations that have an uncertain clustering.

```{r}
plot(out$Lik)
```
