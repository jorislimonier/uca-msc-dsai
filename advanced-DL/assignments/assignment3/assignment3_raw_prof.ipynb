{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "976839e8",
      "metadata": {
        "papermill": {
          "duration": 0.013333,
          "end_time": "2022-04-09T14:37:30.852431",
          "exception": false,
          "start_time": "2022-04-09T14:37:30.839098",
          "status": "completed"
        },
        "tags": [],
        "id": "976839e8"
      },
      "source": [
        "\n",
        "\n",
        "# Transformers and Multi-Head Attention\n",
        "\n",
        "In this laboratory, we will discuss the most impactful architecture over the last 5 years: the Transformer model.\n",
        "Since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. had been published in 2017,\n",
        "the Transformer architecture has continued to beat benchmarks in many domains, prominently in Natural Language Processing but also in many related fields (e.g., [Computer Vision](https://arxiv.org/abs/2010.11929)).\n",
        "\n",
        "Here are two examples of the amazing Transformer applications:\n",
        "\n",
        "*   [DALLÂ·E 2](https://openai.com/dall-e-2/): text 2 image generator\n",
        "*   [GATO](https://arxiv.org/pdf/2205.06175.pdf): multi-modal multi-task learning model\n",
        "\n",
        "\n",
        "As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook.\n",
        "\n",
        "For this notebook we will employ [Pytorch Ligthining](https://pytorch-lightning.readthedocs.io/en/stable/), which is a high-level Pytorch library particularly suited for Transformers \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbeb1c6",
      "metadata": {
        "papermill": {
          "duration": 0.011303,
          "end_time": "2022-04-09T14:37:30.875464",
          "exception": false,
          "start_time": "2022-04-09T14:37:30.864161",
          "status": "completed"
        },
        "tags": [],
        "id": "acbeb1c6"
      },
      "source": [
        "## Setup\n",
        "This notebook requires some packages besides pytorch-lightning. It may take a while to setup the environment. After that you will also need to **restart** the runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3788f0d0",
      "metadata": {
        "papermill": {
          "duration": 0.011526,
          "end_time": "2022-04-09T14:37:34.434868",
          "exception": false,
          "start_time": "2022-04-09T14:37:34.423342",
          "status": "completed"
        },
        "tags": [],
        "id": "3788f0d0"
      },
      "source": [
        "\n",
        "Below, we import some standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c0e503",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:34.458897Z",
          "iopub.status.busy": "2022-04-09T14:37:34.458462Z",
          "iopub.status.idle": "2022-04-09T14:37:37.002927Z",
          "shell.execute_reply": "2022-04-09T14:37:37.002262Z"
        },
        "papermill": {
          "duration": 2.558559,
          "end_time": "2022-04-09T14:37:37.004560",
          "exception": false,
          "start_time": "2022-04-09T14:37:34.446001",
          "status": "completed"
        },
        "tags": [],
        "id": "58c0e503",
        "outputId": "427c617b-f580-4682-cb6a-f91dc1640229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "from functools import partial\n",
        "from urllib.error import HTTPError\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "# Plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"data/\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"saved_models/\"\n",
        "\n",
        "# Set seed to ensure that all operations are deterministic for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44446fb8",
      "metadata": {
        "papermill": {
          "duration": 0.011934,
          "end_time": "2022-04-09T14:37:37.376064",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.364130",
          "status": "completed"
        },
        "tags": [],
        "id": "44446fb8"
      },
      "source": [
        "## The Transformer architecture\n",
        "\n",
        "In this notebook, we will implement the Transformer architecture by hand.\n",
        "As the architecture is so popular, there already exists a Pytorch module `nn.Transformer`\n",
        "([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html))\n",
        "and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
        "on how to use it for next token prediction.\n",
        "However, we will implement it here ourselves, to get through to the smallest details.\n",
        "\n",
        "There are of course many more tutorials out there about attention and Transformers.\n",
        "Below, we list a few that are worth exploring if you are interested in the topic\n",
        "and might want yet another perspective on the topic after this one:\n",
        "\n",
        "* [Transformer: A Novel Neural Network Architecture for Language Understanding\n",
        "(Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
        "* [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations.\n",
        "The focus is on NLP.\n",
        "* [Attention?\n",
        "Attention!\n",
        "(Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
        "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention.\n",
        "Recommended going through if the explanation below is too abstract for you.\n",
        "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899fbed7",
      "metadata": {
        "papermill": {
          "duration": 0.011614,
          "end_time": "2022-04-09T14:37:37.399352",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.387738",
          "status": "completed"
        },
        "tags": [],
        "id": "899fbed7"
      },
      "source": [
        "### What is Attention?\n",
        "\n",
        "<!-- The attention mechanism describes a recent new group of layers in neural networks that has attracted\n",
        "a lot of interest in the past few years, especially in sequence tasks. -->\n",
        "<!-- There are a lot of different possible definitions of \"attention\" in the literature,\n",
        "but the one we will use here is the following:  -->\n",
        "<!-- The goal is to take an average over the features of multiple elements.\n",
        "However, instead of weighting each element equally, we want to weight them depending on their actual values. -->\n",
        "<!-- In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. -->\n",
        "\n",
        "The attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on input queries and elements' keys.\n",
        "This average has to represent:\n",
        "* The numerical representation of the elements of the input sequence (**values**).\n",
        "* The correlations among the elements (for self-attention) or with the output elements (for language models)(**queries**).\n",
        "* The relevant features of each input elements with respect to the task at hand (**keys**).\n",
        "\n",
        "Also, we need to specify a score function $f_{attn}$ that takes the query and a key as input, and output the score/attention weight of the query-key pair.\n",
        "The weights of the average are calculated by a softmax over all score function outputs.\n",
        "Hence, \n",
        "\n",
        "> We assign higher weight to those value vectors whose corresponding key is most similar to the query.\n",
        "\n",
        "\n",
        "If we try to describe it with pseudo-math, we can write:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "Visually, we can show the attention over a sequence of words as follows:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/PyTorchLightning/lightning-tutorials/raw/main/course_UvA-DL/05-transformers-and-MH-attention/attention_example.svg\" width=\"750px\"></center>\n",
        "\n",
        "For every word, we have one key and one value vector.\n",
        "The query is compared to all keys with a score function to determine the weights. <!-- The softmax is not visualized for simplicity. -->\n",
        "Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used.\n",
        "* The attention applied within the Encoder and the Decoder of Transformers is called **self-attention**.\n",
        "In self-attention, each sequence element provides a key, value, and query.\n",
        "<!-- For each element, we check the similarity of the all sequence elements' keys, and return a different, averaged value vector for each element. -->\n",
        "* In Language models (therefore also in Transformers) between the Encoder and the decoder we have the standard attention or encoder-decoder attention. In the latter we use as queries the output of the model, i.e. the decoded or generated output sequence, . \n",
        "\n",
        "We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which in the Transformer is the scaled dot product attention."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf076f5",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.011622,
          "end_time": "2022-04-09T14:37:37.422711",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.411089",
          "status": "completed"
        },
        "tags": [],
        "id": "baf076f5"
      },
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The core concept behind self-attention is the scaled dot product attention.\n",
        "Our goal is to have an attention mechanism comparing each element in a sequence with any other (in an efficient way).\n",
        "The dot product attention takes as input a set of queries\n",
        "$Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$\n",
        "and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length,\n",
        "and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively.\n",
        "For simplicity, we neglect the batch dimension for now.\n",
        "The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$,\n",
        "using the dot product as the similarity metric.\n",
        "In math, we calculate the dot product attention as follows:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys,\n",
        "resulting in a matrix of the shape $T\\times T$.\n",
        "Each row represents the attention logits for a specific element $i$ to all other elements in the sequence.\n",
        "On these, we apply a softmax and multiply with the value vector to obtain a weighted mean\n",
        "(the weights being determined by the attention).\n",
        "The computation graph is visualized below:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/PyTorchLightning/lightning-tutorials/raw/main/course_UvA-DL/05-transformers-and-MH-attention/scaled_dot_product_attn.svg\" width=\"210px\"></center>\n",
        "\n",
        "The scaling factor of $1/\\sqrt{d_k}$ is crucial to maintain an appropriate variance of attention values after initialization.\n",
        "Remember that we intialize our layers (therefore also $Q$ and $K$) to have a variance close to $1$.\n",
        "However, performing a dot product over two vectors with a variance $\\sigma$ results\n",
        "in a scalar having $d_k$-times higher variance:\n",
        "\n",
        "<!-- $$q_i \\sim \\mathcal{N}(0,\\sigma), k_i \\sim \\mathcal{N}(0,\\sigma) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma\\cdot d_k$$\n",
        " -->\n",
        "\n",
        "If we do not scale down the variance back to $\\sigma$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others.\n",
        "The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately (*vanishing gradient*).\n",
        "\n",
        "The masking block `Mask(opt.)` is used to stack **multiple sequences with different lengths into a batch**.\n",
        "To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values.\n",
        "This is usually done by setting the respective attention logits to a very low (negative) values, e.g. $-10^{14}$.\n",
        "\n",
        "Let's now write a function computing the output features given the triple of queries, keys, and values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9070779d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.447403Z",
          "iopub.status.busy": "2022-04-09T14:37:37.446989Z",
          "iopub.status.idle": "2022-04-09T14:37:37.451657Z",
          "shell.execute_reply": "2022-04-09T14:37:37.451096Z"
        },
        "papermill": {
          "duration": 0.018657,
          "end_time": "2022-04-09T14:37:37.453028",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.434371",
          "status": "completed"
        },
        "tags": [],
        "id": "9070779d"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    # FILL IT YOURSELF!\n",
        "    \n",
        "    # Compute attn_logits\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits /= math.sqrt(d_k)\n",
        "\n",
        "    # Apply mask if not None\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, - 1e14)\n",
        "\n",
        "    # Pass through softmax\n",
        "    attention = F.softmax(attn_logits)\n",
        "\n",
        "    # Weight values accordingly\n",
        "    output_values = torch.matmul(attention, v)\n",
        "\n",
        "    return output_values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a475896",
      "metadata": {
        "papermill": {
          "duration": 0.011788,
          "end_time": "2022-04-09T14:37:37.476729",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.464941",
          "status": "completed"
        },
        "tags": [],
        "id": "4a475896"
      },
      "source": [
        "Note that our code above supports any additional dimensionality in front of the sequence length\n",
        "so that we can also use it for batches.\n",
        "However, for a better understanding, let's generate a few random queries, keys, and value vectors,\n",
        "and calculate the attention outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b4422f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.501387Z",
          "iopub.status.busy": "2022-04-09T14:37:37.501015Z",
          "iopub.status.idle": "2022-04-09T14:37:37.510585Z",
          "shell.execute_reply": "2022-04-09T14:37:37.510000Z"
        },
        "papermill": {
          "duration": 0.023453,
          "end_time": "2022-04-09T14:37:37.511961",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.488508",
          "status": "completed"
        },
        "tags": [],
        "id": "f6b4422f",
        "outputId": "ffca60da-96b5-49f4-8fe1-1209fc91b29a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            " tensor([[-0.4846,  0.4063],\n",
            "        [ 0.2174, -1.0264],\n",
            "        [-0.0766, -0.8279]])\n",
            "Attention\n",
            " tensor([[0.0300, 0.6852, 0.2847],\n",
            "        [0.8302, 0.0678, 0.1019],\n",
            "        [0.7071, 0.0857, 0.2072]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# seq_len, d_k = 3, 2\n",
        "# q = torch.randn(seq_len, d_k)\n",
        "# k = torch.randn(seq_len, d_k)\n",
        "# v = torch.randn(seq_len, d_k)\n",
        "q = torch.tensor([[-0.8920, -1.5091],\n",
        "        [ 0.3704,  1.4565],\n",
        "        [ 0.9398,  0.7748]])\n",
        "k = torch.tensor([[ 0.1919,  1.2638],\n",
        "        [-1.2904, -0.7911],\n",
        "        [-0.0209, -0.7185]])\n",
        "v = torch.tensor([[ 0.5186, -1.3125],\n",
        "        [ 0.1920,  0.5428],\n",
        "        [-2.2188,  0.2590]])\n",
        "\n",
        "output_values, attention = scaled_dot_product(q, k, v)\n",
        "\n",
        "print(\"Output\\n\", output_values)\n",
        "print(\"Attention\\n\", attention)\n",
        "\n",
        "assert (output_values - torch.tensor([[-0.4846,  0.4063],\n",
        "                                      [ 0.2174, -1.0264],\n",
        "                                      [-0.0766, -0.8279]]) < 1e-4).all(), \\\n",
        "    f\"Error in computing the attention\"\n",
        "assert (attention - torch.tensor([[0.0300, 0.6852, 0.2847],\n",
        "                                  [0.8302, 0.0678, 0.1019],\n",
        "                                  [0.7071, 0.0857, 0.2072]]) < 1e-4).all(), \\\n",
        "    f\"Error in computing the attention\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d4dd42f",
      "metadata": {
        "papermill": {
          "duration": 0.011801,
          "end_time": "2022-04-09T14:37:37.535654",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.523853",
          "status": "completed"
        },
        "tags": [],
        "id": "8d4dd42f"
      },
      "source": [
        "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand.\n",
        "It is important to fully understand how the scaled dot product attention is calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738d26e7",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.013673,
          "end_time": "2022-04-09T14:37:37.561299",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.547626",
          "status": "completed"
        },
        "tags": [],
        "id": "738d26e7"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence.\n",
        "However, often there are multiple different aspects a sequence element wants to attend to,\n",
        "and a single weighted average is not a good option for it.\n",
        "This is why we extend the attention mechanisms to **multiple heads**,\n",
        "i.e. multiple different query-key-value triplets on the same features.\n",
        "Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys,\n",
        "and sub-values, which we pass through the scaled dot product attention independently.\n",
        "Afterward, we concatenate the heads and combine them with a final weight matrix.\n",
        "Mathematically,\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We refer to this as Multi-Head Attention layer with the learnable parameters\n",
        "$W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$,\n",
        "$W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$,\n",
        "$W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$,\n",
        "and $W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}$ ($D$ being the input dimensionality).\n",
        "Expressed in a computational graph:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/PyTorchLightning/lightning-tutorials/raw/main/course_UvA-DL/05-transformers-and-MH-attention/multihead_attention.svg\" width=\"230px\"></center>\n",
        "\n",
        "How are we applying a Multi-Head Attention layer in a neural network,\n",
        "where we don't have an arbitrary query, key, and value vector as input?\n",
        "Looking at the computation graph above, a simple but effective implementation is to set the current\n",
        "feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$\n",
        "($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$).\n",
        "The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding\n",
        "feature vectors that represent the queries, keys, and values of the input.\n",
        "Using this approach, we can implement the Multi-Head Attention module below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa28f1cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.586727Z",
          "iopub.status.busy": "2022-04-09T14:37:37.586295Z",
          "iopub.status.idle": "2022-04-09T14:37:37.594515Z",
          "shell.execute_reply": "2022-04-09T14:37:37.593932Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.022557,
          "end_time": "2022-04-09T14:37:37.595903",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.573346",
          "status": "completed"
        },
        "tags": [],
        "id": "fa28f1cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7470b5a-0cae-4edd-f2f4-fa5bbefae165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x: tensor([[[0.3360, 0.6676, 0.6393],\n",
            "         [0.2083, 0.5484, 0.1204],\n",
            "         [0.3533, 0.3038, 0.9383],\n",
            "         [0.0499, 0.2048, 0.0107]]])\n",
            "MhA Output tensor([[[ 0.3468,  1.0003, -0.4183, -0.2675],\n",
            "         [ 0.3436,  0.9924, -0.4152, -0.2676],\n",
            "         [-0.1715,  0.9991, -0.1244,  0.1214],\n",
            "         [-0.1695,  1.0088, -0.1291,  0.1232]]], grad_fn=<ViewBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim # dimension of concatenated heads\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # FILL IT YOURSELF!\n",
        "\n",
        "        # Create linear layers for both qkv and output\n",
        "        # TIP: Stack all weight matrices 1...h together for efficiency\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.qkv_proj = nn.Linear(input_dim, embed_dim * 3)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        # FILL IT YOURSELF!\n",
        "        batch_dim, seq_length, input_dim = x.shape\n",
        "        \n",
        "        # Compute linear projection for qkv and separate heads \n",
        "        # QKV: [Batch, Head, SeqLen, Dims]\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.reshape(batch_dim, seq_length, self.num_heads, 3* self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "    \n",
        "        # Apply Dot Product Attention to qkv ()\n",
        "        attention_values, attention = scaled_dot_product(q, k, v)\n",
        "    \n",
        "        # Concatenate heads to [Batch, SeqLen, Embed Dim]\n",
        "        attention_values = attention_values.reshape(batch_dim, seq_length, self.embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        o = self.o_proj(attention_values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n",
        "\n",
        "input_d = 3\n",
        "seq_l = 4\n",
        "embed_d = 4\n",
        "n_heads = 2\n",
        "b_size = 1\n",
        "\n",
        "mh_att = MultiheadAttention(input_d, embed_d, n_heads)\n",
        "\n",
        "x = torch.rand(b_size, seq_l, input_d)\n",
        "x = torch.tensor([[[0.3360, 0.6676, 0.6393],\n",
        "         [0.2083, 0.5484, 0.1204],\n",
        "         [0.3533, 0.3038, 0.9383],\n",
        "         [0.0499, 0.2048, 0.0107]]])\n",
        "print(f\"Input x: {x}\")\n",
        "\n",
        "att_output = mh_att(x)\n",
        "print(f\"MhA Output {att_output}\")\n",
        "assert att_output.shape == torch.Size([1, 4, 4]), \"Error in computing multi-head attention\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83e0f14",
      "metadata": {
        "papermill": {
          "duration": 0.011887,
          "end_time": "2022-04-09T14:37:37.619972",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.608085",
          "status": "completed"
        },
        "tags": [],
        "id": "c83e0f14"
      },
      "source": [
        "#### Attention is permutation equivariant\n",
        "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs.\n",
        "This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$\n",
        "(neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched.\n",
        "Hence, **the multi-head attention is looking at the input not as a sequence, but as a set of elements**.\n",
        "This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable!\n",
        "But what if the order of the input is actually important for solving the task, like language modeling?\n",
        "The answer is to encode the position in the input features, which we will take a closer look at later\n",
        "(topic _Positional encodings_ below).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2468bc",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.012032,
          "end_time": "2022-04-09T14:37:37.645572",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.633540",
          "status": "completed"
        },
        "tags": [],
        "id": "0c2468bc"
      },
      "source": [
        "### Transformer Encoder\n",
        "\n",
        "Next, we will look at how to apply the multi-head attention inside the Transformer architecture.\n",
        "Originally, the Transformer model was designed for machine translation.\n",
        "Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation.\n",
        "The decoder, instead, attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN.\n",
        "\n",
        "While this structure is extremely useful for Sequence-to-Sequence tasks, it is not always necessary and.\n",
        "Many advances in NLP have been made using pure encoder-based Transformer models (e.g. [BERT](https://arxiv.org/abs/1810.04805)-family, \n",
        "the [Vision Transformer](https://arxiv.org/abs/2010.11929), and more). Therefore, we will focus here only on the encoder part.\n",
        "If you have understood the encoder architecture, the decoder is a very small step to implement as well.\n",
        "The full Transformer architecture looks as follows:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/PyTorchLightning/lightning-tutorials/raw/main/course_UvA-DL/05-transformers-and-MH-attention/transformer_architecture.svg\" width=\"400px\"></center>\n",
        "\n",
        "The encoder consists of $N$ identical blocks that are applied in sequence.\n",
        "Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above.\n",
        "The output is added to the original input using a residual connection,\n",
        "and we apply a consecutive Layer Normalization on the sum.\n",
        "Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$\n",
        "($x$ being $Q$, $K$ and $V$ input to the attention layer).\n",
        "\n",
        "The **residual connection** is crucial in the Transformer architecture to:\n",
        "\n",
        "1. Avoid vanishing gradients, as in ResNet, but valid for all deep architectures (Some models contain more than 24 blocks in the encoder)\n",
        "2. Retain the information about the original sequence (remember that Self-Attention do not necessairly consider the input as a sequence)\n",
        "\n",
        "<!-- Remember that the Multi-Head Attention layer ignores the position of elements in a sequence,\n",
        "and can only learn it based on the input features.\n",
        "Removing the residual connections would mean that this information is lost after the first attention layer\n",
        "(after initialization), and with a randomly initialized query and key vector,\n",
        "the output vectors for position $i$ has no relation to its original input.\n",
        "All outputs of the attention are likely to represent similar/same information,\n",
        "and there is no chance for the model to distinguish which information came from which input element.\n",
        "An alternative option to residual connection would be to fix at least one head to focus on its original input,\n",
        "but this is very inefficient and does not have the benefit of the improved gradient flow. -->\n",
        "\n",
        "The **Layer Normalization** also plays an important role in the Transformer architecture as it enables faster training and provides small regularization.\n",
        "Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence.\n",
        "We are not using Batch Normalization because it depends on the batch size which is often small with Transformers.\n",
        "<!-- (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language\n",
        "as the features of words tend to have a much higher variance (there are many, very rare words\n",
        "which need to be considered for a good distribution estimate). -->\n",
        "\n",
        "Additionally to the Multi-Head Attention, a small fully connected **Feed-Forward Network (FFN)** is added to the model,\n",
        "which is applied to each position separately and identically.\n",
        "Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP.\n",
        "The full transformation including the residual connection can be expressed as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
        "    \\text{output} & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "This MLP adds extra complexity to the model and allows transformations on each sequence element separately.\n",
        "You can imagine as this allows the model to \"post-process\" the new information added\n",
        "by the previous Multi-Head Attention, and prepare it for the next attention block.\n",
        "Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$,\n",
        "i.e. the dimensionality of the original input $x$.\n",
        "The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
        "\n",
        "Finally, after looking at all parts of the encoder architecture, we can start implementing it below.\n",
        "We first start by implementing a single encoder block.\n",
        "Additionally to the layers described above, we will add **Dropout layers** in the MLP and on the output of the MLP and Multi-Head Attention for regularization.\n",
        "\n",
        "Also, we will assume now on a constant `input_dim = embed_dim` throughout the Transformer and therefore we will instantiate the attention as `MultiheadAttention(input_dim, input_dim, num_heads)`. The dimensionality of the first input will be addressed later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55bfef0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.670726Z",
          "iopub.status.busy": "2022-04-09T14:37:37.670245Z",
          "iopub.status.idle": "2022-04-09T14:37:37.676639Z",
          "shell.execute_reply": "2022-04-09T14:37:37.676067Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.020505,
          "end_time": "2022-04-09T14:37:37.678049",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.657544",
          "status": "completed"
        },
        "tags": [],
        "id": "c55bfef0"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Dimensionality of the input\n",
        "            num_heads: Number of heads to use in the attention block\n",
        "            dim_feedforward: Dimensionality of the hidden layer in the MLP\n",
        "            dropout: Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # FILL IT YOURSELF!\n",
        "\n",
        "        # Create Attention layer\n",
        "\n",
        "        # Create Two-layer MLP with droput\n",
        "        \n",
        "        # Layers to apply in between the main layers (Layer Norm and Dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Compute Attention part\n",
        "\n",
        "        # Compute MLP part\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fea27f4",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.011884,
          "end_time": "2022-04-09T14:37:37.702098",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.690214",
          "status": "completed"
        },
        "tags": [],
        "id": "0fea27f4"
      },
      "source": [
        "Based on this block, we can implement a module for the full **Transformer encoder**.\n",
        "Additionally to a `forward` function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`.\n",
        "The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder.\n",
        "They helps us in understanding, and partially, explaining the model.\n",
        "Attention scores may not necessarily reflect the true interpretation of the model (it is disputed in literature, check [Attention is not Explanation](https://arxiv.org/abs/1902.10186)\n",
        "and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e4943c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.727879Z",
          "iopub.status.busy": "2022-04-09T14:37:37.726612Z",
          "iopub.status.idle": "2022-04-09T14:37:37.733398Z",
          "shell.execute_reply": "2022-04-09T14:37:37.732829Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.020849,
          "end_time": "2022-04-09T14:37:37.734822",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.713973",
          "status": "completed"
        },
        "tags": [],
        "id": "59e4943c"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for layer in self.layers:\n",
        "            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = layer(x)\n",
        "        return attention_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d9e15f",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.011928,
          "end_time": "2022-04-09T14:37:37.758957",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.747029",
          "status": "completed"
        },
        "tags": [],
        "id": "a5d9e15f"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "We have discussed before that the Multi-Head Attention block is permutation-equivariant.\n",
        "In tasks like language understanding, however, the position is important for interpreting the input words.\n",
        "The position information is therefore added in the input features by means of feature patterns that the network can identify and potentially generalize to larger sequences.\n",
        "The specific pattern chosen by Vaswani et al.\n",
        "are sine and cosine functions of different frequencies, as follows:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$.\n",
        "These values, concatenated for all hidden dimensions, are added to the original input features\n",
        "(in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information.\n",
        "We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$)\n",
        "hidden dimensionalities where we apply a sine/cosine respectively.\n",
        "The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function\n",
        "of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions.\n",
        "The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
        "\n",
        "The positional encoding is implemented below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97458a86",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.784031Z",
          "iopub.status.busy": "2022-04-09T14:37:37.783642Z",
          "iopub.status.idle": "2022-04-09T14:37:37.789832Z",
          "shell.execute_reply": "2022-04-09T14:37:37.789242Z"
        },
        "papermill": {
          "duration": 0.02026,
          "end_time": "2022-04-09T14:37:37.791228",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.770968",
          "status": "completed"
        },
        "tags": [],
        "id": "97458a86"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Args\n",
        "            d_model: Hidden dimensionality of the input.\n",
        "            max_len: Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer(\"pe\", pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61da655f",
      "metadata": {
        "papermill": {
          "duration": 0.012048,
          "end_time": "2022-04-09T14:37:37.815452",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.803404",
          "status": "completed"
        },
        "tags": [],
        "id": "61da655f"
      },
      "source": [
        "To understand the positional encoding, we can visualize it below.\n",
        "We will generate an image of the positional encoding over hidden dimensionality and position in a sequence.\n",
        "Each pixel, therefore, represents the change of the input feature we perform to encode the specific position.\n",
        "Let's do it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e0dece",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.840839Z",
          "iopub.status.busy": "2022-04-09T14:37:37.840293Z",
          "iopub.status.idle": "2022-04-09T14:37:38.380487Z",
          "shell.execute_reply": "2022-04-09T14:37:38.379879Z"
        },
        "papermill": {
          "duration": 0.554884,
          "end_time": "2022-04-09T14:37:38.382455",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.827571",
          "status": "completed"
        },
        "tags": [],
        "id": "35e0dece",
        "outputId": "fff1231f-536c-479e-d75f-bdc0fcb0e7d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1, pe.shape[1] + 1, pe.shape[0] + 1, 1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1] + [i * 10 for i in range(1, 1 + pe.shape[1] // 10)])\n",
        "ax.set_yticks([1] + [i * 10 for i in range(1, 1 + pe.shape[0] // 10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbe84c2",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.016824,
          "end_time": "2022-04-09T14:37:40.272544",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.255720",
          "status": "completed"
        },
        "tags": [],
        "id": "9dbe84c2"
      },
      "source": [
        "### Transformer Encoder Classifier\n",
        "\n",
        "Finally, we can implement a template for a classifier based on the Transformer encoder. \n",
        "\n",
        "Additionally to the Transformer architecture, we add:\n",
        "* a small input network (maps input dimensions to model dimensions)\n",
        "* the positional encoding \n",
        "* an output network (transforming output encodings to predictions). \n",
        "\n",
        "Notice that the output network will take in input a 3D tensor `<batch_samples, seq_len, model_dim>` and produces in output another 2D tensor `<batch_samples, seq_len>` where each output value represents the prediction of the corresponding reversed number.\n",
        "\n",
        "If we would need a classifier over the whole sequence, instead, the common approach is to add an additional `[CLS]` token to the sequence, representing the classifier token and then get the prediction only from that output token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba3970c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.307736Z",
          "iopub.status.busy": "2022-04-09T14:37:40.307446Z",
          "iopub.status.idle": "2022-04-09T14:37:40.319189Z",
          "shell.execute_reply": "2022-04-09T14:37:40.318619Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.031287,
          "end_time": "2022-04-09T14:37:40.320713",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.289426",
          "status": "completed"
        },
        "tags": [],
        "id": "fba3970c"
      },
      "outputs": [],
      "source": [
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        model_dim,\n",
        "        num_classes,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        dropout=0.0,\n",
        "        input_dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Hidden dimensionality of the input\n",
        "            model_dim: Hidden dimensionality to use inside the Transformer\n",
        "            num_classes: Number of classes to predict per sequence element\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers: Number of encoder blocks to use.\n",
        "            lr: Learning rate in the optimizer\n",
        "            warmup: Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters: Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            dropout: Dropout to apply inside the model\n",
        "            input_dropout: Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.model_dim = model_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.input_dropout = input_dropout\n",
        "\n",
        "        # FILL IT YOURSELF!\n",
        "\n",
        "        # Create a Generic Input Encoder Input dim -> Model dim with input dropout\n",
        "\n",
        "        # Create positional encoding for sequences\n",
        "        \n",
        "        # Create transformer Encoder\n",
        "\n",
        "        # Create output classifier per sequence element Model_dim -> num_classes\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask: Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f853d7",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017029,
          "end_time": "2022-04-09T14:37:40.355127",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.338098",
          "status": "completed"
        },
        "tags": [],
        "id": "e2f853d7"
      },
      "source": [
        "## Experiment: Sequence to Sequence\n",
        "\n",
        "After having finished the implementation of the Transformer architecture, we can start experimenting.\n",
        "\n",
        "A Seq-2-Seq task represents a task where the input _and_ the output is a sequence,\n",
        "not necessarily of the same length.\n",
        "Popular tasks in this domain include machine translation and summarization.\n",
        "For this, we usually have a Transformer encoder for interpreting the input sequence,\n",
        "and a decoder for generating the output in an autoregressive manner.\n",
        "Here, however, we will go back to a much simpler example task and use only the encoder, since the output length is fixed.\n",
        "Given a sequence of $N$ numbers between $0$ and $M$, the task is to reverse the input sequence.\n",
        "In Numpy notation, if our input is $x$, the output should be $x$[::-1].\n",
        "Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies.\n",
        "Transformers are built to support such, and hence, we expect it to perform very well.\n",
        "\n",
        "First, let's create a dataset class below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678b995e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.390306Z",
          "iopub.status.busy": "2022-04-09T14:37:40.389769Z",
          "iopub.status.idle": "2022-04-09T14:37:40.394655Z",
          "shell.execute_reply": "2022-04-09T14:37:40.394092Z"
        },
        "papermill": {
          "duration": 0.023865,
          "end_time": "2022-04-09T14:37:40.396024",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.372159",
          "status": "completed"
        },
        "tags": [],
        "id": "678b995e"
      },
      "outputs": [],
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        return inp_data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c57b9a",
      "metadata": {
        "papermill": {
          "duration": 0.017261,
          "end_time": "2022-04-09T14:37:40.430322",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.413061",
          "status": "completed"
        },
        "tags": [],
        "id": "d5c57b9a"
      },
      "source": [
        "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`.\n",
        "The label is simply the tensor flipped over the sequence dimension.\n",
        "We can create the corresponding data loaders below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d155c78",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.465826Z",
          "iopub.status.busy": "2022-04-09T14:37:40.465300Z",
          "iopub.status.idle": "2022-04-09T14:37:40.483277Z",
          "shell.execute_reply": "2022-04-09T14:37:40.482703Z"
        },
        "papermill": {
          "duration": 0.037365,
          "end_time": "2022-04-09T14:37:40.484763",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.447398",
          "status": "completed"
        },
        "tags": [],
        "id": "7d155c78"
      },
      "outputs": [],
      "source": [
        "dataset = partial(ReverseDataset, 10, 16)\n",
        "train_dl = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_dl = data.DataLoader(dataset(1000), batch_size=128)\n",
        "test_dl = data.DataLoader(dataset(10000), batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108b3cc8",
      "metadata": {
        "papermill": {
          "duration": 0.017281,
          "end_time": "2022-04-09T14:37:40.519172",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.501891",
          "status": "completed"
        },
        "tags": [],
        "id": "108b3cc8"
      },
      "source": [
        "Let's look at an arbitrary sample of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54622080",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.554479Z",
          "iopub.status.busy": "2022-04-09T14:37:40.553970Z",
          "iopub.status.idle": "2022-04-09T14:37:40.558389Z",
          "shell.execute_reply": "2022-04-09T14:37:40.557790Z"
        },
        "papermill": {
          "duration": 0.023559,
          "end_time": "2022-04-09T14:37:40.559783",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.536224",
          "status": "completed"
        },
        "tags": [],
        "id": "54622080",
        "outputId": "8e32cea9-1942-4733-85af-2104a656e09e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data: tensor([7, 8, 4, 1, 0, 4, 0, 4, 9, 7, 0, 0, 1, 4, 9, 9])\n",
            "Labels:     tensor([9, 9, 4, 1, 0, 0, 7, 9, 4, 0, 4, 0, 1, 4, 8, 7])\n"
          ]
        }
      ],
      "source": [
        "inp_data, labels = train_dl.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73604b92",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017109,
          "end_time": "2022-04-09T14:37:40.594221",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.577112",
          "status": "completed"
        },
        "tags": [],
        "id": "73604b92"
      },
      "source": [
        "During training, we pass the input sequence through the Transformer encoder and predict the output for each input token.\n",
        "We use the standard Cross-Entropy loss to perform this.\n",
        "Every number is represented as a one-hot vector.\n",
        "Remember that representing the categories as single scalars decreases the expressiveness of the model extremely\n",
        "as $0$ and $1$ are not closer related than $0$ and $9$ in our example.\n",
        "An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module `nn.Embedding`.\n",
        "However, using a one-hot vector with an additional linear layer as in our case has the same effect\n",
        "as an embedding layer (`self.input_net` maps one-hot vector to a dense vector,\n",
        "where each row of the weight matrix represents the embedding for a specific category).\n",
        "\n",
        "To implement the training dynamic, we create a new class inheriting from `TransformerPredictor`\n",
        "and overwriting the training, validation and test step functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3157b207",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.629517Z",
          "iopub.status.busy": "2022-04-09T14:37:40.629034Z",
          "iopub.status.idle": "2022-04-09T14:37:40.635426Z",
          "shell.execute_reply": "2022-04-09T14:37:40.634853Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.025464,
          "end_time": "2022-04-09T14:37:40.636801",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.611337",
          "status": "completed"
        },
        "tags": [],
        "id": "3157b207"
      },
      "outputs": [],
      "source": [
        "def train_step(model, x, y, optim):\n",
        "    model.train()\n",
        "\n",
        "    # Fetch data and transform categories to one-hot vectors\n",
        "    inp_data = F.one_hot(x, num_classes=model.num_classes).float()\n",
        "\n",
        "    # Perform prediction and calculate loss and accuracy\n",
        "    preds = model(inp_data, add_positional_encoding=True)\n",
        "    loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y.view(-1))\n",
        "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "    # Backpropagate and update weights\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def eval_step(model, x, y):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data = F.one_hot(x, num_classes=model.num_classes).float()\n",
        "\n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds = model(inp_data, add_positional_encoding=True)\n",
        "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y.view(-1))\n",
        "        acc = (preds.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "    return loss, acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eca170b",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017272,
          "end_time": "2022-04-09T14:37:40.673633",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.656361",
          "status": "completed"
        },
        "tags": [],
        "id": "1eca170b"
      },
      "source": [
        "Finally, we can create a training function similar to the one we have seen in previous laboratories. We running for $N$ epochs printing the training and validation loss and saving our best model based on the validation.\n",
        "Afterward, we test our models on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237c583b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.709513Z",
          "iopub.status.busy": "2022-04-09T14:37:40.708933Z",
          "iopub.status.idle": "2022-04-09T14:37:40.715633Z",
          "shell.execute_reply": "2022-04-09T14:37:40.715064Z"
        },
        "papermill": {
          "duration": 0.026011,
          "end_time": "2022-04-09T14:37:40.717000",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.690989",
          "status": "completed"
        },
        "tags": [],
        "id": "237c583b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, test_loader, \n",
        "                optim, epochs=5):\n",
        "    best_acc = 0.\n",
        "    pbar = tqdm(range(epochs))\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_acc = 0., 0.\n",
        "        for x, y in train_loader:\n",
        "            loss, acc = train_step(model, x, y, optim)\n",
        "            train_loss += loss\n",
        "            train_acc += acc\n",
        "\n",
        "        val_loss, val_acc = 0., 0.\n",
        "        for x, y in val_loader:\n",
        "            loss, acc = eval_step(model, x, y)\n",
        "            val_loss += loss\n",
        "            val_acc += acc\n",
        "\n",
        "        if val_acc/len(val_loader) > best_acc:\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            best_acc = val_acc/len(val_loader)\n",
        "\n",
        "        pbar.update()\n",
        "        pbar.set_description(f\"Train Acc: {train_acc/len(train_loader)* 100:.2f} \"\n",
        "                            f\"Train Loss: {train_loss/len(train_loader):.2f} \"\n",
        "                            f\"Val Acc: {val_acc/len(val_loader)* 100 :.2f}  \"\n",
        "                            f\"Val loss: {val_loss/len(val_loader):.2f} \")\n",
        "\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for x, y in test_loader:\n",
        "        loss, acc = eval_step(model, x, y)\n",
        "        test_loss += loss\n",
        "        test_acc += acc\n",
        "    \n",
        "    print(f\"Test accuracy: {test_acc/len(test_loader)*100 :.2f}\")\n",
        "\n",
        "    pbar.close()\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170c6143",
      "metadata": {
        "papermill": {
          "duration": 0.017478,
          "end_time": "2022-04-09T14:37:40.752397",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.734919",
          "status": "completed"
        },
        "tags": [],
        "id": "170c6143"
      },
      "source": [
        "Finally, we can train the model.\n",
        "In this setup, we will use a single encoder block and a single head in the Multi-Head Attention.\n",
        "This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted\n",
        "as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3e1d49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.788473Z",
          "iopub.status.busy": "2022-04-09T14:37:40.787980Z",
          "iopub.status.idle": "2022-04-09T14:37:44.274530Z",
          "shell.execute_reply": "2022-04-09T14:37:44.273926Z"
        },
        "papermill": {
          "duration": 3.505945,
          "end_time": "2022-04-09T14:37:44.275998",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.770053",
          "status": "completed"
        },
        "tags": [],
        "id": "da3e1d49",
        "outputId": "e59be824-6a08-4173-942f-148ee4ed891f",
        "colab": {
          "referenced_widgets": [
            "cb881d87e35e4b81b15b69c4a2ac130f",
            "934a698f7f554ccabddb193766abd4d3",
            "36942544884940edb54bd4fa13a5782b",
            "d7571490226342489cb5d54135e10d05",
            "83a6d3c37c454e3faf006be51cab9ed5",
            "cc83b9ad878546dc9ebc9a425bf2420e",
            "5b20f87f3b754a52aace79d6449a2e2d",
            "33883866def3473798ca5152aac28a75",
            "4e56fbea5ada48a8bf81f0a12415bc8b",
            "b5b65cdf33674c27ac1e43a7af3f6373",
            "dbc4e786e1c64b62828dca2aa025b045"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb881d87e35e4b81b15b69c4a2ac130f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 100.00\n"
          ]
        }
      ],
      "source": [
        "reverse_model = TransformerPredictor(\n",
        "    input_dim=train_dl.dataset.num_categories,\n",
        "    model_dim=32,\n",
        "    num_heads=1,\n",
        "    num_classes=train_dl.dataset.num_categories,\n",
        "    num_layers=1,\n",
        "    dropout=0.0,\n",
        ")\n",
        "optimizer = optim.AdamW(reverse_model.parameters(), lr=0.001)\n",
        "\n",
        "reverse_model = train_model(reverse_model, train_dl, val_dl, test_dl, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbab7649",
      "metadata": {
        "papermill": {
          "duration": 0.017964,
          "end_time": "2022-04-09T14:37:44.392392",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.374428",
          "status": "completed"
        },
        "tags": [],
        "id": "bbab7649"
      },
      "source": [
        "As we would have expected, the Transformer can correctly solve the task.\n",
        "However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input?\n",
        "Let's try to visualize it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d2704d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:44.429196Z",
          "iopub.status.busy": "2022-04-09T14:37:44.428820Z",
          "iopub.status.idle": "2022-04-09T14:37:44.436248Z",
          "shell.execute_reply": "2022-04-09T14:37:44.435661Z"
        },
        "papermill": {
          "duration": 0.027384,
          "end_time": "2022-04-09T14:37:44.437643",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.410259",
          "status": "completed"
        },
        "tags": [],
        "id": "c8d2704d"
      },
      "outputs": [],
      "source": [
        "data_input, labels = next(iter(val_dl))\n",
        "inp_data = F.one_hot(data_input, num_classes=reverse_model.num_classes).float()\n",
        "inp_data = inp_data.to(device)\n",
        "attention_maps = reverse_model.get_attention_maps(inp_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f1e662",
      "metadata": {
        "papermill": {
          "duration": 0.017949,
          "end_time": "2022-04-09T14:37:44.473368",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.455419",
          "status": "completed"
        },
        "tags": [],
        "id": "a0f1e662"
      },
      "source": [
        "The object `attention_maps` is a list of length $N$ where $N$ is the number of layers.\n",
        "Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59364e79",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:44.510010Z",
          "iopub.status.busy": "2022-04-09T14:37:44.509466Z",
          "iopub.status.idle": "2022-04-09T14:37:44.514032Z",
          "shell.execute_reply": "2022-04-09T14:37:44.513456Z"
        },
        "papermill": {
          "duration": 0.024284,
          "end_time": "2022-04-09T14:37:44.515442",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.491158",
          "status": "completed"
        },
        "tags": [],
        "id": "59364e79",
        "outputId": "fcbffe72-d64b-460a-93de-9f0b513ce64e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "attention_maps[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9ce1e83",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017929,
          "end_time": "2022-04-09T14:37:44.551386",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.533457",
          "status": "completed"
        },
        "tags": [],
        "id": "d9ce1e83"
      },
      "source": [
        "Next, we will write a plotting function that takes as input the sequences, attention maps, and an index\n",
        "indicating for which batch element we want to visualize the attention map.\n",
        "We will create a plot where over rows, we have different layers, while over columns, we show the different heads.\n",
        "Remember that the softmax has been applied for each row separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c1181d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:44.588212Z",
          "iopub.status.busy": "2022-04-09T14:37:44.587738Z",
          "iopub.status.idle": "2022-04-09T14:37:44.595498Z",
          "shell.execute_reply": "2022-04-09T14:37:44.594933Z"
        },
        "papermill": {
          "duration": 0.027654,
          "end_time": "2022-04-09T14:37:44.596896",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.569242",
          "status": "completed"
        },
        "tags": [],
        "id": "40c1181d"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
        "    if input_data is not None:\n",
        "        input_data = input_data[idx].detach().cpu().numpy()\n",
        "    else:\n",
        "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
        "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
        "\n",
        "    num_heads = attn_maps[0].shape[0]\n",
        "    num_layers = len(attn_maps)\n",
        "    seq_len = input_data.shape[0]\n",
        "    fig_size = 4 if num_heads == 1 else 3\n",
        "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads * fig_size, num_layers * fig_size))\n",
        "    if num_layers == 1:\n",
        "        ax = [ax]\n",
        "    if num_heads == 1:\n",
        "        ax = [[a] for a in ax]\n",
        "    for row in range(num_layers):\n",
        "        for column in range(num_heads):\n",
        "            ax[row][column].imshow(attn_maps[row][column], origin=\"lower\", vmin=0)\n",
        "            ax[row][column].set_xticks(list(range(seq_len)))\n",
        "            ax[row][column].set_xticklabels(input_data.tolist())\n",
        "            ax[row][column].set_yticks(list(range(seq_len)))\n",
        "            ax[row][column].set_yticklabels(input_data.tolist())\n",
        "            ax[row][column].set_title(\"Layer %i, Head %i\" % (row + 1, column + 1))\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b918b716",
      "metadata": {
        "papermill": {
          "duration": 0.020232,
          "end_time": "2022-04-09T14:37:44.634993",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.614761",
          "status": "completed"
        },
        "tags": [],
        "id": "b918b716"
      },
      "source": [
        "Finally, we can plot the attention map of our trained Transformer on the reverse task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655c60eb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:44.672583Z",
          "iopub.status.busy": "2022-04-09T14:37:44.672183Z",
          "iopub.status.idle": "2022-04-09T14:37:44.958647Z",
          "shell.execute_reply": "2022-04-09T14:37:44.958104Z"
        },
        "papermill": {
          "duration": 0.306872,
          "end_time": "2022-04-09T14:37:44.960097",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.653225",
          "status": "completed"
        },
        "tags": [],
        "id": "655c60eb",
        "outputId": "2203f110-be03-4422-8114-35b41550467c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_attention_maps(data_input, attention_maps, idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d4769e6",
      "metadata": {
        "papermill": {
          "duration": 0.019048,
          "end_time": "2022-04-09T14:37:44.998918",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.979870",
          "status": "completed"
        },
        "tags": [],
        "id": "9d4769e6"
      },
      "source": [
        "The model has learned to attend to the token that is on the flipped index of itself.\n",
        "Hence, it actually does what we intended it to do.\n",
        "We see that it however also pays some attention to values close to the flipped index.\n",
        "This is because the model doesn't need the perfect, hard attention to solve this problem,\n",
        "but is fine with this approximate, noisy attention map.\n",
        "The close-by indices are caused by the similarity of the positional encoding,\n",
        "which we also intended with the positional encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c016b9",
      "metadata": {
        "papermill": {
          "duration": 0.042832,
          "end_time": "2022-04-09T14:38:40.841468",
          "exception": false,
          "start_time": "2022-04-09T14:38:40.798636",
          "status": "completed"
        },
        "tags": [],
        "id": "c1c016b9"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we took a closer look at Transformer which is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. \n",
        "In particular we have seen:\n",
        "* The Multi-Head Attention layer which uses a scaled dot product between\n",
        "queries and keys to find correlations and similarities between input elements.\n",
        "* The architecture which is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block.\n",
        "* The property of being permutation-equivariant (if we do not provide any positional encodings), which allows it to generalize to many settings.\n",
        "\n",
        "* How it can be applied to a simple sequence-to-sequence tasks.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "id,colab,colab_type,-all",
      "formats": "ipynb,py:percent",
      "main_language": "python"
    },
    "language_info": {
      "name": "python"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 72.589227,
      "end_time": "2022-04-09T14:38:41.893966",
      "environment_variables": {},
      "exception": null,
      "input_path": "course_UvA-DL/05-transformers-and-MH-attention/Transformers_MHAttention.ipynb",
      "output_path": ".notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb",
      "parameters": {},
      "start_time": "2022-04-09T14:37:29.304739",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb881d87e35e4b81b15b69c4a2ac130f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_934a698f7f554ccabddb193766abd4d3",
              "IPY_MODEL_36942544884940edb54bd4fa13a5782b",
              "IPY_MODEL_d7571490226342489cb5d54135e10d05"
            ],
            "layout": "IPY_MODEL_83a6d3c37c454e3faf006be51cab9ed5"
          }
        },
        "934a698f7f554ccabddb193766abd4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc83b9ad878546dc9ebc9a425bf2420e",
            "placeholder": "â",
            "style": "IPY_MODEL_5b20f87f3b754a52aace79d6449a2e2d",
            "value": "Train Acc: 100.00 Train Loss: 0.00 Val Acc: 100.00  Val loss: 0.00 : 100%"
          }
        },
        "36942544884940edb54bd4fa13a5782b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33883866def3473798ca5152aac28a75",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e56fbea5ada48a8bf81f0a12415bc8b",
            "value": 5
          }
        },
        "d7571490226342489cb5d54135e10d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b65cdf33674c27ac1e43a7af3f6373",
            "placeholder": "â",
            "style": "IPY_MODEL_dbc4e786e1c64b62828dca2aa025b045",
            "value": " 5/5 [00:35&lt;00:00,  6.40s/it]"
          }
        },
        "83a6d3c37c454e3faf006be51cab9ed5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc83b9ad878546dc9ebc9a425bf2420e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b20f87f3b754a52aace79d6449a2e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33883866def3473798ca5152aac28a75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e56fbea5ada48a8bf81f0a12415bc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5b65cdf33674c27ac1e43a7af3f6373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc4e786e1c64b62828dca2aa025b045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}