\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}

% Importing settings from setup.sty
\usepackage{setup}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{glossaries}
% \makenoidxglossaries
\newacronym{pdf}{PDF}{Probability Density Function}
\newacronym{iid}{i.i.d.}{independent and identically distributed}
\newacronym{ols}{OLS}{Ordinary Least Squares}

% \pagenumbering{roman}
\begin{document}

% Inserting title page
\newgeometry{
    left=25mm,
    right=25mm,
    top=10mm,
    bottom=25mm}
\import{./}{title}

\newgeometry{
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm}
\restoregeometry
\pagenumbering{gobble}
\tableofcontents
% \listoffigures
% \listoftables

\clearpage
\pagenumbering{arabic}

\section{Exercise 1}
Let \((x_1, y_1), \ldots (x_N, y_N)\) be observations assumed to be generated by a linear model:
\begin{equation}
    \label{eqn: linear model}
    \tag{LM}
    y_i = a + bx_i + \epsilon_i
\end{equation}
with \(a,b \in \R\) and \(1 \leq i \leq N, \ \epsilon_i \sim \mathcal{N}(0, \sigma^2)\) \gls{iid}
\subsection{Question (a)}
We compute \(\E[y_i]\):
\begin{align*}
    \E \left[ y_i \right]
     & = \E \left[ a + bx_i + \epsilon_i \right]                                      & \textit{(definition)}                                                  \\
     & = \E \left[ a \right] + \E \left[ b x_i \right] + \E \left[ \epsilon_i \right] & \textit{(linearity of expectation)}                                    \\
     & = a + b x_i                                                                    & \textit{(\(a, b, x_i\) deterministic, \(\epsilon_i\) centered normal)} \\
\end{align*}
We compute \(\var(y_i)\):
\begin{align*}
    \var \left[ y_i \right]
     & = \E \left[ y_i^2 \right] - \E \left[ y_i \right]^2                                                                              \\
     & = \E \left[ \left(a + bx_i + \epsilon_i \right)^2 \right] - \left(a + b x_i\right)^2                                             \\
     & = a^2 + b^2x_i^2 + \E\left[\epsilon_i^2\right]                                       & \textit{(linearity of expectation}        \\
     & + 2abx_i + 2a \E \left[\epsilon_i\right] + 2bx_i \E\left[\epsilon_i \right]          & \textit{and only \(\epsilon_i\) random)}  \\
     & - \left(a^2 + b^2x_i^2 + 2abx_i\right)                                                                                           \\
     & = \E\left[\epsilon_i^2\right]                                                        & \textit{(\(\epsilon_i\) centered normal)} \\
     & = \E\left[\epsilon_i^2\right] - {\underbrace{\E\left[\epsilon_i\right]}_{=0}}^2                                                  \\
     & = \var(\epsilon_i)                                                                                                               \\
     & = \sigma^2                                                                                                                       \\
\end{align*}


\subsection{Question (b)}
First let us note that for a given \(1 \leq i \leq N\), by \eqref{eqn: linear model} we have that:
\begin{align}
    \label{eqn: from y_i to epsilon_i}
    y_i = \underbrace{a + bx_i}_{\textrm{deterministic}} + \underbrace{\epsilon_i}_{\mathcal{N}(0, \sigma^2)}
\end{align}
Moreover, we know that the \gls{pdf} of a random variable \(X \sim \mathcal{N}(\mu, \sigma^2)\) is given by:
\begin{equation*}
    f_{\mu, \sigma^2}(t)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(t-\mu)^{2}}{2 \sigma^{2}}\right)
\end{equation*}
and since for a given \(1 \leq i \leq N, \ \epsilon_i  \sim \mathcal{N}(0, \sigma^2)\), its \gls{pdf} is given by:
\begin{equation*}
    f_{0, \sigma^2}(t) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{t^{2}}{2 \sigma^{2}}\right)
\end{equation*}
Now applying the argument from \eqref{eqn: from y_i to epsilon_i} yields that for \(1 \leq i \leq N\), the \gls{pdf} of \(y_i\) is the following:
\begin{equation*}
    f(t; \theta) := f_{(a + bx_i), \sigma^2}(t) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{\left(t - (a+bx_i)\right)^{2}}{2 \sigma^{2}}\right)
\end{equation*}
with the parameter \(\theta := (a,b) \in \R^2\). We write \(f(t; \theta)\) (including \(\theta\)), to stress the fact that we have an influence on our parameter, not over our observations. \\
Given this preamble, our goal is to prove the following:
\begin{equation}
    \label{eqn: ex1b independence}
    f\left( \bigcap_{i=1}^N y_i ; \theta \right) = \prod_{i=1}^N f \left(y_i; \theta\right)
\end{equation}
We note that for a given \(1 \leq i \leq N\), \(a, b\) and \(x_i\) are deterministic, so we can rewrite \eqref{eqn: ex1b independence} as:
\begin{align}
    f\left( \bigcap_{i=1}^N y_i ; \theta \right)
    \nonumber
     & = f\left( \bigcap_{i=1}^N a + bx_i + \epsilon_i ; \theta \right)                                                  \\
    \nonumber
     & = f_{0, \sigma^2} \left( \bigcap_{i=1}^N \epsilon_i \right)      & \textit{(shift of the distribution)}           \\
    \nonumber
     & = \prod_{i=1}^N f_{0, \sigma^2} \left(\epsilon_i \right)         & \textit{(\(\epsilon_i\)'s are \acrshort{iid})} \\
    \nonumber
     & = \prod_{i=1}^N f \left(a + bx_i + \epsilon_i \right)                                                             \\
    \label{eqn: ex1b independence rewritten}
     & = \prod_{i=1}^N f \left(y_i; \theta\right)
\end{align}
Thus the \(y_i\)'s, \(1 \leq i \leq N\) are \gls{iid}.

\subsection{Question (c)}
Let \(g(y_i)\) denote the \gls{pdf} of \(y_i\). We define the likelihood function of \(y_i, \ 1 \leq i \leq N\) as:
\begin{align*}
    \L(\theta)
     & := f(y_1, \ldots, y_N ; \theta)                                           \\
     & = \prod_{i=1}^N f(y_i ; \theta) & \textit{(\(y_i\)'s are \acrshort{iid})} \\
\end{align*}
We also define the log-likelihood as:
\begin{equation*}
    \ell(\theta) := \log \L(\theta)
\end{equation*}
whic is equal to:
\begin{equation*}
    \ell(\theta) = \log \prod_{i=1}^N f(y_i ; \theta) = \sum_{i=1}^N \log f(y_i ; \theta)
\end{equation*}

Thus by \eqref{eqn: ex1b independence rewritten}, we obtain that the log-likelihood becomes:
\begin{align}
    \nonumber
    \ell(\theta)
     & = \sum_{i=1}^N \log f(y_i ; \theta)                                                                                                          \\
    \nonumber
     & = \sum_{i=1}^N \log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{\left(y_i - (a + bx_i)\right)^{2}}{2 \sigma^{2}}\right)\right] \\
    \label{eqn: ex1 log-likelihood}
     & = -\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right] \sum_{i=1}^N \left(y_i - a - bx_i\right)^{2}
\end{align}
We want find \(\aml\) which maximises \(\ell\) with respect to \(a\). To do so, we take the partial derivative with respect to \(a\) and set it to 0. Therefore we get:
\begin{align*}
             &
    \frac{\partial \ell}{\partial a} = 0                                                                                                                                    \\
    \implies &
    \frac{\partial}{\partial a} \left[-\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right] \sum_{i=1}^N \left(y_i - bx_i - \aml \right)^{2}\right] = 0  \\
    \implies &
    -\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right] \sum_{i=1}^N \frac{\partial}{\partial a} \left[ \left(y_i - bx_i - \aml \right)^{2}\right] = 0 \\
    \implies &
    \sum_{i=1}^N \frac{\partial}{\partial a} \left[ \left(y_i - bx_i - \aml \right)^{2}\right] = 0                                                                          \\
    \implies &
    \sum_{i=1}^N -2 \left(y_i - bx_i - \aml \right) = 0                                                                                                                     \\
    \implies &
    \sum_{i=1}^N \left(y_i - bx_i - \aml \right) = 0                                                                                                                        \\
    \implies &
    - N \aml + \sum_{i=1}^N y_i - bx_i = 0                                                                                                                                  \\
    \implies &
    \aml = \frac{1}{N} \sum_{i=1}^N y_i - bx_i                                                                                                                              \\
\end{align*}
where we used the linearity of differentiation. \\
Defining \(\bar{x}, \bar{y}\) as follows:
\begin{align*}
     & \bar{x} := \frac{1}{N} \sum_{i=1}^N x_i \\
     & \bar{y} := \frac{1}{N} \sum_{i=1}^N y_i
\end{align*}
we can rewrite \(\aml\) as:
\begin{equation}
    \label{eqn: preliminary aml}
    \aml = \bar{y} - b \bar{x}
\end{equation}
\subsection{Question (d)}
Now we want to find \(\bml\). We proceed similarily, that is we differentiate \(\ell\) with respect to \(b\) and equate it to 0:
\begin{align*}
             &
    \frac{\partial \ell}{\partial b} = 0                                                                                                                                     \\
    \implies &
    \frac{\partial}{\partial b} \left[-\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right] \sum_{i=1}^N \left(y_i - a - \bml x_i \right)^{2}\right] = 0  \\
    \implies &
    -\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right] \sum_{i=1}^N \frac{\partial}{\partial b} \left[ \left(y_i - a - \bml x_i \right)^{2}\right] = 0 \\
    \implies &
    \sum_{i=1}^N \frac{\partial}{\partial b} \left[ \left(y_i - a - \bml x_i \right)^{2}\right] = 0                                                                          \\
    \implies &
    \sum_{i=1}^N -2x_i \left(y_i - a - \bml x_i \right) = 0                                                                                                                  \\
    \implies &
    \sum_{i=1}^N x_i \left(y_i - a \right) - \sum_{i=1}^N \bml x_i^2 = 0                                                                                                     \\
    \implies &
    \sum_{i=1}^N x_i \left(y_i - a \right) = \bml \sum_{i=1}^N x_i^2                                                                                                         \\
    \implies &
    \bml = \frac{\sum_{i=1}^N x_i \left(y_i - a \right)}{\sum_{j=1}^N x_j^2}                                                                                                 \\
\end{align*}
where we used the linearity of differentiation. \\
We now plug in the value of \(\aml\) found in \eqref{eqn: preliminary aml}, which gives us the following system:
\begin{align*}
             &
    \begin{cases}
        \aml = \bar{y} - \bml \bar{x} \\
        \bml = \frac{1}{\sum_{j=1}^N x_j^2} \sum_{i=1}^N x_i \left(y_i - \aml \right)
    \end{cases} \\
    \implies &
    \begin{cases}
        \aml = \bar{y} - \bml \bar{x} \\
        \bml = \frac{1}{\sum_{j=1}^N x_j^2} \sum_{i=1}^N x_i \left(y_i - (\bar{y} - \bml \bar{x}) \right)
    \end{cases} \\
    \implies &
    \begin{cases}
        \aml = \bar{y} - \bml \bar{x} \\
        \bml \sum_{i=1}^N x_i^2 = \sum_{i=1}^N x_i y_i - x_i\bar{y} + \sum_{i=1}^N\bml \bar{x}x_i
    \end{cases} \\
    \implies &
    \begin{cases}
        \aml = \bar{y} - \bml \bar{x} \\
        \bml \left[\sum_{i=1}^N x_i^2 - \bar{x}x_i\right] = \sum_{i=1}^N x_i y_i - x_i\bar{y}
    \end{cases} \\
    \implies &
    \begin{cases}
        \aml = \bar{y} - \bml \bar{x} \\
        \bml = \left[\sum_{i=1}^N x_i (y_i-\bar{y})\right] / \left[\sum_{i=1}^N x_i(x_i-\bar{x})\right]
    \end{cases} \\
\end{align*}
Now, let \(x\) and \(y\) be as follows:
\begin{align*}
    x :=
    \begin{bmatrix}
        x_1    \\
        \vdots \\
        x_N
    \end{bmatrix} \in \R^n \\
    y :=
    \begin{bmatrix}
        y_1    \\
        \vdots \\
        y_N
    \end{bmatrix} \in \R^n \\
\end{align*}
and let \(\langle \cdot, \cdot \rangle\) denote the classical dot product. \\
Then, we rewrite our previous system and inject \(\bml\) into the equation of \(\aml\) to solve it.
\begin{align*}
             &
    \begin{cases}
        \aml = \bar{y} - \bml \bar{x}                                                 \\
        \bml = \langle x, y-\bar{y}\rangle / {\left\langle x, x-\bar{x}\right\rangle} \\
    \end{cases} \\
    \implies &
    \begin{cases}
        \aml = \bar{y} - \left[\bar{x} \langle x, y-\bar{y}\rangle / \left\langle x, x-\bar{x}\right\rangle\right] \\
        \bml = \langle x, y-\bar{y}\rangle / \left\langle x, x-\bar{x}\right\rangle                                \\
    \end{cases} \\
\end{align*}

\subsection{Question (e)}
We know that the \gls{ols} estimates are found by minimising the Residual Sum of Squares, which is given by:
\begin{equation*}
    R_{SS}(a,b) := \sum_{i=1}^N \left(y_i - a - bx_i\right)^2 = \sum_{i=1}^N \epsilon_i^2
\end{equation*}
In other words, the \gls{ols} estimates are given by:
\begin{equation*}
    (\aols, \bols) = \arg \min_{(a,b) \in \R^2} R_{SS}(a,b)
\end{equation*}
We have that the Maximum Likelihood estimators (\(\aml, \bml\)) maximise our log-likelihood function \(\ell\), which means:
\begin{align*}
             &
    \left(\aml, \bml\right) = \arg \max_{(a,b) \in \R^2} \ell(\theta)                                                                                                                                                            \\
    \implies &
    \left(\aml, \bml\right) = \arg \max_{(a,b) \in \R^2} -\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right] \sum_{i=1}^N \left(y_i - a - bx_i\right)^{2}                                                   \\
    \implies &
    \left(\aml, \bml\right) = \arg \min_{(a,b) \in \R^2} \underbrace{\frac{N}{2 \sigma^2}\log \left[\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right]}_{\text{doesn't depend on \((a, b)\)}} \sum_{i=1}^N \left(y_i - a - bx_i\right)^{2} \\
    \implies &
    \left(\aml, \bml\right) = \arg \min_{(a,b) \in \R^2} \sum_{i=1}^N \left(y_i - a - bx_i\right)^{2}                                                                                                                            \\
    \implies &
    \left(\aml, \bml\right) = \arg \min_{(a,b) \in \R^2} R_{SS}(a,b)                                                                                                                                                             \\
    \implies &
    \left(\aml, \bml\right) = \left(\aols, \bols\right)                                                                                                                                                                          \\
\end{align*}
We get that for \(1 \leq i \leq N, \ \epsilon_i \sim \mathcal{N}\left(0, \sigma^2\right)\), the maximum-likelihood estimates and the \gls{ols} are equal.

\section{Exercise 2} % see lecture from 17/12
\subsection{Question (a)}
We compute \(\E\left[\betols\right]\)
\begin{align*}
    \E\left[\betols\right]
     & = \E\left[ \left( X^T X \right)^{-1}X^T Y \right]                                                                                                                                 \\
     & = \E\left[ \left( X^T X \right)^{-1}X^T \left(X\beta + \epsilon\right) \right]                                                                                                    \\
     & = \E\left[ \left( X^T X \right)^{-1}X^T X \beta + \left( X^T X \right)^{-1}X^T \epsilon \right]                                                                                   \\
     & = \E\left[ \beta \right] + \E\left[ \underbrace{\left( X^T X \right)^{-1}X^T}_{\text{deterministic}} \epsilon \right] & \textit{(linearity of expectation)}                       \\
     & = \E\left[ \beta \right] + \left( X^T X \right)^{-1}X^T \underbrace{\E\left[  \epsilon \right]}_{=0}                                                                              \\
     & = \E\left[ \beta \right]                                                                                              & \textit{(\(\epsilon \sim \mathcal{N}(0, \sigma^2 I_N)\))} \\
     & = \beta                                                                                                               & \textit{(\(\beta\) deterministic)}                        \\
\end{align*}
We have \(\E\left[\betols\right] = \beta\), therefore \(\betols\) is an unbiased estimator.

\subsection{Question (b)}
We know that:
\begin{align*}
    \var\left[\betols\right]
     & = \E\left[ \left(\betols - \beta\right) \left(\betols - \beta\right)^T \right] \\
\end{align*}
Let us first exaluate \(\betols - \beta\):
\begin{align*}
    \betols - \beta
     & = \left( X^T X \right)^{-1}X^T Y - \beta                              \\
     & = \left( X^T X \right)^{-1}X^T \left(X\beta + \epsilon\right) - \beta \\
     & = \beta + \left( X^T X \right)^{-1}X^T \epsilon - \beta               \\
     & = \left( X^T X \right)^{-1}X^T \epsilon                               \\
\end{align*}
We now compute \(\var\left[\betols\right]\):
\begin{align*}
    \var\left[\betols\right]
     & = \E\left[ \left(\betols - \beta\right) \left(\betols - \beta\right)^T \right]                                                                                                                     \\
     & = \E\left[ \left(\left( X^T X \right)^{-1}X^T \epsilon\right) \left(\left( X^T X \right)^{-1}X^T \epsilon\right)^T \right]                                                                         \\
     & = \E\left[ \left( X^T X \right)^{-1}X^T \epsilon \epsilon^T X \left(\left( X^T X \right)^{-1}\right)^{T} \right]                                                                                   \\
     & = \E\left[ \left( X^T X \right)^{-1}X^T \epsilon \epsilon^T X \left(\left( X^T X \right)^{T}\right)^{-1} \right]                                                                                   \\
     & = \E\left[ \left( X^T X \right)^{-1}X^T \epsilon \epsilon^T X \left(X^T X \right)^{-1} \right]                                                                                                     \\
     & = \left( X^T X \right)^{-1}X^T \E\left[\epsilon \epsilon^T \right] X \left(X^T X \right)^{-1}                                                                                                      \\
     & = \left( X^T X \right)^{-1}X^T \sigma^2 I_N X \left(X^T X \right)^{-1}                                                     & \textit{( \(\E[\epsilon\epsilon^T] = \sigma^2 I_N\), since}            \\
     &                                                                                                                            & \textit{\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\) \acrshort{iid})} \\
     & = \sigma^2 \left( X^T X \right)^{-1}X^T X \left(X^T X \right)^{-1}                                                         & \textit{(\(\sigma^2 \in \R\),}                                        \\
     &                                                                                                                            & \textit{therefore commutes)}                                          \\
     & = \sigma^2 \left( X^T X \right)^{-1}                                                                                                                                                               \\
\end{align*}
where we used that:
\begin{itemize}
    \item The transpose of a product is the product of the transposed factors in reverse order
    \item The transpose of the inverse is the inverse of the transpose.
\end{itemize}


\clearpage

% \printnoidxglossaries

\end{document}