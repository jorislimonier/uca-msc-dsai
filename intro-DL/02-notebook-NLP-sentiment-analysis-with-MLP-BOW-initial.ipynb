{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o22rTSh_5HE_"
   },
   "source": [
    "# Sentiment analysis with BOW representation\n",
    "\n",
    "Text classification is a machine learning technique that assigns a set of predefined categories to open-ended text. Text classifiers can be used to organize, structure, and categorize pretty much any kind of text – from documents, medical studies and files, and all over the web.\n",
    "\n",
    "For example, new articles can be organized by topics; support tickets can be organized by urgency; chat conversations can be organized by language; brand mentions can be organized by sentiment; and so on.\n",
    "\n",
    "Text classification is one of the fundamental tasks in natural language processing with broad applications such as **sentiment analysis**, topic labeling, spam detection, and intent detection.\n",
    "\n",
    "**Why is Text Classification Important?**\n",
    "\n",
    "It’s estimated that around 80% of all information is unstructured, with text being one of the most common types of unstructured data. Because of the messy nature of text, analyzing, understanding, organizing, and sorting through text data is hard and time-consuming, so most companies fail to use it to its full potential.\n",
    "\n",
    "This is where text classification with machine learning comes in. Using text classifiers, companies can automatically structure all manner of relevant text, from emails, legal documents, social media, chatbots, surveys, and more in a fast and cost-effective way. This allows companies to save time analyzing text data, automate business processes, and make data-driven business decisions.\n",
    "\n",
    "**How Does Text Classification Work?**\n",
    "\n",
    "Instead of relying on manually crafted rules, machine learning text classification learns to make classifications based on past observations. By using pre-labeled examples as training data, machine learning algorithms can learn the different associations between pieces of text, and that a particular output (i.e., tags) is expected for a particular input (i.e., text). A “tag” is the pre-determined classification or category that any given text could fall into.\n",
    "\n",
    "The first step towards training a machine learning NLP classifier is feature extraction: a method is used to transform each text into a numerical representation in the form of a vector. One of the most frequently used approaches is bag of words, where a vector represents the frequency of a word in a predefined dictionary of words.\n",
    "\n",
    "Then, the machine learning algorithm is fed with training data that consists of pairs of feature sets (vectors for each text example) and tags (e.g. sports, politics) to produce a classification model:\n",
    "\n",
    "![training](https://monkeylearn.com/static/507a7b5d0557f416857a038f553865d1/2ed04/text_process_training.webp)\n",
    "\n",
    "Once it’s trained with enough training samples, the machine learning model can begin to make accurate predictions. The same feature extractor is used to transform unseen text to feature sets, which can be fed into the classification model to get predictions on tags (e.g., sports, politics):\n",
    "\n",
    "![prediction](https://monkeylearn.com/static/afa7e0536886ee7152dfa4c628fe59f0/2b924/text_process_prediction.webp)\n",
    "\n",
    "Text classification with machine learning is usually much more accurate than human-crafted rule systems, especially on complex NLP classification tasks. Also, classifiers with machine learning are easier to maintain and you can always tag new examples to learn new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgzggwPnvq_o"
   },
   "source": [
    "## Today lab\n",
    "\n",
    "In this lab we use part of the 'Amazon_Unlocked_Mobile.csv' dataset published by Kaggle. The dataset contain the following information:\n",
    "* Product Name\n",
    "* Brand Name\n",
    "* Price\n",
    "* Rating\n",
    "* Reviews\n",
    "* Review Votes\n",
    "\n",
    "We are mainly interested by the 'Reviews' (X) and by the 'Rating' (y)\n",
    "\n",
    "The goal is to try to predict the 'Rating' after reading the 'Reviews'. I've prepared for you TRAIN and TEST set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmQlCKQVvq_p",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Today-lab\" data-toc-modified-id=\"Today-lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Today lab</a></span></li><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-Train,-validation-and-test-sets\" data-toc-modified-id=\"About-Train,-validation-and-test-sets-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\" rel=\"nofollow\" target=\"_blank\">About Train, validation and test sets</a></a></span></li><li><span><a href=\"#Undestand-the-dataset\" data-toc-modified-id=\"Undestand-the-dataset-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Undestand the dataset</a></span></li></ul></li><li><span><a href=\"#Build-X-(features-vectors)-and-y-(labels)\" data-toc-modified-id=\"Build-X-(features-vectors)-and-y-(labels)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build X (features vectors) and y (labels)</a></span></li><li><span><a href=\"#Our-previous-baseline\" data-toc-modified-id=\"Our-previous-baseline-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Our previous baseline</a></span></li><li><span><a href=\"#Build-an-MLP-Classifier\" data-toc-modified-id=\"Build-an-MLP-Classifier-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Build an MLP Classifier</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZg2dPXvvq_q"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:55.195443Z",
     "start_time": "2022-01-19T13:17:52.018950Z"
    },
    "id": "8wxTLStrvq_r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:55.977676Z",
     "start_time": "2022-01-19T13:17:55.239096Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2638,
     "status": "ok",
     "timestamp": 1634734106855,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "MLWzp9CAvq_t",
    "outputId": "abb836f6-8137-43a7-decb-f1d7a8a981be"
   },
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/train.csv.gz\")\n",
    "TEST = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/test.csv.gz\")\n",
    "\n",
    "TRAIN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2eC1L4fvq_0"
   },
   "source": [
    "## Build X (features vectors) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:56.008775Z",
     "start_time": "2022-01-19T13:17:55.985761Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1634738820951,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "dOnscXEBvq_0",
    "outputId": "4364f457-545c-49d3-c2fe-5540d52b0f58"
   },
   "outputs": [],
   "source": [
    "# Construct X_train and y_train\n",
    "X_train = TRAIN['Reviews'].fillna(\"\")\n",
    "y_train = TRAIN['Rating']\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:56.063136Z",
     "start_time": "2022-01-19T13:17:56.021507Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1634738821203,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "338OMkmHvq_1",
    "outputId": "aea649ad-4a81-4aa7-976f-4b97c3461644"
   },
   "outputs": [],
   "source": [
    "# Construct X_test and y_test\n",
    "X_test = TEST['Reviews'].fillna(\"\")\n",
    "y_test = TEST['Rating']\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L3YiNzWvq_1"
   },
   "source": [
    "## Features extraction\n",
    "\n",
    "A bag-of-words model is a way of extracting features from text so the text input can be used with machine learning algorithms or neural networks.\n",
    "\n",
    "Each document, in this case a review, is converted into a vector representation. The number of items in the vector representing a document corresponds to the number of words in the vocabulary. The larger the vocabulary, the longer the vector representation, hence the preference for smaller vocabularies in the previous section.\n",
    "\n",
    "Words in a document are scored and the scores are placed in the corresponding location in the representation.\n",
    "\n",
    "In order to extract feature, you can use `CountVectorizer` or `TfidfVectorizer` and you can perform the desired text cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbTccrE7B2JX"
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "> * Quickly remind what are `CountVectorizer`, `TfidfVectorizer` and how they work.\\\n",
    "`CountVectorizer` counts how many times a word occurs in the analysed text.\\\n",
    "`TfidfVectorizer` also counts how many times a word occurs in the analysed text, but then it compares it with its usual frequency within the whole corpus. This allows to get an idea of how a word is over-represented in the analyzed text and it disregards words that simply occur a lot in the whole corpus.\n",
    "\n",
    "> * Build the BOW representation for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:56.091783Z",
     "start_time": "2022-01-19T13:17:56.075456Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preproc_pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    ")\n",
    "# Extract features\n",
    "preproc_X_train = preproc_pipe.fit_transform(X_train)\n",
    "preproc_X_test = preproc_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8234,
     "status": "ok",
     "timestamp": 1634739073199,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "aErFvDg6vq_2",
    "outputId": "1caadb43-483e-4be7-920a-ed3d45dc265d"
   },
   "source": [
    "## Build a baseline with logistic regression.\n",
    "\n",
    "Using the previous BOW representation, fit a logistic regression model and evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$[TODO - Students]$$ \n",
    "> * Quickly remind what are `LogisticRegression` and how they work.\\\n",
    "`LogisticRegression` computes a prediction probability for each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * what are the possible metrics. Choose one and justify your choice.\\\n",
    "We could use accuracy, recall, precision or f1-score. I choose to use the f1-score because our data set is unbalanced (class 1 ->  885, class 2 ->  291, class 3 ->  385, class 4 ->  747, class 5 -> 2692, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:56.129136Z",
     "start_time": "2022-01-19T13:17:56.105750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build your model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "param_search = {\n",
    "    # \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": np.linspace(0.01,4),\n",
    "    \"max_iter\": [1000],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf, param_search, n_jobs=-1, verbose=1, scoring=\"f1_weighted\")\n",
    "search.fit(preproc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_params = search.best_params_\n",
    "log_reg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:17:56.167207Z",
     "start_time": "2022-01-19T13:17:56.137072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate your model\n",
    "y_pred_log_reg = search.predict(preproc_X_test)\n",
    "log_reg_score = f1_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
    "print(f\"Logistic Regression best score: {round(log_reg_score, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdEeQY2Hvq_2"
   },
   "source": [
    "## Build an MLP Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.502107Z",
     "start_time": "2022-01-19T13:17:56.178741Z"
    },
    "id": "Z_XKcRR9vq_3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.511769Z",
     "start_time": "2022-01-19T13:18:02.503798Z"
    }
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "> * Quickly remind what are `Multi Layer Perceptron` and how they work.\\\n",
    "`Multi Layer Perceptron` is a class of feed-forward neural network. They contain an input layer, an output layer and one or more hidden layer, each with an activation function.\n",
    "> * If necessary, One hot encode the output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.515293Z",
     "start_time": "2022-01-19T13:18:02.515272Z"
    },
    "id": "m0jx2omzvq_3"
   },
   "outputs": [],
   "source": [
    "# Encode output vector if necessary.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe_y = OneHotEncoder()\n",
    "y_train_ohe = ohe_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_ohe = ohe_y.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLoZRWAkC2Qw"
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "> * What is the size of the input vector and the output vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.520539Z",
     "start_time": "2022-01-19T13:18:02.520496Z"
    },
    "id": "HuUkAgdxvq_4"
   },
   "outputs": [],
   "source": [
    "# Define constant\n",
    "input_dim = preproc_X_train.shape[1] # number of features of X_train\n",
    "output_dim = y_train_ohe.shape[1] # number of classes (that we spread into 5 dimensions by OneHotEncod-ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGUO66DRDQDY"
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "\n",
    "> * Build a simple network to predict the star rating of a review using the functional API. It should have the following characteristic : one hidden layer with 256 nodes and relu activation.\n",
    "> * What is the activation function of the output layer?\\\n",
    "Softmax since we are tackling a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.524493Z",
     "start_time": "2022-01-19T13:18:02.524459Z"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1634741548737,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "LiuynQmZMFy7"
   },
   "outputs": [],
   "source": [
    "# Build your MLP\n",
    "inputs = Input(shape=(input_dim,))\n",
    "x = Dense(256, activation=\"relu\")(inputs)\n",
    "outputs = Dense(output_dim, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axBVvIHCD4_h"
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "\n",
    "We are now compiling and training the model.\n",
    "> * Using the tensorflow documentation, explain the purpose the EarlyStopping callback and detail its arguments.\\\n",
    "As per the Tensorflow documentation:\\\n",
    "*Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.*\\\n",
    "In other words, `EarlyStopping` stops the training process (*i.e.* the `fit` method) if no more progress is made in terms of the loss.\n",
    "\n",
    "> * Compile the model\n",
    "> * Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.528750Z",
     "start_time": "2022-01-19T13:18:02.528721Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29782,
     "status": "ok",
     "timestamp": 1634740224894,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "C4zg06r3vq_6",
    "outputId": "0f308061-087c-4724-e163-c3649c762f07"
   },
   "outputs": [],
   "source": [
    "# Compile the model and start training\n",
    "# Stop training with early stopping with patience of 20\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=tfa.metrics.F1Score(5))\n",
    "\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=20)\n",
    "history = model.fit(x=preproc_X_train.toarray(), y=y_train_ohe.toarray(), epochs=100, callbacks=[callback], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:14:16.701866Z",
     "start_time": "2022-01-19T13:14:16.693977Z"
    }
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "\n",
    "> * Babysit your model: plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.532757Z",
     "start_time": "2022-01-19T13:18:02.532717Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1634739502335,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "UAGMeWF_vq_7",
    "outputId": "08483bc2-2034-4535-cc6d-0363246d974b"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "# It's possible to plot them very easily using: pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvu-O2JJHNrA"
   },
   "source": [
    "$$[TODO - Students]$$ \n",
    "\n",
    "> * How do you interpret those learning curves ?\\\n",
    "There is clearly something wrong since we see close to no improvement over the epochs. This could be due to overfitting.\n",
    "\n",
    "The model appears to overfit the training data. Various strategies could reduce the overfitting but for this lab we will just change the number and size of layers. We will do that a little later.\n",
    "\n",
    "> * Evaluate the model (on test part) and plot confusion matrix.\n",
    "> * Are you doing better or worse than with our first attempt with Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.536063Z",
     "start_time": "2022-01-19T13:18:02.536032Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1634741041810,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "zS8fi1z3vq_8",
    "outputId": "fd7a327b-b3fb-4e59-c44b-f081a25499da"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "pred_proba = model.predict(preproc_X_test.toarray())\n",
    "y_pred = np.argmax(pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_initial_score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"Initial MLP best score: {round(mlp_initial_score, 4)}\")\n",
    "print(f\"Logistic Regression best score: {round(log_reg_score, 4)}\")\n",
    "print(\"MLP performs worse than linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T13:18:02.541220Z",
     "start_time": "2022-01-19T13:18:02.541076Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1634741048104,
     "user": {
      "displayName": "Antoine Collin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "_gSjyTEMH2nQ",
    "outputId": "1c052541-d573-48ad-b4e8-95061f705763"
   },
   "outputs": [],
   "source": [
    "# Print/plot the confusion matrix\n",
    "tf.math.confusion_matrix(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCfHu2EtKWtC"
   },
   "source": [
    "## Hyper-parameters search\n",
    "\n",
    "Using [KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) and modifying various hyper-parameters, improve your model. Change in particular the number of layers, the number of neurons per layer, the dropout, the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtJUkEx6xGL2"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de 02-notebook-NLP-sentiment-analysis-with-MLP-AC-Students.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
