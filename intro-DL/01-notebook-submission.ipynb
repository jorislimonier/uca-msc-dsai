{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-:-a-visual-intuition-of-activation-functions\" data-toc-modified-id=\"Introduction-:-a-visual-intuition-of-activation-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction : a visual intuition of activation functions</a></span></li><li><span><a href=\"#Build-an-MLP-to-classify-MNIST-images\" data-toc-modified-id=\"Build-an-MLP-to-classify-MNIST-images-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Build an MLP to classify MNIST images</a></span></li></ul></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to the Multi Layer Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction : a visual intuition of activation functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a regression task on the sinus function, we'll try to get an intuition of the effect of activation functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.arange(-3*np.pi, 3*np.pi, 0.01)\n",
    "y = np.sin(X)\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.plot(X, y, label='sinus', color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.scatter(X_train, y_train, alpha=0.03, label='training data')\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='testing data')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$[TODO - Students]$$\n",
    "#\n",
    "* Build a model with 1 hidden layer in 1 dimension and train it on x_train, y_train.\n",
    "* What activation should we use for ouput layer ? A linear activation function since we are in a regression problem\n",
    "* What loss should we use ? MSE since we are in a regression problem\n",
    "#\n",
    "* Try different activations for the hidden layer and plot the predictions obtained on x_test\n",
    "* Plot also a learning curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input layer, you have to fix the number of features\n",
    "inputs = Input(shape=(1,), name=\"inputs\")\n",
    "# One hidden layer with 1 neuron\n",
    "x_sig = Dense(1, activation=\"sigmoid\", name=\"layer_1\")(inputs)\n",
    "out_sig = Dense(1, activation=\"linear\", name=\"output\")(x_sig)\n",
    "model = Model(inputs, out_sig)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot leaning cuve\n",
    "plt.figure(figsize=(13, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_prediction(title, model):\n",
    "    y_hat_test = model.predict(X_test)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X_test, y_test, label='ground_truth', alpha=0.1)\n",
    "    plt.scatter(X_test, y_hat_test, label='predicted', alpha=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_prediction(\"Sigmoid\", model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$[TODO - Students]$$\n",
    "Try adding layers and increasing the layers dimension to better fit the test data. You can use the following function to quickly build your models.\n",
    "* Try different n_layers (for example 1, 10, 100)\n",
    "* Try different hidden_dim (for example 32, 128, 256, 512)\n",
    "* Try different bach size\n",
    "* Try to understand the `patience` parameters of early stopping -> It represents the number of epochs to wait before early stop if no progress is made on the validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_sin_regression(activation, n_layers, hidden_dim):\n",
    "    input = Input(shape=(1,), name='input')\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(input_shape=(1,), units=hidden_dim,\n",
    "                      activation=activation, name='layer_'+str(i))(input)\n",
    "        else:\n",
    "            x = Dense(units=hidden_dim, activation=activation,\n",
    "                      name='layer_'+str(i))(x)\n",
    "\n",
    "    output = Dense(1, activation='linear', name='output')(x)\n",
    "    model = Model(input, output, name='sinus_regression')\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers_val = [1, 10, 100]\n",
    "hidden_dim_val = [32, 128, 256, 512]\n",
    "batch_size_val = [16, 24]\n",
    "\n",
    "# try all combinations of provided values\n",
    "for n_layers in n_layers_val:\n",
    "    for hidden_dim in hidden_dim_val:\n",
    "        for batch_size in batch_size_val:\n",
    "            model = build_sin_regression(\n",
    "                activation=\"sigmoid\", n_layers=n_layers, hidden_dim=hidden_dim)\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            model.summary()\n",
    "\n",
    "            plot_model(model)\n",
    "\n",
    "            callbacks_list = [EarlyStopping(monitor='val_loss', min_delta=0.005, patience=8, verbose=2, mode='min', restore_best_weights=True)\n",
    "                              ]\n",
    "\n",
    "            history = model.fit(X_train, y_train, validation_split=0.1,\n",
    "                                callbacks=callbacks_list, batch_size=batch_size, epochs=20)\n",
    "\n",
    "            plt.figure(figsize=(13, 8))\n",
    "            plt.plot(history.history['loss'], label=\"loss\")\n",
    "            plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            y_hat_test = model.predict(X_test)\n",
    "\n",
    "            plt.scatter(X_test, y_test, label='ground_truth', alpha=0.1)\n",
    "            plt.scatter(X_test, y_hat_test, label='predicted', alpha=0.5)\n",
    "\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an MLP to classify MNIST images\n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We’ll call the images “x” and the labels “y”. Both the training set and test set contain images and their corresponding labels; for example, the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "\n",
    "# the data, shuffled and split between a train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape, y_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape the image from 3d to 2d (nb_items, other dime)\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data (input between 0 and 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One hot encode the label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buid MLP model\n",
    "# You can use the following function\n",
    "def build_MLP(input_shape, activation, layers, nb_class):\n",
    "    input = Input(shape=(input_shape,), name='input')\n",
    "\n",
    "    for i, hidden_size in enumerate(layers):\n",
    "        if i == 0:\n",
    "            x = Dense(input_shape=(input_shape,), units=hidden_size,\n",
    "                      activation=activation, name='layer_'+str(i))(input)\n",
    "        else:\n",
    "            x = Dense(units=hidden_size, activation=activation,\n",
    "                      name='layer_'+str(i))(x)\n",
    "\n",
    "    output = Dense(nb_class, activation='softmax', name='output')(x)\n",
    "    model = Model(input, output, name='mnist_classifier')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_MLP()  # TO BE COMPLETED\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile and fit the model\n",
    "callbacks_list = [EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience=20,\n",
    "                                verbose=2, mode='min', restore_best_weights=True)\n",
    "                  ]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"], optimizer='adam')\n",
    "history = model.fit(''' X''', ''' y ''', validation_split=0.1, callbacks=callbacks_list,\n",
    "                    batch_size=32, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print history keys\n",
    "history.history.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Babysit your model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(26, 8))\n",
    "\n",
    "ax1.plot(history.history['loss'], label=\"loss\")\n",
    "ax1.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(history.history['accuracy'], label=\"accuracy\")\n",
    "ax2.plot(history.history['val_accuracy'], label=\"val_accuracy\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test_enc)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify the network in order to obtain better accuracy (better than 0.96)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
