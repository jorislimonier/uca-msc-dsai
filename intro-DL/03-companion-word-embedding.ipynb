{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#BOW-representation\" data-toc-modified-id=\"BOW-representation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>BOW representation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-BOW-representation\" data-toc-modified-id=\"Binary-BOW-representation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Binary BOW representation</a></span></li><li><span><a href=\"#Count-BOW-representation\" data-toc-modified-id=\"Count-BOW-representation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Count BOW representation</a></span></li><li><span><a href=\"#Frequence-representation\" data-toc-modified-id=\"Frequence-representation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Frequence representation</a></span></li><li><span><a href=\"#TfIDf-representation\" data-toc-modified-id=\"TfIDf-representation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>TfIDf representation</a></span></li><li><span><a href=\"#MLP-predictor\" data-toc-modified-id=\"MLP-predictor-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>MLP predictor</a></span></li></ul></li><li><span><a href=\"#Word-embedding\" data-toc-modified-id=\"Word-embedding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word embedding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-embedding\" data-toc-modified-id=\"Keras-embedding-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Keras embedding</a></span></li><li><span><a href=\"#Glove/Word2Vec/FastText-embedding\" data-toc-modified-id=\"Glove/Word2Vec/FastText-embedding-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Glove/Word2Vec/FastText embedding</a></span></li><li><span><a href=\"#Train-your-own-model-with-gensim\" data-toc-modified-id=\"Train-your-own-model-with-gensim-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Train your own model with gensim</a></span></li></ul></li><li><span><a href=\"#Contextual-word-embedding\" data-toc-modified-id=\"Contextual-word-embedding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Contextual word embedding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:50:33.689901Z",
     "start_time": "2022-01-21T16:50:33.681856Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input, TextVectorization, Dense, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:50:34.322622Z",
     "start_time": "2022-01-21T16:50:34.310911Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = np.array([\"I like chocolate\",\n",
    "            \"I like tea\",\n",
    "            \"You didn't know Paris\",\n",
    "            \"You like chocolate\",\n",
    "            'You hate tea'])\n",
    "labels = np.array([1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW representation\n",
    "\n",
    "We will use here the tools proposed by Keras in tensorflow.keras.preprocessing.text.\n",
    "\n",
    "* The [Tokenizer](tensorflow.keras.preprocessing.text), separates a list of strings into tokens\n",
    "* The `fit_on_sequences` function builds the vocabulary dictionary (i.e. assigns an index to each word). \n",
    "* The `text_to_matrix` function builds the BOW representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary BOW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:58:06.181009Z",
     "start_time": "2022-01-21T16:58:06.165958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "\n",
    "tokenize = Tokenizer(num_words=vocab_size, char_level=False)\n",
    "tokenize.fit_on_texts(texts) # Remember : you have to fit only on a train part\n",
    "tokenize.texts_to_matrix(texts, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count BOW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:58:07.256745Z",
     "start_time": "2022-01-21T16:58:07.244723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "\n",
    "tokenize = Tokenizer(num_words=vocab_size, char_level=False)\n",
    "tokenize.fit_on_texts(texts) # Remember : you have to fit only on a train part\n",
    "tokenize.texts_to_matrix(texts, mode='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:58:51.896134Z",
     "start_time": "2022-01-21T16:58:51.887382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33333333, 0.        , 0.33333333, 0.33333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.33333333, 0.        ,\n",
       "        0.33333333, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.25      , 0.        , 0.        ,\n",
       "        0.        , 0.25      , 0.25      , 0.25      , 0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.        , 0.33333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.33333333, 0.        , 0.        , 0.        , 0.33333333]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "\n",
    "tokenize = Tokenizer(num_words=vocab_size, char_level=False)\n",
    "tokenize.fit_on_texts(texts) # Remember : you have to fit only on a train part\n",
    "tokenize.texts_to_matrix(texts, mode='freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIDf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:58:20.147281Z",
     "start_time": "2022-01-21T16:58:20.139717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.81093022, 0.        , 0.98082925, 0.98082925,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.81093022, 0.        , 0.98082925, 0.        ,\n",
       "        0.98082925, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.81093022, 0.        , 0.        ,\n",
       "        0.        , 1.25276297, 1.25276297, 1.25276297, 0.        ],\n",
       "       [0.        , 0.81093022, 0.81093022, 0.        , 0.98082925,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.81093022, 0.        , 0.        ,\n",
       "        0.98082925, 0.        , 0.        , 0.        , 1.25276297]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "\n",
    "tokenize = Tokenizer(num_words=vocab_size, char_level=False)\n",
    "tokenize.fit_on_texts(texts) # Remember : you have to fit only on a train part\n",
    "tokenize.texts_to_matrix(texts, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T17:04:09.990467Z",
     "start_time": "2022-01-21T17:04:09.943325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_ = Input(shape=(vocab_size, ), name=\"input\", dtype=tf.float32)\n",
    "x = Dense(128, activation=\"relu\", name=\"hidden\")(input_)\n",
    "output_ = Dense(1, activation='sigmoid', name=\"output\")(x)\n",
    "model = Model(input_, output_)\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T17:06:19.938618Z",
     "start_time": "2022-01-21T17:06:19.927546Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tokenize.texts_to_matrix(texts, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T17:06:21.179771Z",
     "start_time": "2022-01-21T17:06:20.497726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(X, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "Word Embedding is a technique for natural language processing. **The technique uses a neural network model** to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, **Word Embedding represents each distinct word with a particular vector**. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.\n",
    "\n",
    "The underlying idea here is that similar words will have a minimum distance between their vectors. For example fruits like apple, mango, banana should be placed close whereas books will be far away from these words. In a broader sense, word embedding will create the vector of fruits which will be placed far away from vector representation of books.\n",
    "\n",
    "Somes techniques used to create embeddings:\n",
    "* Word2vec (Google) - 2 techniques: Continuous Bag of Words (CBoW) and Skip-Gram;\n",
    "* Global Vectors or GloVe (Stanford);\n",
    "* fastText (Facebook) — interesting fact: accounts for out of vocabulary words.\n",
    "\n",
    "It is also easy to build your own embedding:\n",
    "* Using the Keras Embedding layer. The embedding will be adjusted to your dataset and does not require a specific corpus\n",
    "* Using Gensim. The embedding will be adjusted to your corpus using one of the above techniques: word2vec or fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:07.513565Z",
     "start_time": "2022-01-21T14:34:07.473176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:34:07.497218: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "vocab_size = 5000  # Maximum vocab size.\n",
    "max_len = 10  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = TextVectorization(max_tokens=vocab_size,\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:08.179138Z",
     "start_time": "2022-01-21T14:34:07.940003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:34:08.056246: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for large\n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "vectorize_layer.adapt(texts)\n",
    "# You have to fit the vectorize_layer only on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:10.244053Z",
     "start_time": "2022-01-21T14:34:10.229152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=int64, numpy=\n",
       "array([[ 5,  3,  6,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  3,  4,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2, 10,  8,  7,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  3,  6,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  9,  4,  0,  0,  0,  0,  0,  0,  0]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:10.756840Z",
     "start_time": "2022-01-21T14:34:10.745343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'you',\n",
       " 'like',\n",
       " 'tea',\n",
       " 'i',\n",
       " 'chocolate',\n",
       " 'paris',\n",
       " 'know',\n",
       " 'hate',\n",
       " 'didnt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:11.066814Z",
     "start_time": "2022-01-21T14:34:11.057222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 5000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 10,\n",
       " 'pad_to_max_tokens': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:11.493803Z",
     "start_time": "2022-01-21T14:34:11.378069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "Embedding (Embedding)        (None, 10, 256)           1280000   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2561      \n",
      "=================================================================\n",
      "Total params: 1,282,561\n",
      "Trainable params: 1,282,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_ = Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorize_layer(input_)\n",
    "x = Embedding(vocab_size, 256, name=\"Embedding\")(x)\n",
    "x = Flatten()(x)\n",
    "output_ = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input_, output_)\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:13.511100Z",
     "start_time": "2022-01-21T14:34:11.871033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(texts, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove/Word2Vec/FastText embedding\n",
    "\n",
    "**Traditional word embedding** techniques (Glove/Word2Vec/FastText) learn a global word embedding. They first build a global vocabulary using unique words in the documents by ignoring the meaning of words in different context. Then, similar representations are learnt for the words appeared more frequently close each other in the documents. The problem is that in such word representations the words' contextual meaning (the meaning derived from the words' surroundings), is ignored. For example, only one representation is learnt for \"left\" in sentence \"I left my phone on the left side of the table.\" However, \"left\" has two different meanings in the sentence, and needs to have two different representations in the embedding space.\n",
    "\n",
    "For example, consider the two sentences:\n",
    "\n",
    "1. I will show you a valid point of reference and talk to the point.\n",
    "1. Where have you placed the point.\n",
    "\n",
    "The word embeddings from a pre-trained embeddings such as word2vec, the embeddings for the word 'point' is same for both of its occurrences in example 1 and also the same for the word 'point' in example 2. (all three occurrences has same embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:13.669726Z",
     "start_time": "2022-01-21T14:34:13.516228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same steps as Keras Embedding\n",
    "vocab_size = 5000  # Maximum vocab size.\n",
    "max_len = 10  # Sequence length to pad the outputs to.\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=max_len)\n",
    "vectorizer.adapt(texts)\n",
    "texts_vec = vectorizer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:14.088202Z",
     "start_time": "2022-01-21T14:34:14.073380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'you': 2,\n",
       " 'like': 3,\n",
       " 'tea': 4,\n",
       " 'i': 5,\n",
       " 'chocolate': 6,\n",
       " 'paris': 7,\n",
       " 'know': 8,\n",
       " 'hate': 9,\n",
       " 'didnt': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build word dict\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:14.791803Z",
     "start_time": "2022-01-21T14:34:14.788496Z"
    }
   },
   "outputs": [],
   "source": [
    "# If necessary, download glove matrix\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:20.893584Z",
     "start_time": "2022-01-21T14:34:15.589305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Make a dict mapping words (strings) to their NumPy vector representation:\n",
    "path_to_glove_file = \"/users/riveill/DS-models/glove.6B.50d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:22.461583Z",
     "start_time": "2022-01-21T14:34:22.453480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "[UNK] 1\n",
      "you 2\n",
      "like 3\n",
      "tea 4\n",
      "i 5\n",
      "chocolate 6\n",
      "paris 7\n",
      "know 8\n",
      "hate 9\n",
      "didnt 10\n",
      "Converted 9 words (2 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    print(word, i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:26.542658Z",
     "start_time": "2022-01-21T14:34:26.535293Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:31.688806Z",
     "start_time": "2022-01-21T14:34:31.656758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 10, 50)            650       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 1,151\n",
      "Trainable params: 501\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_ = Input(shape=(max_len,), dtype=tf.int32)\n",
    "x = embedding_layer(input_)\n",
    "x = Flatten()(x)\n",
    "output_ = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input_, output_)\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:34.584627Z",
     "start_time": "2022-01-21T14:34:33.807219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(texts_vec, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts_vec, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your own model with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:42.125136Z",
     "start_time": "2022-01-21T14:34:37.496410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/riveill/opt/miniconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/riveill/opt/miniconda3/lib/python3.9/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/riveill/opt/miniconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/riveill/opt/miniconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, you need data to train a model. We will call this data set \"corpus ».\n",
    "\n",
    "Even if the corpus is small enough to fit entirely in memory, we thus implement a user-friendly iterator that reads the corpus line by line to demonstrate how one could work with a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:43.009461Z",
     "start_time": "2022-01-21T14:34:43.004336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Avalaible in your gensim installation\n",
    "corpus_path=\"/Users/riveill/opt/miniconda3/lib/python3.9/site-packages/gensim/test/test_data/\"\n",
    "corpus=\"lee_background.cor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:45.473873Z",
     "start_time": "2022-01-21T14:34:45.197647Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(corpus_path+corpus):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and train a model on our corpus. Don't worry about the training parameters much for now, we'll revisit them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:47.658438Z",
     "start_time": "2022-01-21T14:34:46.945021Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model, we can use it.\n",
    "\n",
    "The main part of the model is model.wv\\ , where \"wv\" stands for \"word vectors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:34:48.523287Z",
     "start_time": "2022-01-21T14:34:48.517431Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:04:08.927613Z",
     "start_time": "2022-01-21T14:04:08.890931Z"
    }
   },
   "source": [
    "Training non-trivial models can take time.  Once the model is built, it can be saved using standard gensim methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:36:02.220354Z",
     "start_time": "2022-01-21T14:36:02.169927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/ptzjj6dn0_x5trgl90dc98g40000gn/T/gensim-model-__mtorja\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    print(temporary_filepath)\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the template exactly as if it were a Glove/Word2Vec/FastText template retrieved from the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:37:27.356963Z",
     "start_time": "2022-01-21T14:37:27.217729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same steps as Keras Embedding\n",
    "vocab_size = 5000  # Maximum vocab size.\n",
    "max_len = 10  # Sequence length to pad the outputs to.\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=max_len)\n",
    "vectorizer.adapt(texts)\n",
    "texts_vec = vectorizer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:37:32.167898Z",
     "start_time": "2022-01-21T14:37:32.158701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build word dict\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T18:12:05.600944Z",
     "start_time": "2022-01-21T18:12:05.550036Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l4/ptzjj6dn0_x5trgl90dc98g40000gn/T/ipykernel_42288/2646765101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load gensim model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporary_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "# Load gensim model\n",
    "new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:42:30.434886Z",
     "start_time": "2022-01-21T14:42:30.428439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 words (7 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 150\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        model.wv[word]\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = model.wv[word]\n",
    "        hits += 1\n",
    "    except :\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:42:56.722094Z",
     "start_time": "2022-01-21T14:42:56.714804Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:43:10.564238Z",
     "start_time": "2022-01-21T14:43:10.536782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 150)           1950      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1501      \n",
      "=================================================================\n",
      "Total params: 3,451\n",
      "Trainable params: 1,501\n",
      "Non-trainable params: 1,950\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_ = Input(shape=(max_len,), dtype=tf.int32)\n",
    "x = embedding_layer(input_)\n",
    "x = Flatten()(x)\n",
    "output_ = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input_, output_)\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:43:22.124054Z",
     "start_time": "2022-01-21T14:43:21.523684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(texts_vec, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts_vec, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual word embedding\n",
    "\n",
    "However, **contextual embeddings** (are generally obtained from the transformer based models: BERT, ELMO, GPT3). The emeddings are obtained from a model by passing the entire sentence to the pre-trained model. Note that, here there is a vocabulary of words, but the vocabulary will not contain the contextual embeddings. The embeddings generated for each word depends on the other words in a given sentence. (The other words in a given sentence is referred as context. The transformer based models work on attention mechanism, and attention is a way to look at the relation between a word with its neighbors). Thus, given a word, it will not have a static embeddings, but the embeddings are dynamically generated from pre-trained (or fine-tuned) model.\n",
    "\n",
    "For example, consider the two sentences:\n",
    "1. I will show you a valid point of reference and talk to the point.\n",
    "1. Where have you placed the point.\n",
    "\n",
    "The embeddings from BERT or ELMO or any such transformer based models, the the two occurrences of the word 'point' in example 1 will have different embeddings. Also, the word 'point' occurring in example 2 will have different embeddings than the ones in example 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:47:33.187137Z",
     "start_time": "2022-01-21T14:47:33.171263Z"
    }
   },
   "source": [
    "We won't be seeing the transformers this year, but if you want to see how it works here is a [great presentation](https://youtu.be/_QejEDB7GM8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
