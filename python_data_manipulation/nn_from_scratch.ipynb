{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "$y = WX + b$\\\n",
    "$(n \\times p) = (n \\times n) (n \\times p) + (n \\times 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "$$\\frac{\\partial Loss(y, \\hat{y})}{\\partial W} = \\frac{\\partial Loss(y, \\hat{y})}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial W}$$\n",
    "and\n",
    "$$\\frac{\\partial Loss(y, \\hat{y})}{\\partial b} = \\frac{\\partial Loss(y, \\hat{y})}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial b}$$\n",
    "With:\n",
    "- $Loss(y, \\hat{y}) := MLE(y, \\hat{y}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    "- $\\frac{\\partial Loss(y, \\hat{y})}{\\partial \\hat{y}} = \\sum_{i=1}^n -2(y_i - \\hat{y}_i)$\n",
    "- $z := WX + b$\n",
    "- $\\hat{y} := \\sigma(WX + b)$, where $\\sigma$ is the activation function (*i.e.* sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self):\n",
    "        self.n_obs = 1\n",
    "        self.n_features = 2\n",
    "        self.X = self.generate_data()\n",
    "        self.y_true = np.logical_or(self.X[0], self.X[1]).astype(int)\n",
    "\n",
    "        self.learn_rate = 10**-3\n",
    "        self.W = np.random.randn(self.n_features)\n",
    "        self.b = np.random.randn(1)\n",
    "\n",
    "    def generate_data(self):\n",
    "        np.random.seed(42)\n",
    "        return np.random.randint(0, 2, size=(self.n_features))\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def loss(y_true, y_hat):\n",
    "        return (y_true - y_hat)**2\n",
    "\n",
    "    def loss_deriv(y_true, y_hat):\n",
    "        return 2 * (y_true - y_hat)\n",
    "\n",
    "    def forward_pass(self):\n",
    "        self.z = np.dot(self.W, self.X) + self.b\n",
    "        self.y_hat = self.sigmoid(self.z)\n",
    "\n",
    "    def backprop(self):\n",
    "        # Perform backpropagation\n",
    "        \n",
    "        # Compute derivatives to update weights\n",
    "        dloss_dyhat = self.loss_deriv(self.y_true, self.y_hat)\n",
    "        dyhat_dz = self.sigmoid_deriv(self.y_hat)\n",
    "        dz_dw = self.X\n",
    "        dloss_dw = dloss_dyhat * dyhat_dz * dz_dw\n",
    "        dloss_db = dloss_dyhat * dyhat_dz * 1\n",
    "        \n",
    "        # Update weights\n",
    "        self.W -= self.learn_rate * dloss_dw\n",
    "        self.b -= self.learn_rate * dloss_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
